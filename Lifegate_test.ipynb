{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "0fwQz6Zfke5i",
        "O36PGywGna2d"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPjXObV+2Co6qspJ18R54jB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Shaping/blob/main/Lifegate_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "zZ2S2CI8K_jT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jsHzrhmfpiTr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "from scipy.optimize import minimize\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deadend dependencies"
      ],
      "metadata": {
        "id": "UMj8NNrGfwu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/med-deadend.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KStXBTWK3f64",
        "outputId": "c7381f11-f4f6-458c-b772-f0d462631a12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'med-deadend'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 130 (delta 52), reused 77 (delta 22), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (130/130), 395.48 KiB | 10.99 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# shaping dependencies"
      ],
      "metadata": {
        "id": "AsZMw4C0f2K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ajagota7/Shaping.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu7X59dK3hqk",
        "outputId": "11d0fcde-6c5d-49c2-f097-353fb2596d8c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Shaping'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 81 (delta 36), reused 67 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (81/81), 11.54 MiB | 30.06 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Shaping"
      ],
      "metadata": {
        "id": "5SkpvWFqz631"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git pull origin main"
      ],
      "metadata": {
        "id": "8QRHhC-G0AiH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/"
      ],
      "metadata": {
        "id": "hZ8dnFnidxL_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Shaping\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/Shaping/lifegate_1.zip', 'r') as zip_ref:\n",
        "    # zip_ref.extractall('/content/med-deadend/lifegate/results/lifegate_1')\n",
        "    zip_ref.extractall('/content/Shaping/')"
      ],
      "metadata": {
        "id": "2noY6FOTdsmY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/med-deadend/lifegate\n",
        "\n",
        "\n",
        "# results_dir = 'results/lifegate_1/'\n",
        "results_dir = '/content/Shaping/'\n",
        "# Load the Q tables from the primary learning agent, Q_D and Q_R value functions\n",
        "with open(results_dir+'tabular_qnet.pkl', 'rb') as fq:\n",
        "    ai = pickle.load(fq)\n",
        "\n",
        "with open(results_dir+'tabular_qd.pkl', 'rb') as fd:\n",
        "    ai_d = pickle.load(fd)\n",
        "\n",
        "with open(results_dir+'tabular_qr.pkl', 'rb') as fr:\n",
        "    ai_r = pickle.load(fr)"
      ],
      "metadata": {
        "id": "7lv4ZIBkeLW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60bf3d2-32a6-4a61-8e37-8c55eccdf8dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/med-deadend/lifegate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((10, 10, 5))\n",
        "q_d = np.zeros_like(q_table)\n",
        "q_r = np.zeros_like(q_table)\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        for a in range(5):\n",
        "            key = tuple([j, i, a])\n",
        "            try:\n",
        "                q_table[i,j,a] = ai.q[key]\n",
        "                q_d[i,j,a] = ai_d.q[key]\n",
        "                q_r[i,j,a] = ai_r.q[key]\n",
        "            except:\n",
        "                pass"
      ],
      "metadata": {
        "id": "Rk0Z42sNebl4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import random"
      ],
      "metadata": {
        "id": "4uTTjsWNfK21"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lifegate import LifeGate"
      ],
      "metadata": {
        "id": "WZDw5-YMfMSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072ad4dc-3ef5-4143-a0a6-8da35eb440b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = yaml.safe_load(open('config.yaml', 'r'))"
      ],
      "metadata": {
        "id": "pzii8SOefSCm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed=params['random_seed'])\n",
        "random.seed(params['random_seed'])\n",
        "random_state = np.random.RandomState(params['random_seed'])"
      ],
      "metadata": {
        "id": "ircWaFyzfVZr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-b44zjyTIRyD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = LifeGate(max_steps=params['episode_max_len'], state_mode='tabular',\n",
        "                        rendering=True, image_saving=False, render_dir=None, rng=random_state, death_drag = 0.1)"
      ],
      "metadata": {
        "id": "XUoJuLN0fabn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_30 = LifeGate(max_steps=30, state_mode='tabular',\n",
        "                        rendering=True, image_saving=False, render_dir=None, rng=random_state, death_drag = 0.1)"
      ],
      "metadata": {
        "id": "7wG_zU6SM3xY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZp-8-f7far2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Shaping"
      ],
      "metadata": {
        "id": "hS65UmL5Yu_K"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Shaping import *"
      ],
      "metadata": {
        "id": "FwGOFhh0ZeGx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Shaping\n",
        "\n",
        "from choose_actions import action_probs_top_n_epsilon\n",
        "from shaping_features import *\n",
        "from gen_policies import *\n",
        "from IS import *\n",
        "from subset_policies import *\n",
        "from v_pi_e import *\n",
        "from optimization import *\n",
        "from neural_net import *\n",
        "from prep_variance import *\n",
        "from SCOPE_variance import SCOPE_variance"
      ],
      "metadata": {
        "id": "NYtD9mJOadhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a9dc7a-6298-4858-aa63-8b732f074c00"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Shaping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating policies"
      ],
      "metadata": {
        "id": "0fwQz6Zfke5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## interm updates"
      ],
      "metadata": {
        "id": "d4uOoMlDEv-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_actions(nb_episodes, env, action_probs):\n",
        "    \"\"\"\n",
        "    Run the experiment for a specified number of episodes.\n",
        "\n",
        "    Parameters:\n",
        "    - nb_episodes: Number of episodes\n",
        "    - env: Experiment environment\n",
        "    - action_probs: Action probabilities\n",
        "\n",
        "    Returns:\n",
        "    - policies: List of policies (pi_b or pi_e)\n",
        "    \"\"\"\n",
        "    # Define the dtype for the structured array\n",
        "    dtype = [\n",
        "        ('state_last', np.float64, (2,)),\n",
        "        ('action', np.int64),\n",
        "        ('reward', np.float64),\n",
        "        ('state', np.float64, (2,)),\n",
        "        ('timestep', np.int64),\n",
        "        ('psi', np.float64)\n",
        "    ]\n",
        "\n",
        "    policies = []\n",
        "    for i in range(nb_episodes):\n",
        "        trajectory = np.empty(0, dtype=dtype)\n",
        "        s = env.reset()\n",
        "        env.render()\n",
        "        term = False\n",
        "        timestep = 0\n",
        "        while not term:\n",
        "            state_last = s\n",
        "            print(s)\n",
        "            action = choose_action(tuple(s), action_probs)\n",
        "            s, r, term, _ = env.step(action)\n",
        "\n",
        "            psi = smallest_distance_to_deadend(state_last, env)\n",
        "            data_point = np.array([(state_last, action, r, s, timestep, psi)], dtype=dtype)\n",
        "            trajectory = np.append(trajectory, data_point)\n",
        "            timestep += 1\n",
        "\n",
        "        policies.append(trajectory)\n",
        "\n",
        "    with open('policies.pkl', 'wb') as f:\n",
        "        pickle.dump(policies, f)\n",
        "\n",
        "    return policies"
      ],
      "metadata": {
        "id": "Xh2UezkqHisR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_good = np.zeros((10, 10, 5))\n",
        "\n",
        "P_pi_b_good[2,9,3] = 1\n",
        "P_pi_b_good[1,9,3] = 1\n",
        "P_pi_b_good[0,9,1] = 1\n",
        "P_pi_b_good[0,8,1] = 1\n",
        "P_pi_b_good[0,7,1] = 1\n",
        "P_pi_b_good[0,6,1] = 1\n",
        "P_pi_b_good[0,5,1] = 1\n",
        "P_pi_b_good[0,4,4] = 1\n",
        "P_pi_b_good[1,4,4] = 1\n",
        "P_pi_b_good[2,4,4] = 1\n",
        "P_pi_b_good[3,4,4] = 1\n",
        "P_pi_b_good[4,4,4] = 1\n",
        "P_pi_b_good[5,4,1] = 1\n",
        "P_pi_b_good[5,5,1] = 1\n",
        "P_pi_b_good[5,4,1] = 1\n",
        "P_pi_b_good[5,3,1] = 1\n",
        "P_pi_b_good[5,2,1] = 1\n",
        "P_pi_b_good[5,1,1] = 1"
      ],
      "metadata": {
        "id": "PYLauaUbIq6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_good_stoch = np.zeros((10, 10, 5))\n",
        "\n",
        "P_pi_b_good_stoch[2,9,3] = 0.5\n",
        "P_pi_b_good_stoch[2,9,1] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[2,8,3] = 0.5\n",
        "P_pi_b_good_stoch[2,8,1] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[2,7,3] = 0.5\n",
        "P_pi_b_good_stoch[2,7,1] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[1,9,3] = 0.5\n",
        "P_pi_b_good_stoch[1,9,1] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[1,8,3] = 0.5\n",
        "P_pi_b_good_stoch[1,8,1] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[1,7,3] = 0.5\n",
        "P_pi_b_good_stoch[1,7,1] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[2,6,3] = 1\n",
        "P_pi_b_good_stoch[1,6,3] = 1\n",
        "P_pi_b_good_stoch[1,5,3] = 1\n",
        "\n",
        "\n",
        "P_pi_b_good_stoch[0,9,1] = 1\n",
        "P_pi_b_good_stoch[0,8,1] = 1\n",
        "P_pi_b_good_stoch[0,7,1] = 1\n",
        "P_pi_b_good_stoch[0,6,1] = 1\n",
        "P_pi_b_good_stoch[0,5,1] = 1\n",
        "\n",
        "P_pi_b_good_stoch[0,4,1] = 0.5\n",
        "P_pi_b_good_stoch[0,4,4] = 0.5\n",
        "P_pi_b_good_stoch[0,4,1] = 0.5\n",
        "P_pi_b_good_stoch[0,4,4] = 0.5\n",
        "P_pi_b_good_stoch[0,3,1] = 0.5\n",
        "P_pi_b_good_stoch[0,3,4] = 0.5\n",
        "P_pi_b_good_stoch[0,2,1] = 0.5\n",
        "P_pi_b_good_stoch[0,2,4] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[1,4,1] = 0.5\n",
        "P_pi_b_good_stoch[1,4,4] = 0.5\n",
        "P_pi_b_good_stoch[1,4,1] = 0.5\n",
        "P_pi_b_good_stoch[1,4,4] = 0.5\n",
        "P_pi_b_good_stoch[1,3,1] = 0.5\n",
        "P_pi_b_good_stoch[1,3,4] = 0.5\n",
        "P_pi_b_good_stoch[1,2,1] = 0.5\n",
        "P_pi_b_good_stoch[1,2,4] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[2,4,1] = 0.5\n",
        "P_pi_b_good_stoch[2,4,4] = 0.5\n",
        "P_pi_b_good_stoch[2,4,1] = 0.5\n",
        "P_pi_b_good_stoch[2,4,4] = 0.5\n",
        "P_pi_b_good_stoch[2,3,1] = 0.5\n",
        "P_pi_b_good_stoch[2,3,4] = 0.5\n",
        "P_pi_b_good_stoch[2,2,1] = 0.5\n",
        "P_pi_b_good_stoch[2,2,4] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[3,4,1] = 0.5\n",
        "P_pi_b_good_stoch[3,4,4] = 0.5\n",
        "P_pi_b_good_stoch[3,4,1] = 0.5\n",
        "P_pi_b_good_stoch[3,4,4] = 0.5\n",
        "P_pi_b_good_stoch[3,3,1] = 0.5\n",
        "P_pi_b_good_stoch[3,3,4] = 0.5\n",
        "P_pi_b_good_stoch[3,2,1] = 0.5\n",
        "P_pi_b_good_stoch[3,2,4] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[4,4,1] = 0.5\n",
        "P_pi_b_good_stoch[4,4,4] = 0.5\n",
        "P_pi_b_good_stoch[4,4,1] = 0.5\n",
        "P_pi_b_good_stoch[4,4,4] = 0.5\n",
        "P_pi_b_good_stoch[4,3,1] = 0.5\n",
        "P_pi_b_good_stoch[4,3,4] = 0.5\n",
        "P_pi_b_good_stoch[4,2,1] = 0.5\n",
        "P_pi_b_good_stoch[4,2,4] = 0.5\n",
        "\n",
        "P_pi_b_good_stoch[0,1,4] = 1\n",
        "P_pi_b_good_stoch[1,1,4] = 1\n",
        "P_pi_b_good_stoch[2,1,4] = 1\n",
        "P_pi_b_good_stoch[3,1,4] = 1\n",
        "P_pi_b_good_stoch[4,1,4] = 1\n",
        "\n",
        "P_pi_b_good_stoch[5,4,1] = 1\n",
        "P_pi_b_good_stoch[5,3,1] = 1\n",
        "P_pi_b_good_stoch[5,2,1] = 1\n",
        "P_pi_b_good_stoch[5,1,1] = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "t5GNtT3tPttg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_bad = np.zeros((10, 10, 5))\n",
        "\n",
        "P_pi_b_bad[2,9,4] = 1\n",
        "\n",
        "P_pi_b_bad[3,9,1] = 1\n",
        "P_pi_b_bad[3,8,1] = 1\n",
        "P_pi_b_bad[3,7,1] = 1\n",
        "P_pi_b_bad[3,6,4] = 1\n",
        "\n",
        "P_pi_b_bad[4,6,2] = 1\n",
        "P_pi_b_bad[4,7,2] = 1\n",
        "P_pi_b_bad[4,8,2] = 1\n",
        "P_pi_b_bad[4,9,4] = 1\n",
        "\n",
        "P_pi_b_bad[5,9,1] = 1\n",
        "P_pi_b_bad[5,8,1] = 1\n",
        "P_pi_b_bad[5,7,1] = 1\n",
        "P_pi_b_bad[5,6,4] = 1\n",
        "\n",
        "P_pi_b_bad[6,6,4] = 1\n",
        "P_pi_b_bad[7,6,4] = 1\n",
        "P_pi_b_bad[8,6,4] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kuKtHy7UgHwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.main_deaths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B0ogRtNlRu6",
        "outputId": "01841628-6c3d-4127-e489-96b0ebdb6cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[9, 0],\n",
              " [9, 1],\n",
              " [9, 2],\n",
              " [9, 3],\n",
              " [9, 4],\n",
              " [9, 5],\n",
              " [9, 6],\n",
              " [9, 7],\n",
              " [9, 8],\n",
              " [9, 9],\n",
              " [8, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gen Policies"
      ],
      "metadata": {
        "id": "RBlY1w9aJppv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(1000, env, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "pi_e = experiment_actions(1000, env, P_pi_e)"
      ],
      "metadata": {
        "id": "wW1SGejBZZlb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(1000, env_30, P_pi_b)\n",
        "\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "pi_e = experiment_actions(1000, env_30, P_pi_e)"
      ],
      "metadata": {
        "id": "1KqavLn-NERh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep"
      ],
      "metadata": {
        "id": "03SCZEAMnGeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 32], output_dim=1, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "LRcpZ6zAowqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = SCOPE_variance(model, 0.9, 10000, pi_b, P_pi_b, P_pi_e, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "jqKcMhNSnRTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IS_tensor, samples_IS, padded_state_tensors, padded_weight_diff_tensors, gamma_weights_last_tensor, states_first_tensor, states_last_tensor = test1.prepare()\n",
        "\n",
        "# Modified class with bootstrap_all_terms\n",
        "IS_tensor, padded_state_tensors, padded_weight_diff_tensors, gamma_weights_last_tensor, states_first_tensor, states_last_tensor = test1.prepare()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uftstgp1nRTP",
        "outputId": "2d127529-334e-4e7d-a5ca-71688f5d883f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-827856c56c37>:89: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  padded_weight_diff_tensors = torch.tensor(padded_weights_difference, dtype = self.dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps, states, states_first, states_last, actions, rewards, gamma_last, weights_last, weights, weights_difference = test1.prep_policies()"
      ],
      "metadata": {
        "id": "IG7OqI1kuQ7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_output, states_first_output, states_last_output = test1.pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)\n",
        "sums_states_weight_diff = test1.states_weight_diff_sums(states_output, padded_weight_diff_tensors)\n",
        "gamma_weights_states_last_sub_states_first = test1.last_first_terms_operations(gamma_weights_last_tensor, states_last_output, states_first_output)\n",
        "\n",
        "\n",
        "# sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE = test1.bootstrap_shaping_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor)\n",
        "samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE = test1.bootstrap_all_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor)\n"
      ],
      "metadata": {
        "id": "-AlHoAZwi9Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_terms = sums_states_weight_diff + gamma_weights_states_last_sub_states_first\n",
        "IS_SCOPE_1 = IS_tensor*sum_terms"
      ],
      "metadata": {
        "id": "Gspo0QLFDzTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdImElYSD_rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPE = sample_sums_states_weight_diff+samples_gamma_weight_states_last_sub_states_first\n",
        "E_IS_SCOPE = torch.mean(torch.mean(samples_IS, dim =1) * torch.mean(SCOPE, dim =1))\n",
        "E_IS_E_SCOPE = torch.mean(torch.mean(samples_IS,dim = 1 ))*torch.mean(torch.mean(SCOPE, dim =1))\n",
        "\n",
        "E_IS_SCOPE_2 = torch.mean(torch.mean(samples_IS_SCOPE, dim =1))\n",
        "E_IS_E_SCOPE_2 = torch.mean(torch.mean(samples_IS,dim = 1 )) * torch.mean(torch.mean(samples_all_shaping, dim =1))"
      ],
      "metadata": {
        "id": "mZ_xYyhiCxe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E_IS_SCOPE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGY9Jt3FDHKZ",
        "outputId": "4806bfd5-cc2a-40dc-9cf1-f9e471dfaf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.1470e+24, dtype=torch.float64, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "E_IS_SCOPE_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbttxO_jDJGy",
        "outputId": "a0767c9a-54f3-4b4a-878c-f97fde383d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.5943e+26, dtype=torch.float64, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_IS_SCOPE[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtJJjUv-Ctv9",
        "outputId": "30a2eb29-85cd-42f7-913a-422b80b5c82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -2.5405e-85,  -0.0000e+00,   7.9024e-62,  -2.9790e-61,  -3.1831e-75,\n",
              "         -1.5365e-64,  -0.0000e+00,  -8.3521e-71,   9.1314e-24,  -6.9648e-66,\n",
              "        -2.0605e-135,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -1.5136e-54,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "          0.0000e+00,   7.9024e-62,   0.0000e+00,  -0.0000e+00,  2.1164e-111,\n",
              "        -9.5684e-135,  -0.0000e+00, -1.4140e-116,  -3.3219e-82,  -0.0000e+00,\n",
              "         -1.5197e-87,  -2.5695e-67,   5.3077e-82,  -0.0000e+00,   2.3741e-47,\n",
              "         -0.0000e+00,  -1.8253e-71,  -3.4879e+11,  -0.0000e+00,  -0.0000e+00,\n",
              "         3.8146e-114,  -0.0000e+00,  -7.5975e-31,  1.2321e-124,  -0.0000e+00,\n",
              "         -0.0000e+00, -2.0958e-170,   0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00, -1.0845e-110,  -1.8398e-69,  -1.8253e-71,   0.0000e+00,\n",
              "         -1.0142e-82,  -3.8087e-71, -4.7846e-116,  -0.0000e+00,  -1.1575e+14,\n",
              "         -8.3983e-61,  -0.0000e+00,  -3.5652e-20,  -4.5806e-83,  -4.3459e-47,\n",
              "         -8.4544e-60,  -0.0000e+00,  -8.8170e-22,  -0.0000e+00,  -0.0000e+00,\n",
              "          1.2354e-36,  -1.5363e-35,  -0.0000e+00, -4.9603e-128,  -0.0000e+00,\n",
              "         -6.3533e-43,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -1.5943e-87,\n",
              "         -3.5388e-13,  -0.0000e+00,  -4.5280e-49,  -1.8006e-40,  -1.9589e-72,\n",
              "         -0.0000e+00,  -1.6958e-50,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "        -6.2039e-105,  -0.0000e+00, -2.1867e-103, -5.2111e-109,  -9.0234e-81,\n",
              "        -7.3637e-125,  -0.0000e+00, -4.3031e-114,  -0.0000e+00,  -0.0000e+00,\n",
              "         -9.0569e-20, -4.9603e-128,  -0.0000e+00,  -0.0000e+00,  -1.2844e-50,\n",
              "          6.8821e-47,  -0.0000e+00,  -0.0000e+00,  -3.6297e-43,  -1.2560e-81,\n",
              "         -0.0000e+00,  -9.6121e-75,  -0.0000e+00,  -0.0000e+00,  -4.7752e-22,\n",
              "         -0.0000e+00,  -8.8170e-22,  -0.0000e+00,  -0.0000e+00,  -3.1837e-84,\n",
              "         -1.1765e-76,  -0.0000e+00,  -1.2844e-50, -7.3798e-163,  -2.0995e-92,\n",
              "         -0.0000e+00, -2.1867e-103,  -0.0000e+00,  -5.9858e-99,  -0.0000e+00,\n",
              "        -3.8876e-136,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -5.4505e-63,\n",
              "         -1.7515e-70,  -0.0000e+00,  -4.5557e-54,  -0.0000e+00,   1.7970e-29,\n",
              "         -4.4059e-35,  -1.8068e-87,  -4.7919e-84,  -1.2284e-23,  -1.5365e-64,\n",
              "          7.8287e-64,  -0.0000e+00,   0.0000e+00,  -2.5680e-16,  1.0151e-134,\n",
              "          5.4627e-50,  -2.1715e-71,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,   2.3966e-44, -4.9603e-128,  -8.2095e-86,  -2.6167e-27,\n",
              "         -6.9755e-72,  -2.5695e-67,  -1.7051e-55, -3.8876e-136,  -1.0863e-21,\n",
              "         9.3247e-114,  -4.4059e-35, -5.3088e-177,  -1.7168e-40,  -0.0000e+00,\n",
              "         -9.1651e-07,  -0.0000e+00,  -0.0000e+00,  -5.1282e-98,  -2.1147e-36,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,   3.2571e-55,\n",
              "          0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -2.5695e-67,\n",
              "         -3.3292e-89,  -0.0000e+00,  -0.0000e+00,   0.0000e+00, -5.9574e-107,\n",
              "         -6.7180e-81,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00, -2.2899e-120,\n",
              "         -1.2131e-90,  -1.1847e-05,  -0.0000e+00,  -0.0000e+00,   5.6168e-64,\n",
              "         -0.0000e+00,  -0.0000e+00,  -7.5275e-79,  -3.9772e-98, -1.4124e-115,\n",
              "          6.7779e-24,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -3.4915e-62,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -4.7835e-21,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00, -7.4818e-121,  -1.4379e-56,  -5.9858e-99,   6.5353e-19,\n",
              "         -0.0000e+00, -1.0845e-110,  -1.6331e-73,  -0.0000e+00,  -3.3794e-44,\n",
              "         -0.0000e+00,  -0.0000e+00,  -1.4403e-82,  -0.0000e+00,  -8.2958e-38,\n",
              "         -0.0000e+00,  -2.1267e-31,  -2.0919e-39,  -6.2543e-89,  -7.2344e-89,\n",
              "         -0.0000e+00, -7.4410e-122,  -0.0000e+00,  -9.2244e-29,  -7.6267e-28,\n",
              "         -0.0000e+00,  -1.4441e-84,  -3.4686e-46,  -0.0000e+00,   1.5539e-32,\n",
              "         -0.0000e+00,   2.8584e-70,  -0.0000e+00, -6.7465e-145,   2.3744e-38,\n",
              "         -1.0969e-41,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.6507e-76,  -0.0000e+00, -6.5637e-111,  -0.0000e+00,  -1.0553e-59,\n",
              "         -6.3674e-30,  -0.0000e+00,  -1.1992e-76, -8.8087e-109,  -0.0000e+00,\n",
              "         -0.0000e+00,  -2.0183e-60,   2.3741e-47,  -0.0000e+00,   6.4848e-87,\n",
              "         -5.4107e-35,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -9.3262e-46,  -0.0000e+00,  -1.0841e-42,  -0.0000e+00,  -3.2018e-02,\n",
              "         -0.0000e+00,  -1.1614e-91,  -0.0000e+00,  -7.3072e-92,  -0.0000e+00,\n",
              "          1.7396e-15, -8.8087e-109,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -3.3292e-89,  -0.0000e+00,  -1.8096e-45,   1.6171e-12,   2.3741e-47,\n",
              "         -2.1147e-36, -7.3798e-163,  -0.0000e+00,   2.4396e-88,  -0.0000e+00,\n",
              "         -0.0000e+00, -1.7320e-101,   3.5779e-27,  -0.0000e+00,  -3.9346e-91,\n",
              "         -1.4120e-71,  -0.0000e+00,  -0.0000e+00,  -1.1299e-90,  -0.0000e+00,\n",
              "         -0.0000e+00,  -5.2727e-61,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -1.0228e-11,  2.1164e-111,  -0.0000e+00,  -0.0000e+00,\n",
              "         -3.3794e-44, -2.3564e-152,  -0.0000e+00,  -2.5200e-98,  -1.4403e-82,\n",
              "         -0.0000e+00, -1.6503e-110,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.5405e-85,  -0.0000e+00,  -0.0000e+00,  -1.7051e-55,  -1.1579e-96,\n",
              "        -1.6503e-110,  -0.0000e+00, -7.9975e-114,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -2.5266e-42,  -3.2963e-38,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00, -1.3348e-146,  -7.9739e-71,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -1.6331e-73,  -1.2096e-60,  -4.9155e-49,  -0.0000e+00,\n",
              "         -8.5756e-93,  -0.0000e+00, -2.3319e-135,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,   2.0816e+09,  -3.3419e-33,  -0.0000e+00,\n",
              "         -9.5822e-03,  -0.0000e+00, -3.0795e-106,  -0.0000e+00,  -0.0000e+00,\n",
              "         -1.1765e-76,  -0.0000e+00,  -0.0000e+00,  -1.0841e-42,  -0.0000e+00,\n",
              "         -4.1517e-59,  -1.0295e-65,  -0.0000e+00,  -1.8006e-40,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -8.4740e-76,   0.0000e+00,\n",
              "        -2.9514e-101,  -2.5200e-98, -2.0605e-135,   2.3977e-39,  -0.0000e+00,\n",
              "         -0.0000e+00,  -1.6331e-73, -5.5873e-109,  -0.0000e+00,  -1.1992e-76,\n",
              "         -3.1831e-75,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -1.8398e-69,\n",
              "        -1.0896e-137,  -0.0000e+00,  -1.1847e-05,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.8369e-95,  -6.2692e-86,  -0.0000e+00,  -0.0000e+00,   1.9911e-59,\n",
              "         -2.3919e-79, -5.9400e-117,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -1.3256e-29,  -0.0000e+00,  -0.0000e+00,  -2.9009e-36,\n",
              "        -5.9194e-108,  -1.7051e-55,  -4.4059e-35,  -0.0000e+00, -2.0605e-135,\n",
              "         -1.5469e-21,  -4.4059e-35,  -4.5557e-54,  -0.0000e+00,  -8.7108e-62,\n",
              "         -1.1976e-94,  -1.8848e-48,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "        -5.2070e-107,  -0.0000e+00,  -2.0882e-86,  -0.0000e+00,  -5.1132e-04,\n",
              "          0.0000e+00,   9.1906e-19,  -3.1091e-65,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "        -3.7267e-107,  -0.0000e+00,  -4.9777e-77,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -4.1925e-25, -3.0795e-106,  -0.0000e+00,\n",
              "         -0.0000e+00,   0.0000e+00,  -0.0000e+00,  -4.5806e-83,  2.1164e-111,\n",
              "         -2.5200e-98, -4.3088e-113,  -0.0000e+00,  -3.6404e-32,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -9.1264e-43,  -0.0000e+00,\n",
              "         -2.4591e-89,  -0.0000e+00,  -0.0000e+00,   1.9275e-45,  -1.1543e-33,\n",
              "         -1.2560e-81,  -0.0000e+00,  -0.0000e+00,   2.3977e-39, -3.3028e-112,\n",
              "          7.8287e-64,  -0.0000e+00,  -0.0000e+00, -7.7889e-104, -6.3394e-172,\n",
              "         -8.8170e-22, -5.5873e-109,  3.8146e-114,  -0.0000e+00,  3.4576e-143,\n",
              "        -1.3164e-160,  -0.0000e+00,  -1.1579e-96,  -0.0000e+00,  -1.1299e-90,\n",
              "         -0.0000e+00,  -0.0000e+00,  -1.1997e-38,  -2.4591e-89,   9.1690e-44,\n",
              "         -2.2622e-69,  -1.2560e-81,  -3.2067e-77,  -3.5388e-13,   2.4880e-25,\n",
              "         -3.5370e-23,   5.4627e-50,  -0.0000e+00, -2.4613e-126,  -0.0000e+00,\n",
              "        -7.9975e-114,  -4.0313e-39,  -0.0000e+00,  -5.2680e-78, -6.3760e-128,\n",
              "         -5.0730e-73,  -9.1651e-07,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "        -2.2899e-120,   6.4848e-87,  -0.0000e+00,  -0.0000e+00,   1.7396e-15,\n",
              "         -2.4878e-79,  -0.0000e+00, -6.7220e-124,  3.4576e-143,  -5.4505e-63,\n",
              "          0.0000e+00,  -0.0000e+00,  -9.2244e-29,  -0.0000e+00,  -0.0000e+00,\n",
              "         -1.8006e-40,  -0.0000e+00,  -0.0000e+00, -3.9715e-111, -7.8048e-109,\n",
              "         -0.0000e+00,  -0.0000e+00,  -6.0378e-78,  -6.9027e-64,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,   0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -4.6446e+26,  -2.8369e-95, -3.7267e-107,\n",
              "         -0.0000e+00,   5.9283e-29,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "        -5.2111e-109,  -4.9777e-77,  -2.5057e-97,  -0.0000e+00,  -0.0000e+00,\n",
              "         -4.3459e-47,  -0.0000e+00,  -9.6952e-11,  -0.0000e+00,  -0.0000e+00,\n",
              "         -1.0423e-88,  -0.0000e+00,  -9.7803e-64,  -0.0000e+00,  -2.6108e-10,\n",
              "         -1.1245e-32,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -1.1614e-91,\n",
              "          5.9283e-29,  -1.8006e-40,  -0.0000e+00,  -1.4581e-42,  -1.9457e-84,\n",
              "          0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -1.2597e-64,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -7.6267e-28,  -1.1340e-54,  -0.0000e+00,   5.6168e-64,\n",
              "         2.1164e-111,  -1.6117e-70, -1.6001e-142, -7.4188e-119,  -1.6297e-40,\n",
              "         -0.0000e+00, -9.6814e-148,   0.0000e+00,  -4.9886e-56,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -3.5652e-20, -5.6374e-108,  -0.0000e+00,\n",
              "          6.4848e-87,  -0.0000e+00, -1.2228e-108,   9.1690e-44,  -2.4591e-89,\n",
              "        -5.2111e-109, -1.0845e-110,  -0.0000e+00,  -0.0000e+00,  -1.2284e-23,\n",
              "        -1.3276e-126,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -2.5405e-85,  -1.1543e-33,  -6.5620e-73,\n",
              "          9.6998e-75,   2.3966e-44,  -0.0000e+00,  -1.5943e-87,   6.8821e-47,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.5695e-67,  -3.9346e-91, -9.5684e-135,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00, -1.6503e-110,  -2.4878e-79,  -0.0000e+00,  -1.2096e-60,\n",
              "         -4.9154e-65,  -0.0000e+00,  -0.0000e+00,  -1.6205e-31, -5.9400e-117,\n",
              "         -3.6297e-43, -3.8447e-105,  -2.1245e-84,  -0.0000e+00,  -0.0000e+00,\n",
              "         -5.9911e-37,  -0.0000e+00,  -0.0000e+00,  -2.5057e-97, -2.6928e-118,\n",
              "         -0.0000e+00, -3.7267e-107,  -2.7470e-44, -2.1867e-103,  -5.0835e-67,\n",
              "        -7.4188e-119, -2.9514e-101,  -8.8828e-84, -3.9715e-111,  -5.5407e-09,\n",
              "         -8.3866e-92,  -0.0000e+00,  -3.7204e-20,   1.1578e-36,  -0.0000e+00,\n",
              "         -4.2977e-22,  -4.1575e-21,  -1.8068e-87,  -0.0000e+00,  -0.0000e+00,\n",
              "        -7.7889e-104,  -5.8622e-39,  -1.5944e+29,  -0.0000e+00,  -2.6167e-27,\n",
              "         -0.0000e+00,  -0.0000e+00,  -2.6167e-27,  -8.2095e-86,  -0.0000e+00,\n",
              "         -1.8470e-62,  -0.0000e+00,  -1.0278e-42,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -1.0553e-59,  -0.0000e+00, -6.5637e-111,  -0.0000e+00,\n",
              "         -4.6573e-78,  -1.3726e-66,  -1.7515e-70,  -2.3103e-40,  -0.0000e+00,\n",
              "         -4.9064e-04,   1.9275e-45,  -0.0000e+00,  -0.0000e+00,  -6.7637e-94,\n",
              "         -1.1299e-90,  -0.0000e+00, -1.2228e-108,  -0.0000e+00,  -1.5363e-35,\n",
              "        -2.6557e-114,  -7.6029e-46,  -2.7147e-61,  -1.9612e-45,  -1.2844e-50,\n",
              "         -1.6958e-50,  -2.7470e-44,  -0.0000e+00, -8.3040e-105,   0.0000e+00,\n",
              "        -7.4818e-121,  -0.0000e+00,   0.0000e+00,   6.8821e-47,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "          5.9283e-29,  -0.0000e+00, -8.8087e-109,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -8.2095e-86,  -0.0000e+00,  -0.0000e+00,  3.8146e-114,\n",
              "         -0.0000e+00,  -0.0000e+00, -5.7092e-133,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -3.1256e-52,  -0.0000e+00,\n",
              "          0.0000e+00,  -2.1977e-38,  -9.2244e-29,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -6.3676e+00,  -3.4200e-45,  -0.0000e+00,\n",
              "         -9.1264e-43, -9.5684e-135,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,   0.0000e+00,  -1.4581e-42,  -0.0000e+00,\n",
              "         -0.0000e+00,   2.0581e-29,   1.7970e-29,  -4.6949e-62, -4.3031e-114,\n",
              "         -0.0000e+00,  -0.0000e+00,  -4.1925e-25,  -0.0000e+00,  -0.0000e+00,\n",
              "        -1.6001e-142,  -0.0000e+00, -5.9574e-107, -1.6001e-142,  -3.7204e-20,\n",
              "        -8.0555e-127,  -0.0000e+00,  -0.0000e+00,  -1.4441e-84,  -0.0000e+00,\n",
              "         -5.1132e-04,   5.9283e-29,  -0.0000e+00,  -8.4817e-65,  -0.0000e+00,\n",
              "         -1.0142e-82,  -0.0000e+00,   0.0000e+00,  -0.0000e+00,  -1.6297e-40,\n",
              "         -1.1806e-76,  -0.0000e+00,   8.8334e-65,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -1.2844e-50,  -6.9755e-72,  -0.0000e+00,\n",
              "        -1.6001e-142,  -0.0000e+00,  -2.7082e-39, -2.3923e-123,  -1.2560e-81,\n",
              "         -5.5725e-99,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -6.2249e-32,\n",
              "         -0.0000e+00,  -3.4879e+11,  -1.2096e-60,  -1.2131e-90, -6.7220e-124,\n",
              "          7.6016e-81,  -5.5407e-09,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.0995e-92, -5.7092e-133,  -4.1575e-21,  -9.1264e-43, -1.8313e-144,\n",
              "         -0.0000e+00,  -1.5363e-35,  -0.0000e+00,  -3.4915e-62,  -0.0000e+00,\n",
              "         -0.0000e+00,  -3.2067e-77,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -1.5469e-21,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  9.3247e-114,  -0.0000e+00, -1.4124e-115,\n",
              "         -0.0000e+00,  -6.5620e-73, -1.6503e-110,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00, -3.0795e-106,  -1.1997e-38, -5.3088e-177,  -0.0000e+00,\n",
              "         -7.8034e-88,  -0.0000e+00,   2.0581e-29,  -2.5405e-85,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -3.9346e-91,  -4.1575e-21,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00, -7.4410e-122,  -0.0000e+00,  -0.0000e+00,\n",
              "          0.0000e+00,  -2.0389e-21,  -1.3726e-66, -7.3637e-125,  -0.0000e+00,\n",
              "         -7.5275e-79,  -0.0000e+00,  -7.9739e-71,  -0.0000e+00,  -0.0000e+00,\n",
              "        -4.7846e-116,  -0.0000e+00, -5.7104e-117,   0.0000e+00,  -0.0000e+00,\n",
              "        -7.4818e-121,  -1.5197e-87,  -0.0000e+00,  -0.0000e+00, -1.4183e-111,\n",
              "         -4.3459e-47,  -0.0000e+00, -3.6588e-102,  -0.0000e+00,  -0.0000e+00,\n",
              "         -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -4.7835e-21,\n",
              "         -4.7835e-21,  -0.0000e+00,  -1.0228e-11,  -0.0000e+00,  -0.0000e+00,\n",
              "         -3.9772e-98,  -0.0000e+00,  -4.2977e-22,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.5680e-16, -1.4183e-111,  -0.0000e+00,  -4.2977e-22,  -1.2793e-97,\n",
              "         -6.8366e-36,  -0.0000e+00,   2.3977e-39,  -9.6952e-11,  -0.0000e+00,\n",
              "         -0.0000e+00,   1.9911e-59,  -1.4120e-71,  -0.0000e+00, -9.5684e-135,\n",
              "        -3.1407e-107,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,  -0.0000e+00,\n",
              "         -2.5266e-42,  -2.6108e-10,  -0.0000e+00,  -0.0000e+00,  -5.3309e-32,\n",
              "         -0.0000e+00, -3.4482e-108,  -0.0000e+00, -3.3028e-112,  -1.8253e-71,\n",
              "         -1.5197e-87,  -0.0000e+00,  -1.2319e-33,  -0.0000e+00,   0.0000e+00],\n",
              "       dtype=torch.float64, grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sums_states_weight_diff[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orvqs0-IwoN4",
        "outputId": "82af141d-6b75-47cc-f39e-77dccf6c0612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5.1683e-01,  5.2720e-01,  2.3982e+00,  5.2951e-01,  5.1357e-01,\n",
              "         5.1364e-01, -1.1535e+00,  5.1631e-01,  5.1753e-01,  5.1325e-01,\n",
              "         9.0952e-02,  3.6275e-01,  5.2720e-01,  7.6119e-02,  7.6752e-02,\n",
              "         5.3759e-01,  9.0952e-02,  7.6337e-02,  9.0952e-02,  9.0657e-02,\n",
              "         5.7605e+00,  2.3982e+00,  3.6205e+00,  9.0436e-02,  3.9866e+00,\n",
              "         7.6361e-02,  5.1632e-01,  7.7653e-02,  1.2674e-01,  9.0436e-02,\n",
              "         7.6119e-02,  5.1709e-01,  3.6920e+00,  5.1663e-01,  9.0973e-02,\n",
              "         5.3759e-01,  5.3762e-01, -2.9977e+06,  5.1357e-01,  5.3602e-01,\n",
              "         4.1946e+00,  5.2720e-01, -6.4696e+01,  5.2645e-01,  5.1683e-01,\n",
              "         5.2907e-01,  5.2645e-01,  3.6920e+00,  5.3771e-01,  4.8009e-01,\n",
              "         4.2201e-01,  9.0952e-02,  8.3283e-02,  5.3762e-01,  4.2676e+00,\n",
              "         5.1366e-01,  7.7653e-02,  5.3583e-01,  5.3728e-01, -5.6362e+07,\n",
              "         2.5242e-01, -1.5415e+10, -1.8276e-02,  8.9459e-02,  9.0952e-02,\n",
              "         5.1665e-01, -2.1687e+00, -4.6237e+01,  5.2644e-01,  5.3773e-01,\n",
              "         9.0433e-02, -3.7703e-01,  5.1365e-01,  7.7653e-02,  5.3583e-01,\n",
              "         5.1357e-01,  9.0955e-02,  9.0431e-02,  7.7654e-02,  5.1709e-01,\n",
              "         4.2142e-01,  5.3759e-01,  5.3759e-01,  5.1357e-01, -2.4942e+01,\n",
              "         8.9367e-02,  5.3773e-01,  5.2656e-01,  9.0952e-02,  5.3583e-01,\n",
              "         5.1364e-01,  5.2901e-01,  5.2644e-01,  7.7619e-02,  9.0952e-02,\n",
              "         9.0952e-02,  4.7722e-01,  5.2653e-01,  5.3771e-01,  5.3759e-01,\n",
              "        -2.7596e-01,  7.7653e-02,  9.0952e-02,  7.6361e-02,  9.0952e-02,\n",
              "         5.1728e-01,  9.0952e-02,  9.0431e-02,  4.7655e-01,  5.3759e-01,\n",
              "         5.2644e-01,  9.0657e-02,  7.6361e-02,  1.3149e-01,  5.1357e-01,\n",
              "         7.7556e-02, -4.6237e+01,  5.2653e-01,  9.0436e-02,  4.3616e-01,\n",
              "         7.7633e-02,  9.0433e-02,  9.0952e-02,  9.0952e-02,  5.1665e-01,\n",
              "         5.3547e-01,  5.2644e-01,  7.6105e-02,  9.0952e-02, -1.3443e+00,\n",
              "         5.0960e-01,  5.3761e-01,  9.0952e-02,  5.3591e-01,  5.1357e-01,\n",
              "         5.2714e-01,  4.7650e-01,  8.2442e-02,  5.2920e-01,  7.4989e-02,\n",
              "         5.1428e-01, -9.2283e-01,  5.3759e-01,  5.2900e-01,  5.1364e-01,\n",
              "         1.3196e+00,  5.3602e-01,  3.5049e+00, -1.3996e+03,  1.0444e+01,\n",
              "         7.6121e-02,  9.0949e-02,  7.7556e-02,  9.0952e-02,  5.1711e-01,\n",
              "         9.0955e-02,  4.2595e+00,  7.7653e-02,  5.1357e-01,  5.2900e-01,\n",
              "         5.1782e-01,  5.1709e-01, -2.5074e+01,  5.0960e-01, -1.8205e+00,\n",
              "         5.2652e-01,  5.1428e-01,  5.3773e-01,  5.3771e-01,  5.2720e-01,\n",
              "        -6.8967e+05,  5.3583e-01, -5.6433e-01,  5.3761e-01,  5.2720e-01,\n",
              "         7.7653e-02,  8.9262e-02, -4.4482e+00,  9.0956e-02,  9.0436e-02,\n",
              "         5.4839e+00,  5.2644e-01,  2.3645e-01,  5.1589e-01,  5.1709e-01,\n",
              "         5.3759e-01,  5.1916e-01,  9.0657e-02,  3.6926e+00,  5.2720e-01,\n",
              "         3.5348e-01,  7.6119e-02,  9.0433e-02,  5.2481e-01,  9.0952e-02,\n",
              "         3.9951e-01,  5.1318e-01,  9.0976e-02,  7.6361e-02,  7.4962e-02,\n",
              "         9.0436e-02,  9.0955e-02,  5.2644e-01,  5.1364e-01,  5.2656e-01,\n",
              "         7.6119e-02,  9.0954e-02,  5.3771e-01,  5.1724e-01,  4.3459e-01,\n",
              "         4.7453e-01,  5.2481e-01,  5.3093e-01,  5.2720e-01,  4.7947e-01,\n",
              "         5.1357e-01,  9.0654e-02,  4.0053e-01,  9.0954e-02,  5.3761e-01,\n",
              "         9.0974e-02,  2.5289e-01,  5.3583e-01,  5.1370e-01,  4.7938e-01,\n",
              "         8.3403e-02,  5.3602e-01,  2.3645e-01,  9.0973e-02,  5.2973e-01,\n",
              "         8.9367e-02,  7.7653e-02,  5.1366e-01,  9.0952e-02,  7.4973e-02,\n",
              "         4.7722e-01,  9.0952e-02,  9.0952e-02,  5.3583e-01,  4.8604e-02,\n",
              "         7.7653e-02,  5.1711e-01,  4.1136e-01,  4.7650e-01,  5.2264e-01,\n",
              "         5.3544e-01,  7.7653e-02,  5.2720e-01,  5.2720e-01,  5.1683e-01,\n",
              "         5.2643e-01,  9.0954e-02,  9.0952e-02,  5.1562e-01, -3.3918e-01,\n",
              "         7.7633e-02,  5.1665e-01,  5.3759e-01,  5.1364e-01,  2.8980e+00,\n",
              "        -1.2457e+01,  1.3655e+06,  5.1364e-01,  9.0952e-02,  7.7619e-02,\n",
              "         7.7653e-02,  5.2907e-01,  5.2481e-01,  8.4154e-02,  7.7633e-02,\n",
              "         5.3602e-01,  5.2644e-01,  5.3773e-01,  4.7939e-01,  7.7246e-02,\n",
              "        -4.3690e+00, -1.0027e+00, -8.2078e+00,  9.0973e-02,  5.2723e-01,\n",
              "         5.3330e-01,  5.1357e-01,  9.0973e-02,  4.2201e-01,  3.9321e+00,\n",
              "         4.7938e-01,  4.4394e-01,  5.1665e-01,  9.0436e-02, -7.3013e+01,\n",
              "         8.3361e-02,  5.1414e-01,  4.2142e-01,  5.1504e-01, -1.6895e+10,\n",
              "         9.0952e-02,  4.7722e-01,  7.7633e-02,  5.1357e-01,  9.0952e-02,\n",
              "         5.1815e+00,  9.0973e-02,  5.2644e-01,  4.8009e-01, -2.7487e+10,\n",
              "         5.3759e-01,  4.7722e-01,  7.5000e-02,  6.7563e-02,  9.0973e-02,\n",
              "         5.2720e-01,  9.0952e-02,  5.3058e-01,  5.1724e-01,  5.1364e-01,\n",
              "         9.0973e-02,  7.7627e-02,  5.3583e-01, -9.8881e+02,  5.2714e-01,\n",
              "         5.1683e-01,  5.3340e-01,  9.0952e-02,  5.2644e-01,  7.7653e-02,\n",
              "         5.1724e-01,  5.3759e-01,  5.3602e-01,  9.0418e-02,  5.3602e-01,\n",
              "         7.7556e-02, -4.9233e+04,  3.9866e+00, -4.4482e+00,  5.3602e-01,\n",
              "         4.8604e-02,  5.3761e-01,  5.3728e-01,  9.0952e-02,  4.1136e-01,\n",
              "         5.3759e-01,  9.0973e-02,  5.3591e-01,  4.7938e-01,  4.8097e-01,\n",
              "         5.1683e-01,  7.6119e-02,  5.2644e-01, -2.5074e+01,  5.3602e-01,\n",
              "         9.0973e-02,  9.0956e-02,  5.2652e-01,  5.2481e-01, -1.1318e+06,\n",
              "         5.1666e-01,  5.2720e-01,  5.1665e-01,  9.0952e-02,  7.6119e-02,\n",
              "         7.7653e-02,  7.6121e-02, -3.0503e+00,  4.7722e-01,  5.2901e-01,\n",
              "         5.2907e-01,  9.0952e-02,  5.1893e-01,  5.3058e-01,  5.2901e-01,\n",
              "         5.2900e-01, -1.1318e+06,  5.1728e-01,  5.1365e-01,  4.7708e-01,\n",
              "         7.7243e-02,  9.0976e-02, -2.0596e+05,  3.9834e-01, -1.1694e+00,\n",
              "        -7.6578e+01,  5.1632e-01,  5.3444e-01,  7.7627e-02,  5.2653e-01,\n",
              "         7.7633e-02, -1.2990e-01,  5.2481e-01,  4.2142e-01,  4.7708e-01,\n",
              "         5.1298e-01,  9.0948e-02,  3.3077e-01,  5.1357e-01,  7.6361e-02,\n",
              "         5.3771e-01,  5.3602e-01,  8.5267e-02,  5.1411e-01,  3.3376e+00,\n",
              "         5.3602e-01,  9.0952e-02,  9.0952e-02,  5.3761e-01,  5.3761e-01,\n",
              "         5.2720e-01,  9.0952e-02,  5.1364e-01,  5.1665e-01, -8.2078e+00,\n",
              "         5.1357e-01,  9.0656e-02,  5.1631e-01,  9.0433e-02,  8.3283e-02,\n",
              "         7.6361e-02,  5.3773e-01,  5.1318e-01,  9.0657e-02,  7.7619e-02,\n",
              "         5.1709e-01,  4.9870e-01,  8.1049e-02,  5.3602e-01,  5.3583e-01,\n",
              "         5.1414e-01,  5.1366e-01,  9.0436e-02,  4.3569e-01,  5.3408e-01,\n",
              "        -1.1694e+00,  5.1357e-01,  5.3005e-01, -1.1318e+06,  5.3771e-01,\n",
              "         5.3602e-01, -2.5074e+01,  5.1428e-01,  9.0952e-02,  9.0952e-02,\n",
              "         1.3058e+01,  5.1428e-01,  8.2442e-02,  5.1364e-01,  7.7574e-02,\n",
              "         4.7995e-01,  5.3602e-01,  6.9435e-02,  9.0657e-02,  9.0973e-02,\n",
              "         5.1709e-01,  5.3404e-01,  7.7653e-02,  5.1357e-01, -1.1658e+06,\n",
              "         3.5049e+00,  4.5514e-01,  5.1665e-01,  5.1047e-01, -7.2190e+10,\n",
              "         5.2653e-01, -2.0584e-01,  5.3602e-01,  5.3544e-01, -2.1687e+00,\n",
              "         5.3773e-01,  7.6361e-02,  5.2644e-01,  5.1770e-01,  4.3616e-01,\n",
              "        -3.4605e+02,  3.8834e-01, -5.7075e-01,  5.3444e-01,  4.3616e-01,\n",
              "         5.2724e-01,  5.4799e+00,  5.2993e-01,  8.9459e-02,  3.9866e+00,\n",
              "         9.0952e-02,  5.3761e-01, -2.2314e+00,  5.1792e-01,  4.7939e-01,\n",
              "         4.3616e-01,  8.9367e-02,  5.1683e-01, -5.4331e+00,  5.2973e-01,\n",
              "         5.1367e-01,  5.3602e-01,  9.0954e-02,  7.6118e-02,  5.1357e-01,\n",
              "         5.3759e-01,  4.3616e-01,  5.3408e-01,  5.3761e-01,  5.1414e-01,\n",
              "         1.3196e+00,  5.3591e-01, -7.5061e-02,  4.8096e-01,  7.6361e-02,\n",
              "        -4.6237e+01,  5.1364e-01,  4.1946e+00,  5.2481e-01,  5.1357e-01,\n",
              "         9.0976e-02,  5.1419e-01,  5.3602e-01,  5.2638e-01,  5.2644e-01,\n",
              "         5.2720e-01, -1.3566e+07, -2.2910e+00,  5.1367e-01,  4.2555e+00,\n",
              "         5.3759e-01,  5.3759e-01,  7.7653e-02,  4.2142e-01,  7.4973e-02,\n",
              "         2.5229e-01,  7.6121e-02,  4.3569e-01,  9.0657e-02,  5.3058e-01,\n",
              "         5.2652e-01,  4.7938e-01,  5.3762e-01,  5.2951e-01,  5.1709e-01,\n",
              "        -4.4662e-01, -6.8967e+05,  5.1605e-01,  5.2973e-01, -2.7469e+03,\n",
              "         9.0952e-02,  3.9321e+00,  5.2907e-01,  5.1297e-01,  5.1815e+00,\n",
              "         5.1411e-01, -2.0584e-01,  7.7633e-02,  5.1357e-01,  5.1357e-01,\n",
              "         4.4238e+00, -2.6895e+01,  5.1562e-01,  7.6361e-02,  5.3544e-01,\n",
              "         5.1357e-01,  5.2993e-01,  5.2653e-01,  5.1665e-01,  5.1711e-01,\n",
              "         5.3771e-01,  5.1364e-01,  5.1365e-01,  5.3583e-01,  9.0431e-02,\n",
              "         5.3759e-01,  9.0657e-02, -4.7566e-02,  3.6920e+00,  5.2652e-01,\n",
              "         5.2799e-01,  5.3728e-01, -5.2253e+15,  5.1709e-01,  5.3773e-01,\n",
              "         5.3544e-01,  7.6356e-02,  5.1631e-01,  5.1685e-01, -2.7487e+10,\n",
              "         7.7619e-02,  5.2644e-01,  5.1366e-01,  5.3759e-01,  5.2720e-01,\n",
              "         9.0952e-02,  8.6001e-02, -6.6563e+03, -3.5506e+00,  5.2973e-01,\n",
              "         5.3602e-01,  5.3759e-01,  5.3759e-01,  7.5080e-02,  3.0557e+00,\n",
              "        -2.5344e-01,  5.3762e-01,  9.0952e-02,  5.2901e-01,  4.7722e-01,\n",
              "         7.6356e-02,  5.1357e-01,  7.7653e-02,  5.1325e-01,  7.7556e-02,\n",
              "         1.3330e+00,  7.6104e-02,  5.2644e-01,  5.2901e-01,  7.6752e-02,\n",
              "         5.1711e-01,  9.0952e-02, -1.1708e+00,  5.2900e-01, -3.2846e+01,\n",
              "         5.2600e-01, -3.3918e-01,  5.1357e-01,  7.7653e-02,  7.4962e-02,\n",
              "         3.9866e+00,  9.0948e-02,  5.3773e-01,  5.3771e-01,  4.7939e-01,\n",
              "         5.1419e-01,  5.3761e-01,  5.7605e+00,  3.8131e-01,  5.2644e-01,\n",
              "         9.0973e-02,  9.0952e-02, -1.8276e-02,  7.6361e-02,  5.1297e-01,\n",
              "         3.9321e+00,  5.2653e-01,  9.0952e-02,  4.2555e+00,  5.1367e-01,\n",
              "         7.7619e-02,  9.0952e-02,  5.2481e-01,  5.1444e-01,  5.2900e-01,\n",
              "         4.3569e-01, -1.0532e+07,  5.1419e-01,  9.0955e-02,  7.7246e-02,\n",
              "         5.2920e-01, -1.1318e+06,  5.1683e-01,  5.1357e-01,  5.1357e-01,\n",
              "         7.4989e-02,  4.2595e+00,  7.6361e-02,  5.1709e-01,  5.1728e-01,\n",
              "         5.3761e-01,  9.0952e-02,  4.0053e-01,  8.1049e-02,  4.7650e-01,\n",
              "         4.2840e-01,  9.0952e-02,  5.2644e-01,  5.3588e-01,  5.1411e-01,\n",
              "         5.1709e-01,  5.2714e-01,  7.6361e-02,  5.3771e-01,  9.0431e-02,\n",
              "         5.1364e-01,  9.0973e-02,  5.1411e-01, -8.0030e+06,  5.1893e-01,\n",
              "         9.0952e-02,  5.1357e-01,  5.1711e-01,  5.3005e-01,  5.1366e-01,\n",
              "         4.7655e-01,  5.3759e-01,  8.4499e-02,  5.2720e-01,  9.0955e-02,\n",
              "         5.1357e-01, -1.8447e-01,  9.0433e-02,  5.1366e-01,  5.2721e-01,\n",
              "         9.0656e-02,  5.3773e-01,  5.1364e-01,  5.2644e-01,  5.3055e-01,\n",
              "         5.3771e-01,  5.3602e-01,  5.2631e-01,  5.1665e-01, -8.5441e+00,\n",
              "         4.7468e-01,  9.0973e-02, -6.4931e+01,  1.9185e+00,  9.0952e-02,\n",
              "         8.3283e-02,  5.1357e-01, -9.2283e-01,  5.2720e-01,  5.2720e-01,\n",
              "         4.8096e-01,  5.1357e-01, -9.3440e+14, -7.3013e+01,  5.2900e-01,\n",
              "         9.0952e-02,  7.7654e-02,  5.2900e-01,  5.1357e-01,  5.3547e-01,\n",
              "         5.0987e-01,  4.3616e-01,  9.0657e-02,  5.3583e-01,  5.1770e-01,\n",
              "         5.3591e-01,  7.7246e-02, -3.6693e+06,  5.3773e-01,  4.3569e-01,\n",
              "         4.8324e-01,  4.7938e-01,  5.2714e-01,  7.6345e-02,  7.7243e-02,\n",
              "        -2.4566e+05,  7.6118e-02,  5.3771e-01,  5.2907e-01,  9.0952e-02,\n",
              "         5.2644e-01,  9.0976e-02,  9.0952e-02,  9.0954e-02, -3.7703e-01,\n",
              "         5.1357e-01,  5.1411e-01,  7.7653e-02,  5.1665e-01,  9.0952e-02,\n",
              "         5.3773e-01,  5.1364e-01,  5.2644e-01,  5.1364e-01,  1.0034e+00,\n",
              "         7.7653e-02,  5.1297e-01,  5.4839e+00,  5.1728e-01,  5.3759e-01,\n",
              "        -2.6111e+01,  4.7939e-01,  5.2644e-01,  2.7664e-01,  5.1477e-01,\n",
              "         7.6356e-02,  5.3588e-01,  9.0973e-02,  9.0952e-02,  5.3583e-01,\n",
              "         5.3340e-01,  5.1357e-01,  5.3761e-01,  9.0957e-02,  4.1946e+00,\n",
              "         5.1365e-01,  5.3602e-01,  5.1683e-01,  4.3616e-01, -8.0030e+06,\n",
              "         4.3569e-01,  7.7556e-02,  9.0952e-02,  5.1665e-01,  5.1782e-01,\n",
              "         3.3888e+00,  5.1357e-01,  5.1562e-01,  5.1683e-01,  5.3602e-01,\n",
              "         5.1666e-01,  5.3759e-01, -1.5824e+02,  5.1357e-01,  5.2644e-01,\n",
              "        -5.4331e+00,  7.6361e-02,  5.3591e-01,  9.0433e-02,  5.2653e-01,\n",
              "         5.2720e-01,  7.7618e-02,  5.6180e+00,  5.1325e-01,  5.1711e-01,\n",
              "         3.6519e-01,  4.3561e-01,  7.4989e-02,  9.0952e-02,  5.2653e-01,\n",
              "         7.7637e-02,  5.3773e-01, -5.7075e-01,  9.0973e-02,  5.1365e-01,\n",
              "         5.3773e-01,  9.0431e-02,  5.2720e-01,  5.3773e-01, -6.4931e+01,\n",
              "         7.7633e-02,  5.3728e-01,  7.7653e-02,  5.1665e-01,  5.2481e-01,\n",
              "        -1.1658e+06,  7.6356e-02, -5.6433e-01,  4.3616e-01,  5.1444e-01,\n",
              "         5.1366e-01,  5.3759e-01,  5.4839e+00,  5.3602e-01,  4.7939e-01,\n",
              "         5.2644e-01,  5.2720e-01,  3.6919e+00,  9.0952e-02,  5.3762e-01,\n",
              "         5.1414e-01,  4.2835e-01,  9.0952e-02,  5.1782e-01,  7.7653e-02,\n",
              "         5.3773e-01,  5.3759e-01, -7.8570e-01,  9.0952e-02,  5.3759e-01,\n",
              "         7.5382e-02,  8.9367e-02, -4.4482e+00,  4.7938e-01,  5.1357e-01,\n",
              "         9.0656e-02, -2.9977e+06,  5.1893e-01,  3.9951e-01,  7.7633e-02,\n",
              "         7.7619e-02, -8.5441e+00,  5.3773e-01,  4.7453e-01,  5.1724e-01,\n",
              "         5.1665e-01,  5.1683e-01,  5.1357e-01, -5.4331e+00,  5.3420e-01,\n",
              "         5.3773e-01, -3.7703e-01,  5.2952e-01,  5.1357e-01,  1.9632e-01,\n",
              "         9.0952e-02,  7.7653e-02,  8.9262e-02,  5.2953e-01,  9.0973e-02,\n",
              "         5.3583e-01,  1.3058e+01,  5.2952e-01,  5.1370e-01,  5.3547e-01,\n",
              "         5.2644e-01,  9.0952e-02,  5.2652e-01,  5.1364e-01,  5.2656e-01,\n",
              "         5.2724e-01,  5.1357e-01,  9.0973e-02,  9.0952e-02,  7.6119e-02,\n",
              "         5.3602e-01,  5.3444e-01, -2.2910e+00,  5.3773e-01,  7.7654e-02,\n",
              "        -2.2579e-01,  4.6518e-01,  4.3561e-01,  5.1683e-01,  4.1597e-01,\n",
              "         9.0657e-02,  9.0436e-02,  7.7246e-02,  9.0436e-02,  7.7556e-02,\n",
              "         5.2714e-01,  5.1357e-01,  5.1367e-01,  5.2720e-01, -3.8281e+02,\n",
              "         5.2973e-01,  5.2644e-01,  9.0954e-02,  5.3537e-01,  4.7708e-01,\n",
              "         1.3330e+00,  5.1356e-01,  4.7938e-01,  9.0952e-02,  9.0657e-02,\n",
              "         5.2644e-01,  9.0952e-02, -3.0503e+00,  4.4394e-01,  9.0973e-02,\n",
              "         5.3583e-01,  5.1357e-01,  5.2951e-01,  3.3888e+00,  4.2835e-01,\n",
              "         7.7653e-02,  7.6119e-02,  9.0973e-02,  5.3761e-01,  5.2959e-01,\n",
              "         9.0952e-02,  5.3583e-01,  5.1724e-01,  5.3404e-01,  5.1357e-01,\n",
              "         7.7633e-02,  7.5080e-02,  5.3340e-01,  9.0955e-02,  9.0974e-02,\n",
              "         9.0974e-02,  5.1683e-01, -4.9233e+04,  5.3771e-01,  5.3110e-01,\n",
              "         5.1364e-01, -3.6693e+06,  8.3283e-02,  5.2481e-01, -3.4611e+00,\n",
              "        -1.3996e+03,  5.2959e-01,  5.2720e-01,  8.3283e-02,  7.7653e-02,\n",
              "         5.1355e-01,  5.1412e-01,  5.3761e-01, -6.6563e+03,  5.3759e-01,\n",
              "         4.7938e-01,  5.3583e-01,  5.1683e-01,  9.0957e-02,  7.6361e-02,\n",
              "         5.1357e-01,  9.0973e-02, -1.3134e+06,  4.2840e-01,  9.0436e-02,\n",
              "         5.2720e-01,  3.0557e+00,  5.2901e-01,  5.3602e-01,  5.1297e-01,\n",
              "         5.1504e-01,  5.1411e-01,  5.1419e-01,  5.1414e-01,  5.3762e-01,\n",
              "         7.6119e-02,  9.0955e-02, -3.0039e+04,  5.1365e-01,  3.5049e+00],\n",
              "       dtype=torch.float64, grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sums_shaping = sums_states_weight_diff + gamma_weights_states_last_sub_states_first"
      ],
      "metadata": {
        "id": "cQwdg78It-sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(sums_shaping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpNY7q-guFCA",
        "outputId": "21738905-c107-4f20-9480-a0e1cead242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-9.7904e+39, dtype=torch.float64, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the min and max values of tensors\n",
        "print(\"padded_weight_diff_tensors - Min:\", padded_weight_diff_tensors.min().item(), \" Max:\", padded_weight_diff_tensors.max().item())\n",
        "print(\"sums_states_weight_diff - Min:\", sums_states_weight_diff.min().item(), \" Max:\", sums_states_weight_diff.max().item())\n",
        "print(\"gamma_weights_last_tensor - Min:\", gamma_weights_last_tensor.min().item(), \" Max:\", gamma_weights_last_tensor.max().item())\n",
        "print(\"samples_IS - Min:\", samples_IS.min().item(), \" Max:\", samples_IS.max().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RZCBfD9vwRx",
        "outputId": "ebaecd5c-ec6a-430d-f578-ee4fc5fe8ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded_weight_diff_tensors - Min: -6979572071212940.0  Max: 1.839130399958296e+16\n",
            "sums_states_weight_diff - Min: -5225342891061462.0  Max: 1365487.6953395614\n",
            "gamma_weights_last_tensor - Min: 1.846463839642936e-238  Max: 143894458319084.28\n",
            "samples_IS - Min: -10076.89890923087  Max: 159882731465649.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(sample_sums_states_weight_diff,dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj6mSyUmUqqp",
        "outputId": "21a53d42-836a-4dc8-d67f-ba381264b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-9.9641e+39, -2.6692e+27, -9.9641e+39,  ..., -1.9928e+40,\n",
              "        -2.9892e+40, -1.3346e+27], dtype=torch.float64,\n",
              "       grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(samples_gamma_weight_states_last_sub_states_first, dim =1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYJ0CkAeUxYJ",
        "outputId": "9c9fe1d0-8246-427a-e25a-4704db91823d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7368e+38, 5.5852e+13, 1.7368e+38,  ..., 3.4735e+38, 5.2103e+38,\n",
              "        2.7925e+13], dtype=torch.float64, grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpf1bgqIRAzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calc Variance"
      ],
      "metadata": {
        "id": "wwmn8QMhO5QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_shaped_variance(samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE):\n",
        "\n",
        "  # states_output, states_first_output, states_last_output = pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)\n",
        "\n",
        "  # Begin calcs without clamping\n",
        "\n",
        "  # IS\n",
        "  E_IS_sq = torch.mean(torch.mean(samples_IS, dim = 1)**2)\n",
        "  E_IS_all_sq = torch.mean(torch.mean(samples_IS, dim = 1))**2\n",
        "\n",
        "  # states_weight_diff\n",
        "  E_s_wdiff_sq = torch.mean(torch.mean(sample_sums_states_weight_diff, dim =1)**2)\n",
        "  E_s_wdiff_all_sq = torch.mean(torch.mean(sample_sums_states_weight_diff, dim = 1))**2\n",
        "\n",
        "  # all terms\n",
        "  # SCOPE = sample_sums_states_weight_diff+samples_gamma_weight_states_last_sub_states_first\n",
        "  # E_IS_SCOPE = torch.mean(torch.mean(samples_IS, dim =1) * torch.mean(SCOPE, dim =1))\n",
        "  # E_IS_E_SCOPE = torch.mean(torch.mean(samples_IS,dim = 1 ))*torch.mean(torch.mean(SCOPE, dim =1))\n",
        "\n",
        "  E_IS_SCOPE = torch.mean(torch.mean(samples_IS_SCOPE, dim =1))\n",
        "  E_IS_E_SCOPE = torch.mean(torch.mean(samples_IS,dim = 1 )) * torch.mean(torch.mean(samples_all_shaping, dim =1))\n",
        "\n",
        "\n",
        "\n",
        "  # SCOPE_variance = E_IS_sq + 2*E_IS_SCOPE + E_s_wdiff_sq - E_IS_all_sq - 2*E_IS_E_SCOPE - E_IS_E_SCOPE\n",
        "  SCOPE_variance = E_IS_sq + 2*E_IS_SCOPE + E_s_wdiff_sq - E_IS_all_sq - 2*E_IS_E_SCOPE - E_s_wdiff_all_sq\n",
        "\n",
        "  IS_variance = E_IS_sq - E_IS_all_sq\n",
        "\n",
        "  return E_IS_sq, E_IS_all_sq, E_s_wdiff_sq, E_s_wdiff_all_sq, E_IS_SCOPE, E_IS_E_SCOPE, IS_variance, SCOPE_variance\n"
      ],
      "metadata": {
        "id": "WlDazbYPvvrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPE = sample_sums_states_weight_diff-samples_gamma_weight_states_last_sub_states_first\n"
      ],
      "metadata": {
        "id": "ut3fyQ4s1l-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,_,_,_,_,IS_variance, SCOPE_variance = calculate_shaped_variance(samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE)"
      ],
      "metadata": {
        "id": "yWcJ_iSFvvru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'IS Variance: {IS_variance.item()} SCOPE Variance: {SCOPE_variance.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ecraXkwWRcI",
        "outputId": "0b7a14e9-64a6-4c20-98a0-f958228fad62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS Variance: 2.514807940970177e+22 SCOPE Variance: 5.892057226414652e+25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPE_variance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJtda6na7iFY",
        "outputId": "ce79b72f-e4fc-4ab7-c2b7-6e09fb2db4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.0262e+82, 1.7582e+80, 2.0262e+82,  ..., 4.0349e+82, 6.0435e+82,\n",
              "        1.7582e+80], dtype=torch.float64, grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing"
      ],
      "metadata": {
        "id": "InzaU1hEzXaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "wEeNxHjkz9Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_var(model, num_epochs, learning_rate, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1):\n",
        "    model.train()\n",
        "\n",
        "    # Enable anomaly detection\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Forward pass\n",
        "        states_output, states_first_output, states_last_output = test1.pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)\n",
        "        sums_states_weight_diff = test1.states_weight_diff_sums(states_output, padded_weight_diff_tensors)\n",
        "        gamma_weights_states_last_sub_states_first = test1.last_first_terms_operations(gamma_weights_last_tensor, states_last_output, states_first_output)\n",
        "        # sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE = test1.bootstrap_shaping_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor)\n",
        "\n",
        "        samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE = test1.bootstrap_all_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor)\n",
        "\n",
        "\n",
        "        E_IS_sq, E_IS_all_sq, E_s_wdiff_sq, E_s_wdiff_all_sq, E_IS_SCOPE, E_IS_E_SCOPE, _, variance_loss = calculate_shaped_variance(samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        print(\"Var loss: \", variance_loss)\n",
        "\n",
        "        # Print each term\n",
        "        print(f\"E_IS_sq: {E_IS_sq}\")\n",
        "        print(f\"E_IS_all_sq: {E_IS_all_sq}\")\n",
        "        print(f\"E_s_wdiff_sq: {E_s_wdiff_sq}\")\n",
        "        print(f\"E_s_wdiff_all_sq: {E_s_wdiff_all_sq}\")\n",
        "        print(f\"E_IS_SCOPE: {E_IS_SCOPE}\")\n",
        "        print(f\"E_IS_E_SCOPE: {E_IS_E_SCOPE}\")\n",
        "\n",
        "        tot = variance_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Retain the graph to avoid clearing it before backward pass\n",
        "        tot.backward(retain_graph=True)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += tot.item()\n",
        "\n",
        "        print(f\"Total Loss: {total_loss}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Disable anomaly detection after running the code\n",
        "    torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(f\"Parameter name: {name}\")\n",
        "            print(f\"Weights: {param.data}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "lEfGUHfGNwiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = train_var(model, 20, 0.0001, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0zmGSWkVyC_",
        "outputId": "18597fc2-7ea7-4000-98c0-75ccad13417b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Var loss:  tensor(-3.0838e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 5.210967549166126e+22\n",
            "E_s_wdiff_all_sq: 2.5172984363039316e+22\n",
            "E_IS_SCOPE: -1.5459488896936224e+25\n",
            "E_IS_E_SCOPE: -1.4417566494849713e+22\n",
            "Total Loss: -3.0838057890344423e+25\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "Var loss:  tensor(-3.3585e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 6.546944000346388e+22\n",
            "E_s_wdiff_all_sq: 3.517525168586692e+22\n",
            "E_IS_SCOPE: -1.6839284068856547e+25\n",
            "E_IS_E_SCOPE: -1.8983210591791954e+22\n",
            "Total Loss: -3.358515944880221e+25\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "Var loss:  tensor(-3.6329e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 8.127328468140865e+22\n",
            "E_s_wdiff_all_sq: 4.669789629828819e+22\n",
            "E_IS_SCOPE: -1.8217910544502339e+25\n",
            "E_IS_E_SCOPE: -2.3493888489215494e+22\n",
            "Total Loss: -3.632910984423342e+25\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "Var loss:  tensor(-3.9072e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 9.956227907220597e+22\n",
            "E_s_wdiff_all_sq: 5.9793486972912325e+22\n",
            "E_IS_SCOPE: -1.959656202176037e+25\n",
            "E_IS_E_SCOPE: -2.7986036228854552e+22\n",
            "Total Loss: -3.907223509955403e+25\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "Var loss:  tensor(-4.1814e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2030804144930232e+23\n",
            "E_s_wdiff_all_sq: 7.444645256087994e+22\n",
            "E_IS_SCOPE: -2.0974953048619344e+25\n",
            "E_IS_E_SCOPE: -3.2460902003401042e+22\n",
            "Total Loss: -4.1813974624933755e+25\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "Var loss:  tensor(-4.4554e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.4346387559415976e+23\n",
            "E_s_wdiff_all_sq: 9.062655780558345e+22\n",
            "E_IS_SCOPE: -2.235296219189662e+25\n",
            "E_IS_E_SCOPE: -3.6915356807178387e+22\n",
            "Total Loss: -4.4554108272980605e+25\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "Var loss:  tensor(-4.7293e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.6896450448835388e+23\n",
            "E_s_wdiff_all_sq: 1.08290772380101e+23\n",
            "E_IS_SCOPE: -2.3730518621395594e+25\n",
            "E_IS_E_SCOPE: -4.134422009246762e+22\n",
            "Total Loss: -4.72925269910883e+25\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "Var loss:  tensor(-5.0029e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.9673881915757514e+23\n",
            "E_s_wdiff_all_sq: 1.2739332729182658e+23\n",
            "E_IS_SCOPE: -2.5107576940337e+25\n",
            "E_IS_E_SCOPE: -4.574329549359406e+22\n",
            "Total Loss: -5.0029173718411355e+25\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "Var loss:  tensor(-5.2764e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.267249722521212e+23\n",
            "E_s_wdiff_all_sq: 1.4789613654870718e+23\n",
            "E_IS_SCOPE: -2.6484105436396702e+25\n",
            "E_IS_E_SCOPE: -5.011115472982626e+22\n",
            "Total Loss: -5.276401164822063e+25\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "Var loss:  tensor(-5.5497e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.5887526111029046e+23\n",
            "E_s_wdiff_all_sq: 1.6977134979120384e+23\n",
            "E_IS_SCOPE: -2.786008235633142e+25\n",
            "E_IS_E_SCOPE: -5.444868998604533e+22\n",
            "Total Loss: -5.549701534196196e+25\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "Var loss:  tensor(-5.8228e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.93150580846501e+23\n",
            "E_s_wdiff_all_sq: 1.9299681728485064e+23\n",
            "E_IS_SCOPE: -2.9235491360209344e+25\n",
            "E_IS_E_SCOPE: -5.875756424517042e+22\n",
            "Total Loss: -5.822816574895699e+25\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "Var loss:  tensor(-6.0957e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 3.2951384287536245e+23\n",
            "E_s_wdiff_all_sq: 2.1755135214562837e+23\n",
            "E_IS_SCOPE: -3.061031877917925e+25\n",
            "E_IS_E_SCOPE: -6.303918086962085e+22\n",
            "Total Loss: -6.095744862647982e+25\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "Var loss:  tensor(-6.3685e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 3.67926590345648e+23\n",
            "E_s_wdiff_all_sq: 2.434124883277089e+23\n",
            "E_IS_SCOPE: -3.1984552495154813e+25\n",
            "E_IS_E_SCOPE: -6.729439799700456e+22\n",
            "Total Loss: -6.368485401288798e+25\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "Var loss:  tensor(-6.6411e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 4.083511343418876e+23\n",
            "E_s_wdiff_all_sq: 2.7055798170633747e+23\n",
            "E_IS_SCOPE: -3.3358493269605055e+25\n",
            "E_IS_E_SCOPE: -7.152390355712248e+22\n",
            "Total Loss: -6.641099750005061e+25\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "Var loss:  tensor(-6.9135e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 4.507797613123608e+23\n",
            "E_s_wdiff_all_sq: 2.9898588786408246e+23\n",
            "E_IS_SCOPE: -3.47318020008451e+25\n",
            "E_IS_E_SCOPE: -7.573118707256295e+22\n",
            "Total Loss: -6.91351996746871e+25\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "Var loss:  tensor(-7.1858e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 4.9525968530722806e+23\n",
            "E_s_wdiff_all_sq: 3.287304205334282e+23\n",
            "E_IS_SCOPE: -3.6104516466931023e+25\n",
            "E_IS_E_SCOPE: -7.992410040288414e+22\n",
            "Total Loss: -7.185750738887278e+25\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "Var loss:  tensor(-7.4578e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 5.422597581911685e+23\n",
            "E_s_wdiff_all_sq: 3.6010637596031907e+23\n",
            "E_IS_SCOPE: -3.7476645794703584e+25\n",
            "E_IS_E_SCOPE: -8.414638867962763e+22\n",
            "Total Loss: -7.457769735040736e+25\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "Var loss:  tensor(-7.7296e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 5.912088932901162e+23\n",
            "E_s_wdiff_all_sq: 3.9273065020191513e+23\n",
            "E_IS_SCOPE: -3.884814950188801e+25\n",
            "E_IS_E_SCOPE: -8.834554946761315e+22\n",
            "Total Loss: -7.7295981582342884e+25\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "Var loss:  tensor(-8.0012e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 6.420042041666823e+23\n",
            "E_s_wdiff_all_sq: 4.2653734809269504e+23\n",
            "E_IS_SCOPE: -4.021901783374139e+25\n",
            "E_IS_E_SCOPE: -9.251635569965136e+22\n",
            "Total Loss: -8.001238802059979e+25\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "Var loss:  tensor(-8.2727e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 6.945934268931316e+23\n",
            "E_s_wdiff_all_sq: 4.614945900147421e+23\n",
            "E_IS_SCOPE: -4.158924544997166e+25\n",
            "E_IS_E_SCOPE: -9.665836613397322e+22\n",
            "Total Loss: -8.272692725138728e+25\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.4510,  0.0021],\n",
            "        [ 0.3396,  0.6556],\n",
            "        [-0.0427,  0.1722],\n",
            "        [-0.1050, -0.3208],\n",
            "        [ 0.6667,  0.6256],\n",
            "        [ 0.0633, -0.0869],\n",
            "        [-0.5012,  0.5401],\n",
            "        [-0.2295, -0.6730],\n",
            "        [-0.2369, -0.6463],\n",
            "        [-0.3752, -0.3359],\n",
            "        [-0.6697,  0.5156],\n",
            "        [-0.2812,  0.2000],\n",
            "        [-0.1604,  0.5577],\n",
            "        [-0.5985, -0.1328],\n",
            "        [-0.4052, -0.3473],\n",
            "        [-0.2117,  0.0203]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.5800,  0.0521,  0.6555,  0.4989, -0.2725,  0.2633, -0.4585,  0.6680,\n",
            "         0.4422,  0.6497,  0.6227,  0.4099, -0.5805,  0.7856,  0.5691,  0.2211],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 1.7559e-01,  1.6233e-01,  2.3772e-01,  2.2629e-01,  1.8467e-01,\n",
            "          3.0120e-01, -2.3406e-01, -1.8666e-01,  7.2061e-02,  2.3042e-01,\n",
            "         -3.1664e-02, -1.7317e-01, -5.9300e-02,  2.1983e-01, -2.3025e-01,\n",
            "         -2.5168e-01],\n",
            "        [ 7.1529e-03,  2.4635e-01, -1.0106e-01, -1.2429e-01, -1.5923e-01,\n",
            "         -1.8235e-02,  1.4430e-02, -1.5267e-01,  1.9970e-01, -2.2146e-02,\n",
            "         -2.2856e-01, -1.7931e-01,  1.9001e-01,  2.4006e-02,  2.1272e-01,\n",
            "          9.1185e-02],\n",
            "        [-6.1545e-02, -1.6846e-01, -1.9149e-01,  9.8640e-02, -2.1220e-01,\n",
            "          1.1819e-01,  9.9572e-02, -1.0969e-01,  1.8555e-01,  2.3056e-01,\n",
            "         -7.3723e-02, -6.4354e-02,  1.8917e-01,  1.4870e-01, -1.2193e-01,\n",
            "          1.9635e-01],\n",
            "        [ 1.9140e-01,  5.9702e-02,  2.0254e-01, -5.3336e-02, -1.2643e-01,\n",
            "          1.1372e-01, -2.2702e-01, -1.5411e-01,  1.7170e-01, -1.0972e-01,\n",
            "         -6.0289e-02,  1.4019e-01, -1.7271e-01,  3.9220e-01, -8.3202e-02,\n",
            "         -1.3331e-01],\n",
            "        [-2.2063e-01, -2.3476e-01, -2.0329e-01,  1.1068e-02,  2.1729e-01,\n",
            "         -2.8234e-01,  8.6570e-02, -9.5660e-02, -1.9528e-01,  8.0604e-02,\n",
            "          7.2492e-02, -1.7119e-01, -1.3159e-01,  9.6841e-02,  1.2702e-01,\n",
            "         -2.1615e-01],\n",
            "        [-1.5366e-01,  1.9952e-01, -3.9314e-02, -1.4778e-01, -1.5110e-01,\n",
            "          8.4972e-02, -2.8361e-01,  6.8140e-02, -1.6945e-01,  2.4092e-02,\n",
            "         -1.8115e-01, -2.7334e-02, -3.8245e-02,  7.6595e-02, -6.4935e-02,\n",
            "          2.7648e-01],\n",
            "        [ 7.1929e-03, -8.6183e-02,  1.9110e-01,  1.1232e-01,  1.7308e-01,\n",
            "         -5.8804e-02, -2.2953e-01, -2.3573e-02, -2.7324e-02,  1.1141e-01,\n",
            "         -2.1644e-01,  2.0180e-01,  1.1418e-02, -1.7533e-01,  1.6569e-01,\n",
            "          1.9179e-01],\n",
            "        [ 3.6563e-02,  2.7998e-02,  2.6304e-01, -1.8567e-01,  1.8540e-01,\n",
            "         -1.3270e-01,  7.6032e-02, -2.9636e-02, -2.1079e-04, -1.0609e-01,\n",
            "          2.2287e-01, -2.2479e-01,  1.1056e-01, -2.3385e-01, -1.6760e-01,\n",
            "          9.9343e-02],\n",
            "        [ 1.7877e-01, -1.1848e-01,  2.7068e-02,  9.9388e-02,  2.4131e-01,\n",
            "         -1.5313e-01,  2.0315e-01, -1.8985e-01, -1.1247e-01, -1.1768e-02,\n",
            "         -2.8191e-02, -7.7164e-02,  1.1229e-01, -2.7218e-01, -9.0932e-02,\n",
            "         -1.9059e-01],\n",
            "        [ 1.5427e-01, -1.1397e-01,  2.4423e-01,  6.1690e-02, -2.4008e-03,\n",
            "          5.1089e-02,  1.0899e-01,  1.0263e-01, -1.7723e-02,  7.6825e-02,\n",
            "          4.6833e-02,  1.8567e-01,  6.8519e-03, -5.1221e-02,  7.3215e-02,\n",
            "          2.1663e-02],\n",
            "        [ 3.0704e-02,  1.6847e-01,  1.9887e-01, -1.9659e-01,  1.6256e-03,\n",
            "          2.7731e-01, -4.4841e-02, -1.0408e-01, -2.2601e-01, -1.8564e-01,\n",
            "          1.8752e-01, -1.7683e-01, -1.8726e-01,  1.9871e-01, -3.5240e-02,\n",
            "         -1.3091e-01],\n",
            "        [ 3.1929e-02,  3.2215e-02,  1.5373e-01, -2.2811e-01, -5.2812e-02,\n",
            "          1.1565e-01, -1.0283e-01,  1.9907e-01,  3.4308e-02, -1.2375e-01,\n",
            "          1.0030e-01,  7.0572e-02, -7.3479e-02, -5.4203e-02, -4.8761e-02,\n",
            "         -1.7009e-01],\n",
            "        [ 1.8268e-01, -3.1818e-02, -2.7465e-01,  2.0304e-01, -2.5103e-01,\n",
            "          1.3380e-01,  5.7078e-02, -1.6024e-01,  2.3616e-01,  1.3602e-01,\n",
            "          1.8643e-01, -1.7847e-01, -1.6410e-01,  2.1505e-01, -1.4369e-01,\n",
            "         -4.8390e-02],\n",
            "        [-1.3087e-01,  1.1866e-01,  3.6743e-02, -1.4394e-01, -1.1981e-01,\n",
            "         -7.0510e-02,  8.3873e-03,  1.4202e-01,  5.7004e-02,  2.4725e-01,\n",
            "          1.7666e-02, -2.3482e-01, -2.2211e-01,  2.0162e-01,  8.6532e-02,\n",
            "          7.8177e-02],\n",
            "        [-1.4544e-01, -3.7057e-01,  3.0520e-01, -7.4018e-02, -3.4248e-02,\n",
            "         -3.6749e-02, -5.1584e-02, -1.4300e-01,  7.9773e-02, -1.7701e-01,\n",
            "          2.3044e-01, -4.2904e-01, -1.0584e-01, -1.3126e-01,  1.9303e-02,\n",
            "          2.8921e-01],\n",
            "        [-2.4521e-01,  2.4946e-01, -3.0612e-01,  4.3380e-02, -1.0360e-02,\n",
            "          4.2349e-02,  7.6393e-02, -1.4996e-01,  2.6837e-02, -1.2947e-01,\n",
            "         -6.0119e-02, -1.2891e-01, -2.9666e-01,  7.5903e-03,  1.1003e-01,\n",
            "         -1.3607e-01],\n",
            "        [-2.3086e-01,  1.2024e-01, -1.7377e-01,  1.9102e-01, -1.4026e-01,\n",
            "         -2.1704e-01,  2.3345e-01,  1.8437e-01, -3.6461e-02, -2.4583e-01,\n",
            "         -4.0944e-01, -3.2514e-01, -3.4505e-01,  2.2425e-01,  2.4350e-03,\n",
            "          2.9624e-01],\n",
            "        [ 1.7996e-01,  6.2290e-03,  4.7523e-02,  1.1118e-02,  1.6657e-01,\n",
            "         -9.6748e-03,  1.1051e-01, -2.2243e-01,  7.2220e-02, -1.1556e-01,\n",
            "          3.8988e-02, -2.0665e-01,  5.8580e-02,  1.9515e-01, -1.9692e-01,\n",
            "         -1.6146e-01],\n",
            "        [-2.4386e-01, -1.9133e-01,  3.3801e-02, -2.1412e-01,  1.1644e-01,\n",
            "         -3.6942e-01, -1.6242e-01, -1.4523e-01,  1.8902e-01,  8.2447e-02,\n",
            "         -4.7722e-01,  5.2936e-02, -9.7944e-02,  9.3941e-02,  5.6936e-02,\n",
            "         -2.5046e-01],\n",
            "        [-2.1121e-02, -3.0001e-01, -4.9378e-02,  1.1902e-01,  7.8042e-02,\n",
            "          3.1770e-01,  3.9925e-02,  2.2312e-01,  5.2246e-02,  9.3138e-02,\n",
            "          1.1217e-01,  5.3098e-02,  2.2271e-01,  3.9138e-01,  5.3523e-02,\n",
            "         -1.2653e-01],\n",
            "        [ 1.9650e-01, -1.4172e-01,  2.8714e-02, -2.9400e-02,  2.3476e-01,\n",
            "          2.9832e-01,  1.4017e-01,  2.4546e-01,  9.2309e-03,  1.0585e-01,\n",
            "         -1.4108e-01, -1.6467e-01, -2.5441e-01,  1.3023e-01,  5.3941e-02,\n",
            "          3.4543e-02],\n",
            "        [ 6.2436e-02, -8.1427e-02,  6.8004e-03, -2.4202e-01, -6.0287e-02,\n",
            "         -1.4947e-01,  1.9532e-01, -2.4530e-01, -2.7072e-02,  1.3022e-01,\n",
            "          3.4743e-01,  1.9774e-01,  5.8964e-02, -4.1659e-02,  1.8627e-01,\n",
            "         -1.6851e-01],\n",
            "        [-2.3660e-01,  1.4217e-01, -2.7298e-01, -2.3568e-01,  1.8135e-01,\n",
            "         -9.9376e-02, -2.1404e-01, -7.8582e-02, -7.0298e-02, -1.0721e-02,\n",
            "          1.1153e-01,  1.2434e-01,  1.4717e-01,  1.4747e-01,  1.7566e-01,\n",
            "          1.6961e-01],\n",
            "        [-2.1504e-01, -5.6804e-02,  1.2303e-01, -1.2970e-01, -9.0758e-04,\n",
            "          9.0054e-02, -1.4854e-01, -5.5772e-03, -3.3620e-02, -1.3572e-01,\n",
            "         -1.2283e-01, -3.5842e-02,  2.0300e-01,  1.7445e-01,  1.9926e-01,\n",
            "          1.4480e-02],\n",
            "        [-1.8815e-02, -8.8772e-02,  1.6216e-01,  3.7306e-02,  1.0765e-01,\n",
            "         -2.1936e-02,  1.8456e-01,  1.4760e-01, -7.9331e-02,  1.5561e-01,\n",
            "          2.0303e-01, -1.4509e-01, -8.1366e-02, -3.1612e-01,  5.6964e-02,\n",
            "         -1.7146e-01],\n",
            "        [ 2.7233e-04,  2.0508e-01,  2.3661e-01,  3.0674e-02,  1.2631e-01,\n",
            "          7.2449e-02, -2.0767e-01,  6.0481e-02,  5.9943e-02,  1.2595e-01,\n",
            "         -1.5610e-01, -1.2661e-01,  2.0987e-01, -2.9389e-01, -2.3863e-01,\n",
            "          1.8265e-01],\n",
            "        [-2.3910e-01,  2.7046e-01,  1.7394e-01,  1.4304e-01,  2.2724e-01,\n",
            "          1.5284e-01,  6.0089e-02, -2.4203e-02,  1.4016e-01,  1.2150e-01,\n",
            "          1.1739e-01,  4.5788e-02, -2.4074e-01,  7.2400e-02, -1.8276e-01,\n",
            "         -1.9739e-01],\n",
            "        [ 3.0014e-02,  8.0140e-03, -3.5501e-02,  1.3929e-01, -2.8273e-01,\n",
            "         -8.2736e-02, -2.0914e-01, -1.9566e-01,  1.4068e-01, -2.0598e-01,\n",
            "          1.6162e-01,  1.1231e-01,  1.1277e-01, -6.3876e-02, -5.7794e-02,\n",
            "         -4.4601e-02],\n",
            "        [-6.8022e-02,  1.2553e-01, -7.4896e-02,  5.8154e-02,  1.5711e-01,\n",
            "         -2.2227e-01,  1.3049e-01, -1.2746e-01, -2.7043e-02, -7.6503e-03,\n",
            "          2.5763e-02,  7.0767e-02, -5.9477e-03, -4.2300e-01, -9.2563e-02,\n",
            "          1.0574e-01],\n",
            "        [ 2.4462e-01, -4.4803e-02,  6.5335e-02,  2.1805e-01,  2.5980e-01,\n",
            "          1.4863e-02,  1.3684e-01,  1.9241e-01, -1.3083e-01,  1.8042e-01,\n",
            "         -9.6065e-02,  1.9216e-01,  1.2768e-01, -3.9125e-02,  8.0967e-02,\n",
            "          6.9863e-02],\n",
            "        [-2.0339e-01, -1.3332e-01,  1.2844e-02, -2.4017e-01,  1.1610e-01,\n",
            "         -1.6190e-01, -5.8829e-02,  1.3671e-01, -6.2188e-02,  1.3888e-01,\n",
            "          1.7607e-01,  1.4970e-01,  6.7824e-02,  7.6705e-02,  7.5816e-02,\n",
            "         -1.3883e-01],\n",
            "        [-8.8667e-02, -9.7587e-02,  7.9214e-02, -1.6263e-01, -3.5249e-02,\n",
            "          5.5645e-02, -2.3595e-02,  7.8961e-02, -1.2570e-01,  2.1665e-01,\n",
            "          2.7871e-02,  1.2805e-01,  4.8840e-02,  7.5699e-02, -1.5474e-01,\n",
            "         -1.3606e-01]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.0152, -0.0980,  0.0864,  0.0689,  0.2498, -0.0135,  0.1991,  0.1723,\n",
            "         0.1992, -0.0625,  0.1075, -0.0684,  0.2318, -0.0104, -0.2247, -0.0120,\n",
            "        -0.1201,  0.2344,  0.0327,  0.0824,  0.2338,  0.0025, -0.0555, -0.2357,\n",
            "         0.0321, -0.2639,  0.1652,  0.0119,  0.0660,  0.0135, -0.1878, -0.2406],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.0168,  0.1780,  0.1248,  0.0681, -0.1202, -0.0241,  0.1450, -0.0256,\n",
            "         -0.0325, -0.1131,  0.1277, -0.0804,  0.0521,  0.0330,  0.1509,  0.0562,\n",
            "         -0.1326,  0.0699, -0.1098,  0.1518,  0.1084, -0.3186,  0.1366, -0.1010,\n",
            "         -0.0525, -0.0417,  0.0804, -0.0123, -0.1114, -0.0858, -0.1678,  0.1389]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([-0.0005], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = train_var(model4, 500, 0.0001, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K7RunrrFPEy7",
        "outputId": "fa9e6784-1b9a-4a4f-be7b-e7e99b2b6a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Var loss:  tensor(1.6838e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2080531216272573e+22\n",
            "E_s_wdiff_all_sq: 7.854446300022197e+18\n",
            "E_IS_SCOPE: -3.2924307594909997e+22\n",
            "E_IS_E_SCOPE: -1.0101281026055919e+22\n",
            "Total Loss: 1.683838514322106e+21\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "Var loss:  tensor(2.4018e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.5681370975049804e+22\n",
            "E_s_wdiff_all_sq: 5.3651102132735515e+20\n",
            "E_IS_SCOPE: -2.958920317364135e+22\n",
            "E_IS_E_SCOPE: -6.916923741107582e+21\n",
            "Total Loss: 2.401815260791626e+21\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "Var loss:  tensor(2.3126e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.060980670643068e+22\n",
            "E_s_wdiff_all_sq: 2.9444178650483524e+20\n",
            "E_IS_SCOPE: -3.670629845441955e+22\n",
            "E_IS_E_SCOPE: -1.3322424158069409e+22\n",
            "Total Loss: 2.3125616815015744e+21\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "Var loss:  tensor(2.2341e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.0633852358230838e+22\n",
            "E_s_wdiff_all_sq: 2.5690002961311597e+20\n",
            "E_IS_SCOPE: -3.649677342924772e+22\n",
            "E_IS_E_SCOPE: -1.3148560016495734e+22\n",
            "Total Loss: 2.2340649589243705e+21\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "Var loss:  tensor(1.6178e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2198354392569919e+22\n",
            "E_s_wdiff_all_sq: 3.652535509560464e+18\n",
            "E_IS_SCOPE: -3.332237717470749e+22\n",
            "E_IS_E_SCOPE: -1.0305375073970072e+22\n",
            "Total Loss: 1.6178046747669262e+21\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "Var loss:  tensor(2.0155e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.4497548452943698e+22\n",
            "E_s_wdiff_all_sq: 3.101753381008754e+20\n",
            "E_IS_SCOPE: -3.054246607770964e+22\n",
            "E_IS_E_SCOPE: -7.818279936741865e+21\n",
            "Total Loss: 2.0155355174517866e+21\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "Var loss:  tensor(2.0581e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.4616360950367095e+22\n",
            "E_s_wdiff_all_sq: 3.4398198073409634e+20\n",
            "E_IS_SCOPE: -3.036719256598841e+22\n",
            "E_IS_E_SCOPE: -7.676013074519667e+21\n",
            "Total Loss: 2.0580944516510541e+21\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "Var loss:  tensor(1.6603e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2939949932413746e+22\n",
            "E_s_wdiff_all_sq: 7.213751316865191e+19\n",
            "E_IS_SCOPE: -3.213132082516763e+22\n",
            "E_IS_E_SCOPE: -9.278309953212547e+21\n",
            "Total Loss: 1.6603175514178983e+21\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "Var loss:  tensor(1.6789e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1369778160525388e+22\n",
            "E_s_wdiff_all_sq: 1.6735233495303238e+19\n",
            "E_IS_SCOPE: -3.4343132217582195e+22\n",
            "E_IS_E_SCOPE: -1.1282428200323708e+22\n",
            "Total Loss: 1.6788777360338956e+21\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "Var loss:  tensor(1.9160e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.0759040073523123e+22\n",
            "E_s_wdiff_all_sq: 1.146480960753118e+20\n",
            "E_IS_SCOPE: -3.550444204464186e+22\n",
            "E_IS_E_SCOPE: -1.2339266693254665e+22\n",
            "Total Loss: 1.9160354737051664e+21\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "Var loss:  tensor(1.7996e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.0934688991814363e+22\n",
            "E_s_wdiff_all_sq: 6.6358570149114085e+19\n",
            "E_IS_SCOPE: -3.5050020881405305e+22\n",
            "E_IS_E_SCOPE: -1.1938953376668441e+22\n",
            "Total Loss: 1.7995867687108477e+21\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "Var loss:  tensor(1.5873e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.177134347775273e+22\n",
            "E_s_wdiff_all_sq: 2.4829363686499043e+17\n",
            "E_IS_SCOPE: -3.352526279960078e+22\n",
            "E_IS_E_SCOPE: -1.057280545146834e+22\n",
            "Total Loss: 1.5873136426579615e+21\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "Var loss:  tensor(1.6518e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2952111814378758e+22\n",
            "E_s_wdiff_all_sq: 9.31244535538539e+19\n",
            "E_IS_SCOPE: -3.191352877889636e+22\n",
            "E_IS_E_SCOPE: -9.126238020662737e+21\n",
            "Total Loss: 1.6518477282760217e+21\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "Var loss:  tensor(1.7801e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.359938149685174e+22\n",
            "E_s_wdiff_all_sq: 1.9379745607524398e+20\n",
            "E_IS_SCOPE: -3.1165569678760726e+22\n",
            "E_IS_E_SCOPE: -8.454597465652572e+21\n",
            "Total Loss: 1.780113945989774e+21\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "Var loss:  tensor(1.6882e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.3203151331044018e+22\n",
            "E_s_wdiff_all_sq: 1.319710181949291e+20\n",
            "E_IS_SCOPE: -3.1602686497240834e+22\n",
            "E_IS_E_SCOPE: -8.847446901451736e+21\n",
            "Total Loss: 1.688198450619331e+21\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "Var loss:  tensor(1.5562e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2215919712213322e+22\n",
            "E_s_wdiff_all_sq: 2.0899687616620753e+19\n",
            "E_IS_SCOPE: -3.283190220996491e+22\n",
            "E_IS_E_SCOPE: -9.951990093450058e+21\n",
            "Total Loss: 1.5561649823354454e+21\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "Var loss:  tensor(1.6003e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1377950739780977e+22\n",
            "E_s_wdiff_all_sq: 6.728656834605102e+18\n",
            "E_IS_SCOPE: -3.4107211151752986e+22\n",
            "E_IS_E_SCOPE: -1.1096232306045503e+22\n",
            "Total Loss: 1.6003047641132869e+21\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "Var loss:  tensor(1.6798e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1052807949431192e+22\n",
            "E_s_wdiff_all_sq: 3.5202491836908536e+19\n",
            "E_IS_SCOPE: -3.471015741715299e+22\n",
            "E_IS_E_SCOPE: -1.163307656577775e+22\n",
            "Total Loss: 1.6798022221602352e+21\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "Var loss:  tensor(1.6168e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1245231656492002e+22\n",
            "E_s_wdiff_all_sq: 1.5721640040077249e+19\n",
            "E_IS_SCOPE: -3.43792134152922e+22\n",
            "E_IS_E_SCOPE: -1.1327314441601773e+22\n",
            "Total Loss: 1.6168275604146917e+21\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "Var loss:  tensor(1.5268e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.186523423103466e+22\n",
            "E_s_wdiff_all_sq: 2.7415667138166825e+18\n",
            "E_IS_SCOPE: -3.3405647924408474e+22\n",
            "E_IS_E_SCOPE: -1.0441599899703955e+22\n",
            "Total Loss: 1.5268174910313474e+21\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "Var loss:  tensor(1.5536e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.263193783444266e+22\n",
            "E_s_wdiff_all_sq: 5.60570436459224e+19\n",
            "E_IS_SCOPE: -3.2396879704542404e+22\n",
            "E_IS_E_SCOPE: -9.522452985415874e+21\n",
            "Total Loss: 1.5536167913072424e+21\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "Var loss:  tensor(1.6038e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.3038238778747116e+22\n",
            "E_s_wdiff_all_sq: 1.038464977379727e+20\n",
            "E_IS_SCOPE: -3.1941390037369942e+22\n",
            "E_IS_E_SCOPE: -9.100087115917017e+21\n",
            "Total Loss: 1.603799461460053e+21\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "Var loss:  tensor(1.5579e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.280744616265086e+22\n",
            "E_s_wdiff_all_sq: 7.232327063685525e+19\n",
            "E_IS_SCOPE: -3.226097241733373e+22\n",
            "E_IS_E_SCOPE: -9.374780287438828e+21\n",
            "Total Loss: 1.5579216000016533e+21\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "Var loss:  tensor(1.4955e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2186054363945735e+22\n",
            "E_s_wdiff_all_sq: 1.4320783436180496e+19\n",
            "E_IS_SCOPE: -3.311621804425452e+22\n",
            "E_IS_E_SCOPE: -1.0131275611539864e+22\n",
            "Total Loss: 1.4955245197580546e+21\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "Var loss:  tensor(1.5135e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1632689759298687e+22\n",
            "E_s_wdiff_all_sq: 1.084200194891547e+18\n",
            "E_IS_SCOPE: -3.399041051449923e+22\n",
            "E_IS_E_SCOPE: -1.0904521428491936e+22\n",
            "Total Loss: 1.5135124254778102e+21\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "Var loss:  tensor(1.5428e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1424102425190768e+22\n",
            "E_s_wdiff_all_sq: 9.995917013511735e+18\n",
            "E_IS_SCOPE: -3.438730299143147e+22\n",
            "E_IS_E_SCOPE: -1.1248423262188292e+22\n",
            "Total Loss: 1.5428456385944727e+21\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "Var loss:  tensor(1.5052e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1604771700615028e+22\n",
            "E_s_wdiff_all_sq: 2.497813187449078e+18\n",
            "E_IS_SCOPE: -3.41309817677249e+22\n",
            "E_IS_E_SCOPE: -1.10047754887802e+22\n",
            "Total Loss: 1.5052140412075927e+21\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "Var loss:  tensor(1.4629e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2080736777878213e+22\n",
            "E_s_wdiff_all_sq: 5.962303699342489e+18\n",
            "E_IS_SCOPE: -3.344352078594305e+22\n",
            "E_IS_E_SCOPE: -1.0373720528271083e+22\n",
            "Total Loss: 1.4629362005071322e+21\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "Var loss:  tensor(1.4770e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2592625258055936e+22\n",
            "E_s_wdiff_all_sq: 3.991967222209619e+19\n",
            "E_IS_SCOPE: -3.2780876560970264e+22\n",
            "E_IS_E_SCOPE: -9.766007546533942e+21\n",
            "Total Loss: 1.4769741854190055e+21\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "Var loss:  tensor(1.4901e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2793876705606977e+22\n",
            "E_s_wdiff_all_sq: 5.9502027677624975e+19\n",
            "E_IS_SCOPE: -3.2554403762262136e+22\n",
            "E_IS_E_SCOPE: -9.552317118832756e+21\n",
            "Total Loss: 1.490099947282744e+21\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "Var loss:  tensor(1.4566e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.255984103315449e+22\n",
            "E_s_wdiff_all_sq: 3.5746662274154856e+19\n",
            "E_IS_SCOPE: -3.2879345052357864e+22\n",
            "E_IS_E_SCOPE: -9.83578305695054e+21\n",
            "Total Loss: 1.4565795089921522e+21\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "Var loss:  tensor(1.4298e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.21067556028737e+22\n",
            "E_s_wdiff_all_sq: 5.902081057804411e+18\n",
            "E_IS_SCOPE: -3.3523765883237877e+22\n",
            "E_IS_E_SCOPE: -1.0407510391118763e+22\n",
            "Total Loss: 1.4298344194560062e+21\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "Var loss:  tensor(1.4413e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.175482671195341e+22\n",
            "E_s_wdiff_all_sq: 3.608061351989825e+17\n",
            "E_IS_SCOPE: -3.4076115135540777e+22\n",
            "E_IS_E_SCOPE: -1.0896870497663756e+22\n",
            "Total Loss: 1.4412873435648958e+21\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "Var loss:  tensor(1.4412e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1683486231838048e+22\n",
            "E_s_wdiff_all_sq: 1.556677069839308e+18\n",
            "E_IS_SCOPE: -3.420723472477327e+22\n",
            "E_IS_E_SCOPE: -1.100803445272771e+22\n",
            "Total Loss: 1.4411995501764154e+21\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "Var loss:  tensor(1.4114e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1899953007459526e+22\n",
            "E_s_wdiff_all_sq: 5.0133590337038566e+17\n",
            "E_IS_SCOPE: -3.3877475235530344e+22\n",
            "E_IS_E_SCOPE: -1.0706104847560193e+22\n",
            "Total Loss: 1.411396488781185e+21\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "Var loss:  tensor(1.3971e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2270487781157556e+22\n",
            "E_s_wdiff_all_sq: 1.452261542540968e+19\n",
            "E_IS_SCOPE: -3.334386714471336e+22\n",
            "E_IS_E_SCOPE: -1.0222092984288681e+22\n",
            "Total Loss: 1.3971118542986527e+21\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "Var loss:  tensor(1.4043e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.25455514272645e+22\n",
            "E_s_wdiff_all_sq: 3.5443729775789773e+19\n",
            "E_IS_SCOPE: -3.2980659698734673e+22\n",
            "E_IS_E_SCOPE: -9.890679039300559e+21\n",
            "Total Loss: 1.4043485573986008e+21\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "Var loss:  tensor(1.3937e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2523128111479212e+22\n",
            "E_s_wdiff_all_sq: 3.3737923484991574e+19\n",
            "E_IS_SCOPE: -3.302263786840805e+22\n",
            "E_IS_E_SCOPE: -9.92258909122313e+21\n",
            "Total Loss: 1.393699058034271e+21\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "Var loss:  tensor(1.3697e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2241589554837774e+22\n",
            "E_s_wdiff_all_sq: 1.3463824661238282e+19\n",
            "E_IS_SCOPE: -3.341677226186248e+22\n",
            "E_IS_E_SCOPE: -1.0271178566861467e+22\n",
            "Total Loss: 1.3696601413989884e+21\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "Var loss:  tensor(1.3640e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1928459720067207e+22\n",
            "E_s_wdiff_all_sq: 1.2785224077778248e+18\n",
            "E_IS_SCOPE: -3.3881247263236477e+22\n",
            "E_IS_E_SCOPE: -1.0683328741037868e+22\n",
            "Total Loss: 1.3640308264096256e+21\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "Var loss:  tensor(1.3643e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1786838981563338e+22\n",
            "E_s_wdiff_all_sq: 2550734243817073.0\n",
            "E_IS_SCOPE: -3.410544001229776e+22\n",
            "E_IS_E_SCOPE: -1.0880103563929582e+22\n",
            "Total Loss: 1.3643490584583356e+21\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "Var loss:  tensor(1.3475e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1880218899585343e+22\n",
            "E_s_wdiff_all_sq: 6.887443066764826e+17\n",
            "E_IS_SCOPE: -3.3964912908349544e+22\n",
            "E_IS_E_SCOPE: -1.0749667745270736e+22\n",
            "Total Loss: 1.347475728400235e+21\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "Var loss:  tensor(1.3311e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2132323808601113e+22\n",
            "E_s_wdiff_all_sq: 9.0125476743935e+18\n",
            "E_IS_SCOPE: -3.3596038639327665e+22\n",
            "E_IS_E_SCOPE: -1.04142431226007e+22\n",
            "Total Loss: 1.3310553074496543e+21\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "Var loss:  tensor(1.3288e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2359518219820369e+22\n",
            "E_s_wdiff_all_sq: 2.355155912170939e+19\n",
            "E_IS_SCOPE: -3.3284183456581075e+22\n",
            "E_IS_E_SCOPE: -1.0129853799297874e+22\n",
            "Total Loss: 1.3287921142536088e+21\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "Var loss:  tensor(1.3210e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.238773829114204e+22\n",
            "E_s_wdiff_all_sq: 2.6027424365793227e+19\n",
            "E_IS_SCOPE: -3.325724543577118e+22\n",
            "E_IS_E_SCOPE: -1.0099905093363302e+22\n",
            "Total Loss: 1.3210421093913623e+21\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "Var loss:  tensor(1.3036e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2205940343263744e+22\n",
            "E_s_wdiff_all_sq: 1.3827445038710628e+19\n",
            "E_IS_SCOPE: -3.351893605020439e+22\n",
            "E_IS_E_SCOPE: -1.0329141024401517e+22\n",
            "Total Loss: 1.3035707257612906e+21\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "Var loss:  tensor(1.2943e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1971769088341126e+22\n",
            "E_s_wdiff_all_sq: 3.3808223308120694e+18\n",
            "E_IS_SCOPE: -3.3868683983787127e+22\n",
            "E_IS_E_SCOPE: -1.0637275207129033e+22\n",
            "Total Loss: 1.2943061518557412e+21\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "Var loss:  tensor(1.2897e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1854936498453332e+22\n",
            "E_s_wdiff_all_sq: 7.516945630442011e+17\n",
            "E_IS_SCOPE: -3.405854906542557e+22\n",
            "E_IS_E_SCOPE: -1.080127225483636e+22\n",
            "Total Loss: 1.289734541813041e+21\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "Var loss:  tensor(1.2763e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1921079734718163e+22\n",
            "E_s_wdiff_all_sq: 2.076841512173565e+18\n",
            "E_IS_SCOPE: -3.397632371078965e+22\n",
            "E_IS_E_SCOPE: -1.071993393085295e+22\n",
            "Total Loss: 1.2763135153994832e+21\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "Var loss:  tensor(1.2627e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2110884012131495e+22\n",
            "E_s_wdiff_all_sq: 9.069685530858882e+18\n",
            "E_IS_SCOPE: -3.3718835790941866e+22\n",
            "E_IS_E_SCOPE: -1.0480477852306265e+22\n",
            "Total Loss: 1.2627253968683265e+21\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "Var loss:  tensor(1.2564e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2278191404034225e+22\n",
            "E_s_wdiff_all_sq: 1.8685241388524634e+19\n",
            "E_IS_SCOPE: -3.350828240418175e+22\n",
            "E_IS_E_SCOPE: -1.0282236121184601e+22\n",
            "Total Loss: 1.256414368926305e+21\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "Var loss:  tensor(1.2467e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2292507139117263e+22\n",
            "E_s_wdiff_all_sq: 1.943348264484393e+19\n",
            "E_IS_SCOPE: -3.351490470015154e+22\n",
            "E_IS_E_SCOPE: -1.0278635985077858e+22\n",
            "Total Loss: 1.246685103749526e+21\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "Var loss:  tensor(1.2325e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2156577494832555e+22\n",
            "E_s_wdiff_all_sq: 1.0911978081941123e+19\n",
            "E_IS_SCOPE: -3.3732205758776047e+22\n",
            "E_IS_E_SCOPE: -1.0464088822638019e+22\n",
            "Total Loss: 1.2325118548962894e+21\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "Var loss:  tensor(1.2232e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.199476757834252e+22\n",
            "E_s_wdiff_all_sq: 3.8209152072431867e+18\n",
            "E_IS_SCOPE: -3.399230348502566e+22\n",
            "E_IS_E_SCOPE: -1.0688333226491356e+22\n",
            "Total Loss: 1.2232396974670364e+21\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "Var loss:  tensor(1.2150e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1934386231559609e+22\n",
            "E_s_wdiff_all_sq: 2.0219336569202732e+18\n",
            "E_IS_SCOPE: -3.4108479363212556e+22\n",
            "E_IS_E_SCOPE: -1.0783161025489789e+22\n",
            "Total Loss: 1.2149899913056339e+21\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "Var loss:  tensor(1.2020e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2012212744656058e+22\n",
            "E_s_wdiff_all_sq: 4.200909866380894e+18\n",
            "E_IS_SCOPE: -3.4019954462078697e+22\n",
            "E_IS_E_SCOPE: -1.0693855981469404e+22\n",
            "Total Loss: 1.2019511746086463e+21\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "Var loss:  tensor(1.1903e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.214390478637759e+22\n",
            "E_s_wdiff_all_sq: 9.84692014117684e+18\n",
            "E_IS_SCOPE: -3.3848198683308272e+22\n",
            "E_IS_E_SCOPE: -1.0531587344468344e+22\n",
            "Total Loss: 1.1903488628678501e+21\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "Var loss:  tensor(1.1801e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.216407197502897e+22\n",
            "E_s_wdiff_all_sq: 1.1790627644726143e+19\n",
            "E_IS_SCOPE: -3.3809238666236645e+22\n",
            "E_IS_E_SCOPE: -1.049547874794727e+22\n",
            "Total Loss: 1.1801102960992631e+21\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "Var loss:  tensor(1.1689e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2084167305902185e+22\n",
            "E_s_wdiff_all_sq: 8.847664656674314e+18\n",
            "E_IS_SCOPE: -3.3900875979975534e+22\n",
            "E_IS_E_SCOPE: -1.0579472643091774e+22\n",
            "Total Loss: 1.1689126849282069e+21\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "Var loss:  tensor(1.1585e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2001962823171067e+22\n",
            "E_s_wdiff_all_sq: 6.025374071502084e+18\n",
            "E_IS_SCOPE: -3.4005369668284935e+22\n",
            "E_IS_E_SCOPE: -1.0673082323603003e+22\n",
            "Total Loss: 1.1585498671119744e+21\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "Var loss:  tensor(1.1473e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2064932943767403e+22\n",
            "E_s_wdiff_all_sq: 8.533814493309938e+18\n",
            "E_IS_SCOPE: -3.3947366417133333e+22\n",
            "E_IS_E_SCOPE: -1.0609657567980491e+22\n",
            "Total Loss: 1.1472522231439824e+21\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "Var loss:  tensor(1.1367e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.212830196089783e+22\n",
            "E_s_wdiff_all_sq: 1.1377345801781285e+19\n",
            "E_IS_SCOPE: -3.3892766133473864e+22\n",
            "E_IS_E_SCOPE: -1.0548613427595294e+22\n",
            "Total Loss: 1.1366893864377574e+21\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "Var loss:  tensor(1.1254e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2092257992794534e+22\n",
            "E_s_wdiff_all_sq: 9.581448977066113e+18\n",
            "E_IS_SCOPE: -3.3962956285604794e+22\n",
            "E_IS_E_SCOPE: -1.0603645791052633e+22\n",
            "Total Loss: 1.1253622044446163e+21\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "Var loss:  tensor(1.1144e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2028444769477068e+22\n",
            "E_s_wdiff_all_sq: 6.633882610849539e+18\n",
            "E_IS_SCOPE: -3.407678218877208e+22\n",
            "E_IS_E_SCOPE: -1.0697158144042446e+22\n",
            "Total Loss: 1.1144342337620206e+21\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "Var loss:  tensor(1.1031e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2051917330869666e+22\n",
            "E_s_wdiff_all_sq: 7.10035377010985e+18\n",
            "E_IS_SCOPE: -3.4089221360463743e+22\n",
            "E_IS_E_SCOPE: -1.0693856547036306e+22\n",
            "Total Loss: 1.1031236607528665e+21\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "Var loss:  tensor(1.0918e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2127351467841742e+22\n",
            "E_s_wdiff_all_sq: 9.71706088519e+18\n",
            "E_IS_SCOPE: -3.404097726408538e+22\n",
            "E_IS_E_SCOPE: -1.0632784592245564e+22\n",
            "Total Loss: 1.0918301261094464e+21\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "Var loss:  tensor(1.0806e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2146223448982945e+22\n",
            "E_s_wdiff_all_sq: 1.0052183228978827e+19\n",
            "E_IS_SCOPE: -3.4059757960315407e+22\n",
            "E_IS_E_SCOPE: -1.0635270460826664e+22\n",
            "Total Loss: 1.0805983205338907e+21\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "Var loss:  tensor(1.0691e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2101062659164732e+22\n",
            "E_s_wdiff_all_sq: 7.609704325882423e+18\n",
            "E_IS_SCOPE: -3.4157812232006274e+22\n",
            "E_IS_E_SCOPE: -1.0711846053681341e+22\n",
            "Total Loss: 1.0690557658979799e+21\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "Var loss:  tensor(1.0577e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2082168099871543e+22\n",
            "E_s_wdiff_all_sq: 6.393003270357774e+18\n",
            "E_IS_SCOPE: -3.422562451272381e+22\n",
            "E_IS_E_SCOPE: -1.075957872307528e+22\n",
            "Total Loss: 1.0577346533515382e+21\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "Var loss:  tensor(1.0460e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.213020326599022e+22\n",
            "E_s_wdiff_all_sq: 7.831166897434421e+18\n",
            "E_IS_SCOPE: -3.4207913238684225e+22\n",
            "E_IS_E_SCOPE: -1.0727856352283935e+22\n",
            "Total Loss: 1.0460252551753446e+21\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "Var loss:  tensor(1.0346e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2177250193971446e+22\n",
            "E_s_wdiff_all_sq: 9.487218270428697e+18\n",
            "E_IS_SCOPE: -3.418695069608659e+22\n",
            "E_IS_E_SCOPE: -1.0694381326064123e+22\n",
            "Total Loss: 1.0345721896924076e+21\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "Var loss:  tensor(1.0228e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2161087009954743e+22\n",
            "E_s_wdiff_all_sq: 8.49785090682553e+18\n",
            "E_IS_SCOPE: -3.4241322838926444e+22\n",
            "E_IS_E_SCOPE: -1.0732085248959202e+22\n",
            "Total Loss: 1.0227764886812314e+21\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "Var loss:  tensor(1.0111e+21, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2121070845041656e+22\n",
            "E_s_wdiff_all_sq: 6.68449501634125e+18\n",
            "E_IS_SCOPE: -3.432317794684397e+22\n",
            "E_IS_E_SCOPE: -1.0796089635684118e+22\n",
            "Total Loss: 1.0110632681078386e+21\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "Var loss:  tensor(9.9918e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2127058158385526e+22\n",
            "E_s_wdiff_all_sq: 6.826442545100489e+18\n",
            "E_IS_SCOPE: -3.434318336583652e+22\n",
            "E_IS_E_SCOPE: -1.0803469958265608e+22\n",
            "Total Loss: 9.991807112110827e+20\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "Var loss:  tensor(9.8722e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2168073481343449e+22\n",
            "E_s_wdiff_all_sq: 8.508589829581772e+18\n",
            "E_IS_SCOPE: -3.4314882434000207e+22\n",
            "E_IS_E_SCOPE: -1.0766942331751084e+22\n",
            "Total Loss: 9.872150182980559e+20\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "Var loss:  tensor(9.7525e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2176684387901119e+22\n",
            "E_s_wdiff_all_sq: 8.980974322210485e+18\n",
            "E_IS_SCOPE: -3.43240522518299e+22\n",
            "E_IS_E_SCOPE: -1.0766197186037736e+22\n",
            "Total Loss: 9.752508520563013e+20\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "Var loss:  tensor(9.6309e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2139674910767213e+22\n",
            "E_s_wdiff_all_sq: 7.545514491885366e+18\n",
            "E_IS_SCOPE: -3.438931357036018e+22\n",
            "E_IS_E_SCOPE: -1.0817988395033115e+22\n",
            "Total Loss: 9.630923648479758e+20\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "Var loss:  tensor(9.5101e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2115370444249251e+22\n",
            "E_s_wdiff_all_sq: 6.778270906114173e+18\n",
            "E_IS_SCOPE: -3.4435642213529958e+22\n",
            "E_IS_E_SCOPE: -1.0852947379384148e+22\n",
            "Total Loss: 9.510075650435543e+20\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "Var loss:  tensor(9.3868e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.21355216942951e+22\n",
            "E_s_wdiff_all_sq: 7.81528263127965e+18\n",
            "E_IS_SCOPE: -3.442446713968044e+22\n",
            "E_IS_E_SCOPE: -1.0834671522602918e+22\n",
            "Total Loss: 9.38681392444746e+20\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "Var loss:  tensor(9.2643e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2156487501320747e+22\n",
            "E_s_wdiff_all_sq: 9.02246369271829e+18\n",
            "E_IS_SCOPE: -3.440994847517194e+22\n",
            "E_IS_E_SCOPE: -1.081392153187406e+22\n",
            "Total Loss: 9.26434556300822e+20\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "Var loss:  tensor(9.1402e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2136907593662983e+22\n",
            "E_s_wdiff_all_sq: 8.473657060423804e+18\n",
            "E_IS_SCOPE: -3.444711980633373e+22\n",
            "E_IS_E_SCOPE: -1.0841089739270146e+22\n",
            "Total Loss: 9.140166085077337e+20\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "Var loss:  tensor(9.0160e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2104044271443062e+22\n",
            "E_s_wdiff_all_sq: 7.335639167722385e+18\n",
            "E_IS_SCOPE: -3.450389617466081e+22\n",
            "E_IS_E_SCOPE: -1.088575519237585e+22\n",
            "Total Loss: 9.015969089507662e+20\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "Var loss:  tensor(8.8908e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.210462980474097e+22\n",
            "E_s_wdiff_all_sq: 7.573405599137854e+18\n",
            "E_IS_SCOPE: -3.451885488122717e+22\n",
            "E_IS_E_SCOPE: -1.0891360132994127e+22\n",
            "Total Loss: 8.890798509707808e+20\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "Var loss:  tensor(8.7648e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2128173657756937e+22\n",
            "E_s_wdiff_all_sq: 8.77376816568012e+18\n",
            "E_IS_SCOPE: -3.450570525847417e+22\n",
            "E_IS_E_SCOPE: -1.0870547232488094e+22\n",
            "Total Loss: 8.764842479746516e+20\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "Var loss:  tensor(8.6383e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.212906469517107e+22\n",
            "E_s_wdiff_all_sq: 8.930388870312846e+18\n",
            "E_IS_SCOPE: -3.4525139627025638e+22\n",
            "E_IS_E_SCOPE: -1.0878989833763344e+22\n",
            "Total Loss: 8.638343521115979e+20\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "Var loss:  tensor(8.5112e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2104141925470454e+22\n",
            "E_s_wdiff_all_sq: 7.879041544775077e+18\n",
            "E_IS_SCOPE: -3.458123738420071e+22\n",
            "E_IS_E_SCOPE: -1.0920456986651288e+22\n",
            "Total Loss: 8.511175267246743e+20\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "Var loss:  tensor(8.3837e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2095989539369612e+22\n",
            "E_s_wdiff_all_sq: 7.551307781983021e+18\n",
            "E_IS_SCOPE: -3.4616653838467564e+22\n",
            "E_IS_E_SCOPE: -1.0942534790485513e+22\n",
            "Total Loss: 8.383656435927986e+20\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "Var loss:  tensor(8.2549e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2116214096607895e+22\n",
            "E_s_wdiff_all_sq: 8.415079779994917e+18\n",
            "E_IS_SCOPE: -3.4615901098296012e+22\n",
            "E_IS_E_SCOPE: -1.0930998201774231e+22\n",
            "Total Loss: 8.254859150403358e+20\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "Var loss:  tensor(8.1257e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2127881795100419e+22\n",
            "E_s_wdiff_all_sq: 8.926240639637699e+18\n",
            "E_IS_SCOPE: -3.4626769304095033e+22\n",
            "E_IS_E_SCOPE: -1.0930050732786012e+22\n",
            "Total Loss: 8.125747949701599e+20\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "Var loss:  tensor(7.9961e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2112026495103854e+22\n",
            "E_s_wdiff_all_sq: 8.237318787873422e+18\n",
            "E_IS_SCOPE: -3.4672668280155697e+22\n",
            "E_IS_E_SCOPE: -1.0961612287202992e+22\n",
            "Total Loss: 7.996062061032081e+20\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "Var loss:  tensor(7.8658e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2099940651323695e+22\n",
            "E_s_wdiff_all_sq: 7.605545050523092e+18\n",
            "E_IS_SCOPE: -3.4719271398748027e+22\n",
            "E_IS_E_SCOPE: -1.0992368307587204e+22\n",
            "Total Loss: 7.865821862910241e+20\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "Var loss:  tensor(7.7343e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2116220275722086e+22\n",
            "E_s_wdiff_all_sq: 8.067148890868569e+18\n",
            "E_IS_SCOPE: -3.473374738855318e+22\n",
            "E_IS_E_SCOPE: -1.099220797935788e+22\n",
            "Total Loss: 7.734288463911348e+20\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "Var loss:  tensor(7.6028e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2137587735952986e+22\n",
            "E_s_wdiff_all_sq: 8.68580370972083e+18\n",
            "E_IS_SCOPE: -3.474494150470191e+22\n",
            "E_IS_E_SCOPE: -1.0988164678149632e+22\n",
            "Total Loss: 7.602781706998356e+20\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "Var loss:  tensor(7.4706e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2134175266144829e+22\n",
            "E_s_wdiff_all_sq: 8.33367465602258e+18\n",
            "E_IS_SCOPE: -3.4783819423621598e+22\n",
            "E_IS_E_SCOPE: -1.1010813608694362e+22\n",
            "Total Loss: 7.470566546864897e+20\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "Var loss:  tensor(7.3375e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2120698413984216e+22\n",
            "E_s_wdiff_all_sq: 7.654825889031122e+18\n",
            "E_IS_SCOPE: -3.4832144012901597e+22\n",
            "E_IS_E_SCOPE: -1.1043087591814073e+22\n",
            "Total Loss: 7.337525733250145e+20\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "Var loss:  tensor(7.2035e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2125549607379469e+22\n",
            "E_s_wdiff_all_sq: 7.800669340819894e+18\n",
            "E_IS_SCOPE: -3.4854958292456954e+22\n",
            "E_IS_E_SCOPE: -1.1052213694925352e+22\n",
            "Total Loss: 7.203535169433905e+20\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "Var loss:  tensor(7.0694e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.214051485679126e+22\n",
            "E_s_wdiff_all_sq: 8.436832673704211e+18\n",
            "E_IS_SCOPE: -3.4862528911040294e+22\n",
            "E_IS_E_SCOPE: -1.1047802340738316e+22\n",
            "Total Loss: 7.069434666273871e+20\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "Var loss:  tensor(6.9349e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2138996324570675e+22\n",
            "E_s_wdiff_all_sq: 8.40636883391521e+18\n",
            "E_IS_SCOPE: -3.488983493688199e+22\n",
            "E_IS_E_SCOPE: -1.1062028830543643e+22\n",
            "Total Loss: 6.93492352139393e+20\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "Var loss:  tensor(6.7992e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.212507933697997e+22\n",
            "E_s_wdiff_all_sq: 7.759711596577149e+18\n",
            "E_IS_SCOPE: -3.493699909694722e+22\n",
            "E_IS_E_SCOPE: -1.1093587469670529e+22\n",
            "Total Loss: 6.799229617988908e+20\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "Var loss:  tensor(6.6627e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2126933425903955e+22\n",
            "E_s_wdiff_all_sq: 7.675937753143454e+18\n",
            "E_IS_SCOPE: -3.4968613877174485e+22\n",
            "E_IS_E_SCOPE: -1.110949578580027e+22\n",
            "Total Loss: 6.662724386575708e+20\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "Var loss:  tensor(6.5265e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2118295601655837e+22\n",
            "E_s_wdiff_all_sq: 7.393457080459871e+18\n",
            "E_IS_SCOPE: -3.5003825086458283e+22\n",
            "E_IS_E_SCOPE: -1.1131307214810055e+22\n",
            "Total Loss: 6.526464828712073e+20\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "Var loss:  tensor(6.3911e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2185973985349649e+22\n",
            "E_s_wdiff_all_sq: 1.01791643920203e+19\n",
            "E_IS_SCOPE: -3.495239374762885e+22\n",
            "E_IS_E_SCOPE: -1.106994767943105e+22\n",
            "Total Loss: 6.391089380868657e+20\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "Var loss:  tensor(6.2507e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2136319738575835e+22\n",
            "E_s_wdiff_all_sq: 8.142100704686345e+18\n",
            "E_IS_SCOPE: -3.503571868745321e+22\n",
            "E_IS_E_SCOPE: -1.113736891045439e+22\n",
            "Total Loss: 6.250685047343541e+20\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "Var loss:  tensor(6.1144e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2087617619006325e+22\n",
            "E_s_wdiff_all_sq: 6.422063779349371e+18\n",
            "E_IS_SCOPE: -3.511513274938363e+22\n",
            "E_IS_E_SCOPE: -1.120200239658005e+22\n",
            "Total Loss: 6.11438719680987e+20\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "Var loss:  tensor(5.9739e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2120737570239967e+22\n",
            "E_s_wdiff_all_sq: 7.808324648228662e+18\n",
            "E_IS_SCOPE: -3.509748065004141e+22\n",
            "E_IS_E_SCOPE: -1.1174510754122558e+22\n",
            "Total Loss: 5.973879422265946e+20\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "Var loss:  tensor(5.8353e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2162968083986043e+22\n",
            "E_s_wdiff_all_sq: 9.638060411956275e+18\n",
            "E_IS_SCOPE: -3.5072453658775975e+22\n",
            "E_IS_E_SCOPE: -1.1139129005088728e+22\n",
            "Total Loss: 5.835271914020489e+20\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "Var loss:  tensor(5.6940e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2137580062259297e+22\n",
            "E_s_wdiff_all_sq: 8.512270176500157e+18\n",
            "E_IS_SCOPE: -3.513236770320805e+22\n",
            "E_IS_E_SCOPE: -1.1182825869870771e+22\n",
            "Total Loss: 5.6940167515728354e+20\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "Var loss:  tensor(5.5543e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2095684426043654e+22\n",
            "E_s_wdiff_all_sq: 6.873550518731116e+18\n",
            "E_IS_SCOPE: -3.521004777742553e+22\n",
            "E_IS_E_SCOPE: -1.1243921260508644e+22\n",
            "Total Loss: 5.5543206242029666e+20\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "Var loss:  tensor(5.4122e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.211184758891624e+22\n",
            "E_s_wdiff_all_sq: 7.617602314669122e+18\n",
            "E_IS_SCOPE: -3.5213492215301592e+22\n",
            "E_IS_E_SCOPE: -1.1236091168822407e+22\n",
            "Total Loss: 5.412160744820467e+20\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "Var loss:  tensor(5.2705e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.214812585238545e+22\n",
            "E_s_wdiff_all_sq: 9.313738670386946e+18\n",
            "E_IS_SCOPE: -3.5190487640541667e+22\n",
            "E_IS_E_SCOPE: -1.120393945465897e+22\n",
            "Total Loss: 5.2704834498079595e+20\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "Var loss:  tensor(5.1279e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2131420756020654e+22\n",
            "E_s_wdiff_all_sq: 8.851903681805893e+18\n",
            "E_IS_SCOPE: -3.522899041069885e+22\n",
            "E_IS_E_SCOPE: -1.1230423947615305e+22\n",
            "Total Loss: 5.1279118717063633e+20\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "Var loss:  tensor(4.9853e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2091093760979184e+22\n",
            "E_s_wdiff_all_sq: 7.273800898953008e+18\n",
            "E_IS_SCOPE: -3.530378582897494e+22\n",
            "E_IS_E_SCOPE: -1.128897779932501e+22\n",
            "Total Loss: 4.9853491070610964e+20\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "Var loss:  tensor(4.8414e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2098322663072957e+22\n",
            "E_s_wdiff_all_sq: 7.567748981419239e+18\n",
            "E_IS_SCOPE: -3.532324057333828e+22\n",
            "E_IS_E_SCOPE: -1.1294739428627817e+22\n",
            "Total Loss: 4.841392119816158e+20\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "Var loss:  tensor(4.6980e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2135996318699995e+22\n",
            "E_s_wdiff_all_sq: 9.036747450801875e+18\n",
            "E_IS_SCOPE: -3.530978938996114e+22\n",
            "E_IS_E_SCOPE: -1.126843288614729e+22\n",
            "Total Loss: 4.697956069213607e+20\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "Var loss:  tensor(4.5539e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2134749561741143e+22\n",
            "E_s_wdiff_all_sq: 8.947076666151327e+18\n",
            "E_IS_SCOPE: -3.534148910052979e+22\n",
            "E_IS_E_SCOPE: -1.128517897897711e+22\n",
            "Total Loss: 4.553877073146602e+20\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "Var loss:  tensor(4.4090e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2102280523613894e+22\n",
            "E_s_wdiff_all_sq: 7.58825348756846e+18\n",
            "E_IS_SCOPE: -3.541062934509949e+22\n",
            "E_IS_E_SCOPE: -1.1337265590590505e+22\n",
            "Total Loss: 4.408980148882034e+20\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "Var loss:  tensor(4.2635e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2101040142819569e+22\n",
            "E_s_wdiff_all_sq: 7.573095252815326e+18\n",
            "E_IS_SCOPE: -3.543980535359371e+22\n",
            "E_IS_E_SCOPE: -1.135227882976325e+22\n",
            "Total Loss: 4.263453346236759e+20\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "Var loss:  tensor(4.1168e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2129312227831413e+22\n",
            "E_s_wdiff_all_sq: 8.815906185424067e+18\n",
            "E_IS_SCOPE: -3.543145657752981e+22\n",
            "E_IS_E_SCOPE: -1.1332399964825884e+22\n",
            "Total Loss: 4.116783769512145e+20\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "Var loss:  tensor(3.9707e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2129276424265883e+22\n",
            "E_s_wdiff_all_sq: 8.973023490845377e+18\n",
            "E_IS_SCOPE: -3.5454495983748543e+22\n",
            "E_IS_E_SCOPE: -1.134290140752373e+22\n",
            "Total Loss: 3.970680890417595e+20\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "Var loss:  tensor(3.8237e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2101396448771196e+22\n",
            "E_s_wdiff_all_sq: 7.757830553102307e+18\n",
            "E_IS_SCOPE: -3.5520072943078486e+22\n",
            "E_IS_E_SCOPE: -1.1391013150754131e+22\n",
            "Total Loss: 3.823694245783887e+20\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "Var loss:  tensor(3.6757e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2101589800352104e+22\n",
            "E_s_wdiff_all_sq: 7.568626379815266e+18\n",
            "E_IS_SCOPE: -3.555780874143794e+22\n",
            "E_IS_E_SCOPE: -1.141117389001589e+22\n",
            "Total Loss: 3.675733972256639e+20\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "Var loss:  tensor(3.5272e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2133500199739933e+22\n",
            "E_s_wdiff_all_sq: 8.599337443984928e+18\n",
            "E_IS_SCOPE: -3.5560002140252473e+22\n",
            "E_IS_E_SCOPE: -1.1397047848270029e+22\n",
            "Total Loss: 3.5271887374684206e+20\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "Var loss:  tensor(3.3789e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2142785055565522e+22\n",
            "E_s_wdiff_all_sq: 8.817136149464127e+18\n",
            "E_IS_SCOPE: -3.5585433688957807e+22\n",
            "E_IS_E_SCOPE: -1.1405964049542711e+22\n",
            "Total Loss: 3.3788923597981064e+20\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "Var loss:  tensor(3.2294e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2120776852312835e+22\n",
            "E_s_wdiff_all_sq: 7.797444480348061e+18\n",
            "E_IS_SCOPE: -3.5646489801883994e+22\n",
            "E_IS_E_SCOPE: -1.1449021931728665e+22\n",
            "Total Loss: 3.2294245343261046e+20\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "Var loss:  tensor(3.0794e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2115403123188319e+22\n",
            "E_s_wdiff_all_sq: 7.568939925811033e+18\n",
            "E_IS_SCOPE: -3.5683686433781696e+22\n",
            "E_IS_E_SCOPE: -1.1470610342222066e+22\n",
            "Total Loss: 3.079406919928932e+20\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "Var loss:  tensor(2.9286e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2135820367177324e+22\n",
            "E_s_wdiff_all_sq: 8.478356919777422e+18\n",
            "E_IS_SCOPE: -3.5685792951848042e+22\n",
            "E_IS_E_SCOPE: -1.1460183369630207e+22\n",
            "Total Loss: 2.928639820736307e+20\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "Var loss:  tensor(2.7779e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2140165405517856e+22\n",
            "E_s_wdiff_all_sq: 8.69040690733726e+18\n",
            "E_IS_SCOPE: -3.5709542682080307e+22\n",
            "E_IS_E_SCOPE: -1.1469543959836764e+22\n",
            "Total Loss: 2.777913305693044e+20\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "Var loss:  tensor(2.6258e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2121119769083255e+22\n",
            "E_s_wdiff_all_sq: 7.798521990456603e+18\n",
            "E_IS_SCOPE: -3.5767462779568483e+22\n",
            "E_IS_E_SCOPE: -1.150943692666e+22\n",
            "Total Loss: 2.6258439962805823e+20\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "Var loss:  tensor(2.4743e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2120516103250008e+22\n",
            "E_s_wdiff_all_sq: 7.614902570840139e+18\n",
            "E_IS_SCOPE: -3.5805307264536462e+22\n",
            "E_IS_E_SCOPE: -1.15298148631276e+22\n",
            "Total Loss: 2.4742557326165148e+20\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "Var loss:  tensor(2.3217e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2142033045783449e+22\n",
            "E_s_wdiff_all_sq: 8.463252554697501e+18\n",
            "E_IS_SCOPE: -3.58108669594462e+22\n",
            "E_IS_E_SCOPE: -1.152126423133578e+22\n",
            "Total Loss: 2.321712306001606e+20\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "Var loss:  tensor(2.1685e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2143886463254502e+22\n",
            "E_s_wdiff_all_sq: 8.594348642653532e+18\n",
            "E_IS_SCOPE: -3.5837171162239233e+22\n",
            "E_IS_E_SCOPE: -1.1533074111513965e+22\n",
            "Total Loss: 2.1684588301970204e+20\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "Var loss:  tensor(2.0145e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2122656214780607e+22\n",
            "E_s_wdiff_all_sq: 7.835351096493454e+18\n",
            "E_IS_SCOPE: -3.588909749357273e+22\n",
            "E_IS_E_SCOPE: -1.156963725164919e+22\n",
            "Total Loss: 2.0145239228449345e+20\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "Var loss:  tensor(1.8598e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2116587504463832e+22\n",
            "E_s_wdiff_all_sq: 7.842054687835633e+18\n",
            "E_IS_SCOPE: -3.591691991489834e+22\n",
            "E_IS_E_SCOPE: -1.1585051820466264e+22\n",
            "Total Loss: 1.859825457677174e+20\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "Var loss:  tensor(1.7054e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2127241736386242e+22\n",
            "E_s_wdiff_all_sq: 8.555112100219276e+18\n",
            "E_IS_SCOPE: -3.5923674091904486e+22\n",
            "E_IS_E_SCOPE: -1.15808567177009e+22\n",
            "Total Loss: 1.7054311538174093e+20\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "Var loss:  tensor(1.5501e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2125134238247589e+22\n",
            "E_s_wdiff_all_sq: 8.533636510262068e+18\n",
            "E_IS_SCOPE: -3.5954667843890572e+22\n",
            "E_IS_E_SCOPE: -1.159704359178439e+22\n",
            "Total Loss: 1.5500873552138705e+20\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "Var loss:  tensor(1.3934e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2113305957379978e+22\n",
            "E_s_wdiff_all_sq: 7.918880979269823e+18\n",
            "E_IS_SCOPE: -3.6005627774352402e+22\n",
            "E_IS_E_SCOPE: -1.1629738387696611e+22\n",
            "Total Loss: 1.3934498146678106e+20\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "Var loss:  tensor(1.2372e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2119076915685639e+22\n",
            "E_s_wdiff_all_sq: 8.005162939135414e+18\n",
            "E_IS_SCOPE: -3.6036068511186265e+22\n",
            "E_IS_E_SCOPE: -1.1642901561206362e+22\n",
            "Total Loss: 1.2372398663396858e+20\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "Var loss:  tensor(1.0809e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.213297720219529e+22\n",
            "E_s_wdiff_all_sq: 8.55781666713699e+18\n",
            "E_IS_SCOPE: -3.6051434308650897e+22\n",
            "E_IS_E_SCOPE: -1.164329958974709e+22\n",
            "Total Loss: 1.0808676383653993e+20\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "Var loss:  tensor(9.2405e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2128708857438969e+22\n",
            "E_s_wdiff_all_sq: 8.48763461236741e+18\n",
            "E_IS_SCOPE: -3.608367032371898e+22\n",
            "E_IS_E_SCOPE: -1.1660985745921e+22\n",
            "Total Loss: 9.240485746577926e+19\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "Var loss:  tensor(7.6648e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2117665998150513e+22\n",
            "E_s_wdiff_all_sq: 8.14955694662722e+18\n",
            "E_IS_SCOPE: -3.6123798920388103e+22\n",
            "E_IS_E_SCOPE: -1.1686166742476337e+22\n",
            "Total Loss: 7.664779450509165e+19\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "Var loss:  tensor(6.0822e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2111003667703475e+22\n",
            "E_s_wdiff_all_sq: 8.045203847492336e+18\n",
            "E_IS_SCOPE: -3.6156687509448928e+22\n",
            "E_IS_E_SCOPE: -1.1705037969994805e+22\n",
            "Total Loss: 6.0821968491808555e+19\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "Var loss:  tensor(4.5004e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2112585362610982e+22\n",
            "E_s_wdiff_all_sq: 8.33121977073828e+18\n",
            "E_IS_SCOPE: -3.617740301194891e+22\n",
            "E_IS_E_SCOPE: -1.171304828703877e+22\n",
            "Total Loss: 4.500360953124094e+19\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "Var loss:  tensor(2.9147e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2117110253448205e+22\n",
            "E_s_wdiff_all_sq: 8.591672382237137e+18\n",
            "E_IS_SCOPE: -3.6200836211480593e+22\n",
            "E_IS_E_SCOPE: -1.172187660925064e+22\n",
            "Total Loss: 2.9147067940710908e+19\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "Var loss:  tensor(1.3173e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2117068352872522e+22\n",
            "E_s_wdiff_all_sq: 8.493027115927862e+18\n",
            "E_IS_SCOPE: -3.6236785327513887e+22\n",
            "E_IS_E_SCOPE: -1.1740531847299932e+22\n",
            "Total Loss: 1.3172649446315393e+19\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "Var loss:  tensor(-2.8655e+18, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2115045973815909e+22\n",
            "E_s_wdiff_all_sq: 8.204898352980747e+18\n",
            "E_IS_SCOPE: -3.6279690314680703e+22\n",
            "E_IS_E_SCOPE: -1.1764463259738832e+22\n",
            "Total Loss: -2.865466627227386e+18\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "Var loss:  tensor(-1.8837e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.21182928109298e+22\n",
            "E_s_wdiff_all_sq: 8.016914411021243e+18\n",
            "E_IS_SCOPE: -3.63209903236539e+22\n",
            "E_IS_E_SCOPE: -1.1785590517616624e+22\n",
            "Total Loss: -1.8836873826359312e+19\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "Var loss:  tensor(-3.4878e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2127278674333649e+22\n",
            "E_s_wdiff_all_sq: 8.165706400204357e+18\n",
            "E_IS_SCOPE: -3.635125478450819e+22\n",
            "E_IS_E_SCOPE: -1.1797424419202824e+22\n",
            "Total Loss: -3.487822737248669e+19\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "Var loss:  tensor(-5.0993e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2135913387610308e+22\n",
            "E_s_wdiff_all_sq: 8.430319069066772e+18\n",
            "E_IS_SCOPE: -3.637685334953238e+22\n",
            "E_IS_E_SCOPE: -1.180624032837817e+22\n",
            "Total Loss: -5.09929166181761e+19\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "Var loss:  tensor(-6.7169e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2135347018513852e+22\n",
            "E_s_wdiff_all_sq: 8.367625721076847e+18\n",
            "E_IS_SCOPE: -3.641155817954212e+22\n",
            "E_IS_E_SCOPE: -1.182417368615821e+22\n",
            "Total Loss: -6.71688723939889e+19\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "Var loss:  tensor(-8.3421e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2128855530491697e+22\n",
            "E_s_wdiff_all_sq: 8.131162501062754e+18\n",
            "E_IS_SCOPE: -3.6450744394082956e+22\n",
            "E_IS_E_SCOPE: -1.1847044150901915e+22\n",
            "Total Loss: -8.342139526669612e+19\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "Var loss:  tensor(-9.9733e+19, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2123977943652638e+22\n",
            "E_s_wdiff_all_sq: 8.085648236205699e+18\n",
            "E_IS_SCOPE: -3.648302565185138e+22\n",
            "E_IS_E_SCOPE: -1.1864753728287364e+22\n",
            "Total Loss: -9.973276548626016e+19\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "Var loss:  tensor(-1.1610e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2122836464148526e+22\n",
            "E_s_wdiff_all_sq: 8.255304982092833e+18\n",
            "E_IS_SCOPE: -3.6508355153682198e+22\n",
            "E_IS_E_SCOPE: -1.1876563661881833e+22\n",
            "Total Loss: -1.1610344786860232e+20\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "Var loss:  tensor(-1.3250e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.21218220693654e+22\n",
            "E_s_wdiff_all_sq: 8.437896556983168e+18\n",
            "E_IS_SCOPE: -3.653336550031556e+22\n",
            "E_IS_E_SCOPE: -1.1888109242496026e+22\n",
            "Total Loss: -1.3250179407587102e+20\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "Var loss:  tensor(-1.4892e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2119185830499483e+22\n",
            "E_s_wdiff_all_sq: 8.382579473740544e+18\n",
            "E_IS_SCOPE: -3.656727125519529e+22\n",
            "E_IS_E_SCOPE: -1.1906118321752961e+22\n",
            "Total Loss: -1.489223049304395e+20\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "Var loss:  tensor(-1.6541e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2117650689385132e+22\n",
            "E_s_wdiff_all_sq: 8.26333388347596e+18\n",
            "E_IS_SCOPE: -3.6604477559866455e+22\n",
            "E_IS_E_SCOPE: -1.1925937647230688e+22\n",
            "Total Loss: -1.6541207895394065e+20\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "Var loss:  tensor(-1.8192e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2120739593354765e+22\n",
            "E_s_wdiff_all_sq: 8.229488802725618e+18\n",
            "E_IS_SCOPE: -3.664043847529751e+22\n",
            "E_IS_E_SCOPE: -1.194337817449285e+22\n",
            "Total Loss: -1.819234240599341e+20\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "Var loss:  tensor(-1.9848e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2125690287053961e+22\n",
            "E_s_wdiff_all_sq: 8.348566220547242e+18\n",
            "E_IS_SCOPE: -3.6671138775860448e+22\n",
            "E_IS_E_SCOPE: -1.1956674411768094e+22\n",
            "Total Loss: -1.9848461966088182e+20\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "Var loss:  tensor(-2.1512e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2128147442954976e+22\n",
            "E_s_wdiff_all_sq: 8.420738146647486e+18\n",
            "E_IS_SCOPE: -3.670274869727986e+22\n",
            "E_IS_E_SCOPE: -1.1971382361353921e+22\n",
            "Total Loss: -2.151234578412051e+20\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "Var loss:  tensor(-2.3184e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2126009066524512e+22\n",
            "E_s_wdiff_all_sq: 8.344881417173497e+18\n",
            "E_IS_SCOPE: -3.673834145068559e+22\n",
            "E_IS_E_SCOPE: -1.199025233810653e+22\n",
            "Total Loss: -2.3183741082530533e+20\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "Var loss:  tensor(-2.4857e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.212222862724474e+22\n",
            "E_s_wdiff_all_sq: 8.248367842994133e+18\n",
            "E_IS_SCOPE: -3.677405529269655e+22\n",
            "E_IS_E_SCOPE: -1.2009744608298323e+22\n",
            "Total Loss: -2.485687235516249e+20\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "Var loss:  tensor(-2.6531e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2121951517397496e+22\n",
            "E_s_wdiff_all_sq: 8.258280085326128e+18\n",
            "E_IS_SCOPE: -3.6807117129377695e+22\n",
            "E_IS_E_SCOPE: -1.202629835048986e+22\n",
            "Total Loss: -2.653082801865375e+20\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "Var loss:  tensor(-2.8219e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2125247681776715e+22\n",
            "E_s_wdiff_all_sq: 8.264314752660881e+18\n",
            "E_IS_SCOPE: -3.6842280926602175e+22\n",
            "E_IS_E_SCOPE: -1.204301443460597e+22\n",
            "Total Loss: -2.8219145790795284e+20\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "Var loss:  tensor(-2.9902e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2130926858498822e+22\n",
            "E_s_wdiff_all_sq: 8.219817826283616e+18\n",
            "E_IS_SCOPE: -3.688056087900254e+22\n",
            "E_IS_E_SCOPE: -1.2061030600384847e+22\n",
            "Total Loss: -2.990236886499391e+20\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "Var loss:  tensor(-3.1591e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.213479631084819e+22\n",
            "E_s_wdiff_all_sq: 8.23389938709003e+18\n",
            "E_IS_SCOPE: -3.6915697644503143e+22\n",
            "E_IS_E_SCOPE: -1.207753652447686e+22\n",
            "Total Loss: -3.1590999502574374e+20\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "Var loss:  tensor(-3.3286e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2136468634913175e+22\n",
            "E_s_wdiff_all_sq: 8.289467987627777e+18\n",
            "E_IS_SCOPE: -3.694824043785155e+22\n",
            "E_IS_E_SCOPE: -1.2093024621805932e+22\n",
            "Total Loss: -3.328589656703579e+20\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "Var loss:  tensor(-3.4989e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2134705751119041e+22\n",
            "E_s_wdiff_all_sq: 8.291764886023741e+18\n",
            "E_IS_SCOPE: -3.698149670076634e+22\n",
            "E_IS_E_SCOPE: -1.2110106369742548e+22\n",
            "Total Loss: -3.4988913148421997e+20\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "Var loss:  tensor(-3.6700e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.213039316564978e+22\n",
            "E_s_wdiff_all_sq: 8.264366484504631e+18\n",
            "E_IS_SCOPE: -3.701491381174132e+22\n",
            "E_IS_E_SCOPE: -1.2128119695772934e+22\n",
            "Total Loss: -3.6699596081228415e+20\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "Var loss:  tensor(-3.8416e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2123261165467863e+22\n",
            "E_s_wdiff_all_sq: 8.239663391580307e+18\n",
            "E_IS_SCOPE: -3.704699648541147e+22\n",
            "E_IS_E_SCOPE: -1.214616576249274e+22\n",
            "Total Loss: -3.8415510817507954e+20\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "Var loss:  tensor(-4.0132e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2116852350069638e+22\n",
            "E_s_wdiff_all_sq: 8.291866007374452e+18\n",
            "E_IS_SCOPE: -3.7076294944289123e+22\n",
            "E_IS_E_SCOPE: -1.2162111886370726e+22\n",
            "Total Loss: -4.013224696946591e+20\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "Var loss:  tensor(-4.1853e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.210642780049374e+22\n",
            "E_s_wdiff_all_sq: 8.01366323478059e+18\n",
            "E_IS_SCOPE: -3.711735197027742e+22\n",
            "E_IS_E_SCOPE: -1.2187223125450188e+22\n",
            "Total Loss: -4.1852735400876546e+20\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "Var loss:  tensor(-4.3586e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2109008702331886e+22\n",
            "E_s_wdiff_all_sq: 8.16398986667352e+18\n",
            "E_IS_SCOPE: -3.714713199838404e+22\n",
            "E_IS_E_SCOPE: -1.2200439302998408e+22\n",
            "Total Loss: -4.3585797573920267e+20\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "Var loss:  tensor(-4.5314e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.212087637752977e+22\n",
            "E_s_wdiff_all_sq: 8.638088036883652e+18\n",
            "E_IS_SCOPE: -3.71681940797954e+22\n",
            "E_IS_E_SCOPE: -1.2204764859696138e+22\n",
            "Total Loss: -4.531377932708474e+20\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "Var loss:  tensor(-4.7048e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2123902708499142e+22\n",
            "E_s_wdiff_all_sq: 8.733292606359138e+18\n",
            "E_IS_SCOPE: -3.7200627660297574e+22\n",
            "E_IS_E_SCOPE: -1.2219597293820418e+22\n",
            "Total Loss: -4.704813209329766e+20\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "Var loss:  tensor(-4.8790e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2115666260143678e+22\n",
            "E_s_wdiff_all_sq: 8.331541217390411e+18\n",
            "E_IS_SCOPE: -3.724784002286469e+22\n",
            "E_IS_E_SCOPE: -1.2248011196940373e+22\n",
            "Total Loss: -4.8790078506281625e+20\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "Var loss:  tensor(-5.0527e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2106786529175245e+22\n",
            "E_s_wdiff_all_sq: 8.213216407732219e+18\n",
            "E_IS_SCOPE: -3.728333488111103e+22\n",
            "E_IS_E_SCOPE: -1.2268844606422237e+22\n",
            "Total Loss: -5.0527000407833156e+20\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "Var loss:  tensor(-5.2267e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2104740401032292e+22\n",
            "E_s_wdiff_all_sq: 8.510874854617543e+18\n",
            "E_IS_SCOPE: -3.7305213053481672e+22\n",
            "E_IS_E_SCOPE: -1.2278310889669188e+22\n",
            "Total Loss: -5.226736272217169e+20\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "Var loss:  tensor(-5.4007e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2100940052908405e+22\n",
            "E_s_wdiff_all_sq: 8.792421467982654e+18\n",
            "E_IS_SCOPE: -3.7327119683469124e+22\n",
            "E_IS_E_SCOPE: -1.2288382970208966e+22\n",
            "Total Loss: -5.400709937011706e+20\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "Var loss:  tensor(-5.5752e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2093763765539095e+22\n",
            "E_s_wdiff_all_sq: 8.614067751322986e+18\n",
            "E_IS_SCOPE: -3.7365843230369256e+22\n",
            "E_IS_E_SCOPE: -1.2310774985500672e+22\n",
            "Total Loss: -5.5751832899562766e+20\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "Var loss:  tensor(-5.7494e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.208933879273105e+22\n",
            "E_s_wdiff_all_sq: 8.377411254038716e+18\n",
            "E_IS_SCOPE: -3.7408247779311686e+22\n",
            "E_IS_E_SCOPE: -1.2334712859416265e+22\n",
            "Total Loss: -5.7493877794175504e+20\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "Var loss:  tensor(-5.9243e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2093801035991972e+22\n",
            "E_s_wdiff_all_sq: 8.56574149282789e+18\n",
            "E_IS_SCOPE: -3.7437849953760174e+22\n",
            "E_IS_E_SCOPE: -1.2347130385251166e+22\n",
            "Total Loss: -5.924283060731039e+20\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "Var loss:  tensor(-6.0998e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2100305099436887e+22\n",
            "E_s_wdiff_all_sq: 8.85042078515889e+18\n",
            "E_IS_SCOPE: -3.746482005382111e+22\n",
            "E_IS_E_SCOPE: -1.2357091174867993e+22\n",
            "Total Loss: -6.099820738995814e+20\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "Var loss:  tensor(-6.2760e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2099466959183845e+22\n",
            "E_s_wdiff_all_sq: 8.780537904300356e+18\n",
            "E_IS_SCOPE: -3.7502545657424113e+22\n",
            "E_IS_E_SCOPE: -1.2376647901713424e+22\n",
            "Total Loss: -6.276012408223386e+20\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "Var loss:  tensor(-6.4528e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2093927200575749e+22\n",
            "E_s_wdiff_all_sq: 8.502721682101603e+18\n",
            "E_IS_SCOPE: -3.7546498660110006e+22\n",
            "E_IS_E_SCOPE: -1.2401904736392521e+22\n",
            "Total Loss: -6.452765007649303e+20\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "Var loss:  tensor(-6.6288e+20, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2091641687126665e+22\n",
            "E_s_wdiff_all_sq: 8.421209012866424e+18\n",
            "E_IS_SCOPE: -3.758405071735987e+22\n",
            "E_IS_E_SCOPE: -1.2421832478324517e+22\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-01d57a9c684f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_IS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_state_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_first_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_last_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_all_shaping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_IS_SCOPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-5d294114b799>\u001b[0m in \u001b[0;36mtrain_var\u001b[0;34m(model, num_epochs, learning_rate, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Retain the graph to avoid clearing it before backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = train_var(model, 200, 0.0001, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V5HankiakEiu",
        "outputId": "6616ce3c-94b2-4fdf-9833-cd1af30ac8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Var loss:  tensor(-5.0872e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2091641687126665e+22\n",
            "E_s_wdiff_all_sq: 8.421209012866424e+18\n",
            "E_IS_SCOPE: -2.547315218031649e+25\n",
            "E_IS_E_SCOPE: -1.2421832478324517e+22\n",
            "Total Loss: -5.087179914210118e+25\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "Var loss:  tensor(-5.3797e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1588018418409055e+22\n",
            "E_s_wdiff_all_sq: 8.04536804186359e+20\n",
            "E_IS_SCOPE: -2.694291367756086e+25\n",
            "E_IS_E_SCOPE: -1.7349946687008094e+22\n",
            "Total Loss: -5.379704141723259e+25\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "Var loss:  tensor(-5.6711e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.4111873352395464e+22\n",
            "E_s_wdiff_all_sq: 3.463374565893483e+21\n",
            "E_IS_SCOPE: -2.8408122806833696e+25\n",
            "E_IS_E_SCOPE: -2.215367046350286e+22\n",
            "Total Loss: -5.671052464951478e+25\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "Var loss:  tensor(-5.9621e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.9427952560721845e+22\n",
            "E_s_wdiff_all_sq: 7.837952653226776e+21\n",
            "E_IS_SCOPE: -2.987325832672755e+25\n",
            "E_IS_E_SCOPE: -2.682911211818654e+22\n",
            "Total Loss: -5.962145328513011e+25\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "Var loss:  tensor(-6.2530e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.7263608969798527e+22\n",
            "E_s_wdiff_all_sq: 1.375398874050293e+22\n",
            "E_IS_SCOPE: -3.1338148723437466e+25\n",
            "E_IS_E_SCOPE: -3.1355187507019592e+22\n",
            "Total Loss: -6.252982019597437e+25\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "Var loss:  tensor(-6.5435e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 3.748339183415563e+22\n",
            "E_s_wdiff_all_sq: 2.116529777547345e+22\n",
            "E_IS_SCOPE: -3.280275788491791e+25\n",
            "E_IS_E_SCOPE: -3.5795217994547766e+22\n",
            "Total Loss: -6.543549864460832e+25\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "Var loss:  tensor(-6.8338e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 5.004461314920678e+22\n",
            "E_s_wdiff_all_sq: 3.0068462342649105e+22\n",
            "E_IS_SCOPE: -3.4266963700336215e+25\n",
            "E_IS_E_SCOPE: -4.018966356332541e+22\n",
            "Total Loss: -6.833816571742354e+25\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "Var loss:  tensor(-7.1238e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 6.481053799005235e+22\n",
            "E_s_wdiff_all_sq: 4.037113835105185e+22\n",
            "E_IS_SCOPE: -3.57308221505971e+25\n",
            "E_IS_E_SCOPE: -4.452089429979757e+22\n",
            "Total Loss: -7.1238123000895045e+25\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "Var loss:  tensor(-7.4135e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 8.170428479059414e+22\n",
            "E_s_wdiff_all_sq: 5.203125263787489e+22\n",
            "E_IS_SCOPE: -3.7194150366745127e+25\n",
            "E_IS_E_SCOPE: -4.879709737567206e+22\n",
            "Total Loss: -7.413505707716294e+25\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "Var loss:  tensor(-7.7029e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.006562379984491e+23\n",
            "E_s_wdiff_all_sq: 6.5009257231549674e+22\n",
            "E_IS_SCOPE: -3.8656945868055057e+25\n",
            "E_IS_E_SCOPE: -5.302351238455299e+22\n",
            "Total Loss: -7.702901688154831e+25\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "Var loss:  tensor(-7.9920e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.215950259010862e+23\n",
            "E_s_wdiff_all_sq: 7.926284934170995e+22\n",
            "E_IS_SCOPE: -4.0119165595627445e+25\n",
            "E_IS_E_SCOPE: -5.720242133076124e+22\n",
            "Total Loss: -7.991998082195182e+25\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "Var loss:  tensor(-8.2808e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.4444713311004935e+23\n",
            "E_s_wdiff_all_sq: 9.47475141144426e+22\n",
            "E_IS_SCOPE: -4.158076958043214e+25\n",
            "E_IS_E_SCOPE: -6.133460319520498e+22\n",
            "Total Loss: -8.280794013875891e+25\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "Var loss:  tensor(-8.5693e+25, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.691368025139844e+23\n",
            "E_s_wdiff_all_sq: 1.1141678394072568e+23\n",
            "E_IS_SCOPE: -4.3041720954261145e+25\n",
            "E_IS_E_SCOPE: -6.5420000998008646e+22\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5d2902af0be1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_IS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_state_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_first_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_last_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_all_shaping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_IS_SCOPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-5d294114b799>\u001b[0m in \u001b[0;36mtrain_var\u001b[0;34m(model, num_epochs, learning_rate, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Retain the graph to avoid clearing it before backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = train_var(model, 20, 0.001, samples_IS, padded_state_tensors, states_first_tensor, states_last_tensor, samples_all_shaping, samples_IS_SCOPE, test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHLpALXBN40b",
        "outputId": "8cffe722-37df-4139-8ebf-3479a805cb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Var loss:  tensor(-1.3998e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.7000520983736854e+26\n",
            "E_s_wdiff_all_sq: 1.1991158006418987e+26\n",
            "E_IS_SCOPE: -2.0238984353616566e+26\n",
            "E_IS_E_SCOPE: 1.7432066784634587e+24\n",
            "Total Loss: -1.3997894919094345e+26\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "Var loss:  tensor(-1.8930e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.4851380256092592e+26\n",
            "E_s_wdiff_all_sq: 1.0868796875981285e+26\n",
            "E_IS_SCOPE: -2.1642866384407017e+26\n",
            "E_IS_E_SCOPE: 1.6592145943890643e+24\n",
            "Total Loss: -1.892960208309719e+26\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "Var loss:  tensor(-2.3750e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.2864222596610805e+26\n",
            "E_s_wdiff_all_sq: 9.831123864853387e+25\n",
            "E_IS_SCOPE: -2.3071556879644427e+26\n",
            "E_IS_E_SCOPE: 1.5775847477927258e+24\n",
            "Total Loss: -2.3749651779074894e+26\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "Var loss:  tensor(-2.8423e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 2.1107685666169942e+26\n",
            "E_s_wdiff_all_sq: 8.908178298603343e+25\n",
            "E_IS_SCOPE: -2.454130057488368e+26\n",
            "E_IS_E_SCOPE: 1.5012467817496386e+24\n",
            "Total Loss: -2.842277471018134e+26\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "Var loss:  tensor(-3.2993e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.95377408787041e+26\n",
            "E_s_wdiff_all_sq: 8.078126017319421e+25\n",
            "E_IS_SCOPE: -2.6052506542281885e+26\n",
            "E_IS_E_SCOPE: 1.4291329785199847e+24\n",
            "Total Loss: -3.299349729147468e+26\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "Var loss:  tensor(-3.7487e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.8132467079007727e+26\n",
            "E_s_wdiff_all_sq: 7.329651482777037e+25\n",
            "E_IS_SCOPE: -2.7606843344170066e+26\n",
            "E_IS_E_SCOPE: 1.360869471486018e+24\n",
            "Total Loss: -3.748696564283724e+26\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "Var loss:  tensor(-4.1896e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.688620283159106e+26\n",
            "E_s_wdiff_all_sq: 6.659493094301345e+25\n",
            "E_IS_SCOPE: -2.9198032751618074e+26\n",
            "E_IS_E_SCOPE: 1.2967482268817199e+24\n",
            "Total Loss: -4.189637233176863e+26\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "Var loss:  tensor(-4.6262e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.576707664912865e+26\n",
            "E_s_wdiff_all_sq: 6.052083489607422e+25\n",
            "E_IS_SCOPE: -3.08305451841946e+26\n",
            "E_IS_E_SCOPE: 1.2357861039386222e+24\n",
            "Total Loss: -4.626223474250116e+26\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "Var loss:  tensor(-5.0587e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.4763612808585628e+26\n",
            "E_s_wdiff_all_sq: 5.50174234409973e+25\n",
            "E_IS_SCOPE: -3.2499784261118546e+26\n",
            "E_IS_E_SCOPE: 1.1778527193751983e+24\n",
            "Total Loss: -5.058679672152305e+26\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "Var loss:  tensor(-5.4889e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.3848700547605917e+26\n",
            "E_s_wdiff_all_sq: 4.995988928518009e+25\n",
            "E_IS_SCOPE: -3.420172749344094e+26\n",
            "E_IS_E_SCOPE: 1.1220005061754078e+24\n",
            "Total Loss: -5.488883978318761e+26\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "Var loss:  tensor(-5.9179e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.3006442730443957e+26\n",
            "E_s_wdiff_all_sq: 4.527488207429612e+25\n",
            "E_IS_SCOPE: -3.593391101377815e+26\n",
            "E_IS_E_SCOPE: 1.067680399123898e+24\n",
            "Total Loss: -5.917916860890854e+26\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "Var loss:  tensor(-6.3462e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.2229417912969604e+26\n",
            "E_s_wdiff_all_sq: 4.092707164503446e+25\n",
            "E_IS_SCOPE: -3.769466846843736e+26\n",
            "E_IS_E_SCOPE: 1.0146928561134615e+24\n",
            "Total Loss: -6.346181207279818e+26\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "Var loss:  tensor(-6.7612e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.1518335335596193e+26\n",
            "E_s_wdiff_all_sq: 3.6950381982478167e+25\n",
            "E_IS_SCOPE: -3.9421864533730626e+26\n",
            "E_IS_E_SCOPE: 9.636992190084379e+23\n",
            "Total Loss: -6.761198868962662e+26\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "Var loss:  tensor(-7.1583e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.0875381753628806e+26\n",
            "E_s_wdiff_all_sq: 3.3357822679325225e+25\n",
            "E_IS_SCOPE: -4.10933498575679e+26\n",
            "E_IS_E_SCOPE: 9.152111976069571e+23\n",
            "Total Loss: -7.158336651284811e+26\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "Var loss:  tensor(-7.5579e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 1.0300003666921198e+26\n",
            "E_s_wdiff_all_sq: 3.008197928388507e+25\n",
            "E_IS_SCOPE: -4.281029201156582e+26\n",
            "E_IS_E_SCOPE: 8.686640466504931e+23\n",
            "Total Loss: -7.557866476226461e+26\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "Var loss:  tensor(-7.9625e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 9.780866290057678e+25\n",
            "E_s_wdiff_all_sq: 2.706676561722053e+25\n",
            "E_IS_SCOPE: -4.458086666377223e+26\n",
            "E_IS_E_SCOPE: 8.235217772818606e+23\n",
            "Total Loss: -7.962540876273037e+26\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "Var loss:  tensor(-8.3770e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 9.25864062540654e+25\n",
            "E_s_wdiff_all_sq: 2.408043092938196e+25\n",
            "E_IS_SCOPE: -4.639890551795465e+26\n",
            "E_IS_E_SCOPE: 7.762667967592406e+23\n",
            "Total Loss: -8.376953564158955e+26\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "Var loss:  tensor(-8.7964e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 8.772145597482964e+25\n",
            "E_s_wdiff_all_sq: 2.128158973430442e+25\n",
            "E_IS_SCOPE: -4.826000005768793e+26\n",
            "E_IS_E_SCOPE: 7.292373507211456e+23\n",
            "Total Loss: -8.796411091516826e+26\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "Var loss:  tensor(-9.2201e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 8.332681175269682e+25\n",
            "E_s_wdiff_all_sq: 1.8710506668717508e+25\n",
            "E_IS_SCOPE: -5.016565446564469e+26\n",
            "E_IS_E_SCOPE: 6.832218590962401e+23\n",
            "Total Loss: -9.220107950580761e+26\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "Var loss:  tensor(-9.6468e+26, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "E_IS_sq: 5.058576277992128e+22\n",
            "E_IS_all_sq: 2.543768337021951e+22\n",
            "E_s_wdiff_sq: 7.938632829251062e+25\n",
            "E_s_wdiff_all_sq: 1.636020374096819e+25\n",
            "E_IS_SCOPE: -5.210907454597642e+26\n",
            "E_IS_E_SCOPE: 6.382979689732807e+23\n",
            "Total Loss: -9.64684908454528e+26\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[ 0.6824,  0.3805],\n",
            "        [ 0.6172,  0.3862],\n",
            "        [-0.0410, -0.2904],\n",
            "        [ 0.3265, -0.6162],\n",
            "        [-0.4672,  0.3825],\n",
            "        [-0.4019,  0.5381],\n",
            "        [-0.0941, -0.5162],\n",
            "        [-0.5809,  0.0319],\n",
            "        [-0.1979,  0.2670],\n",
            "        [-0.4053, -0.3357],\n",
            "        [ 0.0179,  0.6101],\n",
            "        [ 0.3949,  0.0047],\n",
            "        [ 0.7054,  0.4417],\n",
            "        [-0.3734, -0.1384],\n",
            "        [ 0.1148,  0.4802],\n",
            "        [ 0.3828, -0.3846]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.2569,  0.1246,  0.6364, -0.6841, -0.2801, -0.0636, -0.0846, -0.5966,\n",
            "         0.2269, -0.0934,  0.5676, -0.6006,  0.3885, -0.4913, -0.3735,  0.0440],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 4.8119e-02, -4.0558e-02,  2.2185e-01,  1.7136e-01,  2.0620e-01,\n",
            "         -7.6490e-02, -4.9387e-03,  1.0780e-01, -9.7940e-02, -5.2611e-02,\n",
            "          8.6816e-02,  2.0431e-01, -1.1189e-01, -2.3857e-01,  4.5704e-02,\n",
            "         -2.4755e-02],\n",
            "        [-1.1661e-01,  1.6673e-01, -1.2172e-01, -1.7377e-02, -1.9116e-01,\n",
            "          2.1572e-01,  1.8230e-01, -3.6067e-02, -4.8273e-02,  2.0668e-01,\n",
            "          1.8825e-01, -2.2010e-01,  3.0955e-02, -2.3602e-01,  1.6450e-01,\n",
            "         -1.9474e-01],\n",
            "        [-2.3669e-01, -8.8660e-02,  1.5782e-01,  5.4865e-02, -3.7314e-02,\n",
            "         -2.2701e-01, -1.7717e-01,  1.0917e-01, -1.3188e-01, -2.3322e-01,\n",
            "         -2.0656e-01, -6.6348e-02,  2.9485e-03, -1.5408e-01,  4.4631e-02,\n",
            "          5.9431e-03],\n",
            "        [-3.1699e-02,  2.1269e-01,  6.5254e-02,  1.6674e-01,  1.0271e-01,\n",
            "         -1.5640e-01, -1.1433e-01,  9.1803e-02,  4.8161e-02,  6.3060e-02,\n",
            "         -1.2206e-01,  1.8972e-01, -1.1215e-01, -6.9293e-02, -1.9905e-01,\n",
            "         -8.7023e-05],\n",
            "        [-5.5292e-02, -2.1445e-01,  1.0447e-01,  1.9652e-01, -2.0886e-01,\n",
            "          8.6149e-02, -6.7564e-02,  1.6284e-01,  1.8813e-01,  1.2977e-01,\n",
            "         -1.8909e-03,  1.5335e-01,  2.2191e-01, -2.0367e-01, -1.9575e-01,\n",
            "         -1.1563e-02],\n",
            "        [-1.5999e-01,  3.4475e-02,  1.4412e-01, -2.0440e-01, -2.4086e-01,\n",
            "          4.3020e-03,  7.6393e-02,  2.0897e-01,  6.2470e-02, -1.5482e-01,\n",
            "          1.8559e-01,  1.2675e-01, -3.5940e-02,  1.3949e-01, -2.4669e-01,\n",
            "         -1.6998e-01],\n",
            "        [-1.4121e-03,  9.2029e-02,  1.6774e-02,  2.4159e-01, -1.8972e-01,\n",
            "          2.2045e-01,  2.4686e-01, -1.0914e-02, -1.4103e-01,  9.1213e-02,\n",
            "         -4.3784e-02, -1.6879e-01,  4.5783e-02,  1.8536e-01,  6.9759e-02,\n",
            "          2.2215e-01],\n",
            "        [-1.2859e-01, -9.0884e-02,  5.4626e-02, -3.8355e-02,  1.1648e-01,\n",
            "          7.5265e-02,  2.2921e-01,  2.3633e-01,  1.0943e-01,  9.5796e-02,\n",
            "          9.7141e-02,  1.2300e-01, -2.5879e-01, -1.2350e-01, -1.5009e-02,\n",
            "          1.3043e-02],\n",
            "        [-1.6694e-02,  1.5965e-01, -4.7113e-02, -2.1283e-01,  7.2415e-02,\n",
            "         -1.0634e-01,  7.0834e-02,  1.0853e-01, -1.9885e-01, -2.0899e-01,\n",
            "         -8.6902e-02,  6.2196e-02,  4.3863e-02,  2.2491e-01,  6.7579e-02,\n",
            "         -2.4318e-01],\n",
            "        [ 1.6246e-02, -1.8128e-01, -1.9994e-01,  7.6150e-02, -2.1505e-01,\n",
            "         -2.4253e-01, -7.8125e-02, -2.0187e-01,  7.0338e-03,  5.8107e-02,\n",
            "         -2.5847e-03, -1.5630e-01, -6.0726e-02, -1.1809e-01, -1.6551e-01,\n",
            "         -5.4297e-02],\n",
            "        [ 2.4383e-01,  4.2616e-02,  5.6662e-02,  1.3916e-01,  2.2539e-01,\n",
            "          1.9607e-01, -1.7537e-01, -1.8012e-01, -1.2684e-01, -1.7179e-01,\n",
            "         -3.5418e-02,  1.2716e-01,  1.2756e-01, -5.8339e-02,  9.8414e-02,\n",
            "          2.5362e-02],\n",
            "        [ 1.6507e-01,  1.7497e-01,  6.1446e-02, -9.7664e-02,  1.8058e-01,\n",
            "         -2.5965e-01, -3.1194e-02, -1.4002e-01,  1.0309e-01,  4.9587e-02,\n",
            "          1.3151e-01,  6.2520e-02, -1.0080e-01,  1.8290e-01, -2.2798e-03,\n",
            "          2.0217e-03],\n",
            "        [-1.5221e-01,  1.4097e-01, -7.1615e-04, -6.8548e-02,  2.2222e-01,\n",
            "          1.2362e-01,  1.8278e-01, -2.1061e-01, -7.6397e-02,  1.7362e-01,\n",
            "          4.3270e-02, -2.1091e-01,  4.2701e-02,  1.2101e-01, -4.7703e-02,\n",
            "         -2.0536e-01],\n",
            "        [-6.6673e-02, -4.6614e-02, -2.1227e-01,  2.9418e-02, -2.1981e-01,\n",
            "          1.3636e-01,  1.7628e-01, -1.8667e-02, -1.4600e-01,  1.7796e-01,\n",
            "         -8.2919e-02, -2.2852e-01, -1.9402e-02, -1.0765e-01,  1.1574e-01,\n",
            "         -1.5714e-01],\n",
            "        [-1.0918e-01, -1.9517e-01,  9.8679e-02,  1.5596e-01, -4.4319e-02,\n",
            "         -1.7486e-01, -1.5448e-01,  2.9712e-02, -3.3289e-03,  1.5767e-01,\n",
            "          1.1929e-01, -2.6514e-01, -1.0920e-03,  8.7649e-02,  7.2158e-02,\n",
            "          6.2090e-02],\n",
            "        [-8.3881e-03,  6.5977e-02,  2.1695e-01,  1.9215e-01, -2.9525e-02,\n",
            "          1.7588e-01, -5.8884e-02,  8.2231e-03,  5.6097e-02, -1.6881e-01,\n",
            "         -1.4676e-01, -1.2744e-01, -1.5995e-01, -1.6587e-02, -2.8196e-02,\n",
            "          2.3959e-01],\n",
            "        [ 9.9272e-02, -8.2990e-02, -1.1069e-01,  1.9435e-01,  2.0021e-01,\n",
            "          1.6971e-01,  5.8639e-02, -1.3337e-01, -1.8521e-02,  2.2435e-01,\n",
            "          6.4262e-02,  5.9340e-02, -5.0475e-02, -5.0460e-03,  1.2239e-01,\n",
            "          1.1172e-01],\n",
            "        [ 8.1245e-02,  1.7842e-01, -1.4347e-01, -1.9353e-01, -9.7532e-02,\n",
            "          3.3191e-02,  9.2401e-02,  3.2447e-02, -7.3296e-02,  4.2897e-02,\n",
            "         -5.4902e-03, -1.7552e-01,  2.3623e-01,  1.8997e-01,  1.3470e-01,\n",
            "         -2.2129e-01],\n",
            "        [-1.9724e-01, -8.5698e-02,  4.3844e-02,  1.9982e-01,  2.4789e-01,\n",
            "         -2.3965e-01, -1.8726e-01,  2.2428e-01, -1.4151e-01,  1.6169e-02,\n",
            "          1.3268e-01,  1.4430e-01, -5.2540e-02, -2.0882e-01, -1.0297e-01,\n",
            "          7.5755e-02],\n",
            "        [-1.3038e-01, -2.1669e-01, -5.8820e-02, -9.0053e-02,  2.5145e-01,\n",
            "         -2.2259e-01,  2.0062e-01, -6.5883e-02,  2.1339e-01,  6.7655e-02,\n",
            "         -1.5339e-01,  1.2299e-01, -6.6090e-02, -9.7631e-02,  2.4591e-01,\n",
            "          1.6375e-02],\n",
            "        [ 2.0833e-01,  2.2925e-01, -8.1671e-02, -7.4458e-02,  2.4689e-01,\n",
            "         -1.7912e-01, -3.0479e-02,  2.2088e-01,  4.1135e-02, -1.4143e-01,\n",
            "          1.4276e-02, -4.4181e-02,  2.1406e-02, -8.7408e-02, -2.6004e-02,\n",
            "         -3.1061e-02],\n",
            "        [ 2.7277e-02, -9.2468e-02, -1.8294e-01, -2.0342e-01,  1.7406e-01,\n",
            "          6.5903e-02,  8.9643e-02, -5.2062e-02,  2.6414e-01, -1.2718e-01,\n",
            "         -2.3894e-01,  1.6539e-01,  1.8297e-02,  6.2260e-02, -9.3196e-02,\n",
            "          4.3673e-03],\n",
            "        [-5.9226e-02, -1.7806e-01, -1.5932e-01,  2.5755e-02,  1.8840e-01,\n",
            "         -2.3118e-01,  4.6522e-02, -1.5184e-01,  5.4040e-03, -6.1534e-03,\n",
            "         -7.5576e-02,  1.5076e-01,  1.9803e-01, -5.7331e-02,  1.2015e-03,\n",
            "         -2.8805e-02],\n",
            "        [ 9.5230e-02, -5.9195e-02,  5.9797e-02, -1.4853e-01,  2.2916e-01,\n",
            "          6.8206e-03, -1.0452e-02, -6.4086e-02,  1.3535e-01,  2.1016e-01,\n",
            "         -1.2791e-01,  1.3874e-01,  2.4114e-01,  1.8081e-01,  1.3184e-01,\n",
            "         -2.1979e-01],\n",
            "        [ 1.9239e-01,  3.5063e-02, -1.7934e-01,  1.7502e-01,  8.2594e-02,\n",
            "          1.0295e-01,  2.3758e-01,  1.2021e-01, -5.1508e-02,  1.5860e-01,\n",
            "          1.1935e-01, -7.2050e-02, -1.4643e-01,  2.2494e-01, -1.6159e-01,\n",
            "         -1.2161e-02],\n",
            "        [-1.1569e-01,  2.4863e-01,  8.7303e-02, -4.0421e-02,  1.6243e-01,\n",
            "          1.6153e-02,  2.2084e-01, -1.5133e-01, -3.1824e-02,  1.2209e-01,\n",
            "          2.3849e-01,  3.1212e-02, -4.0532e-02,  1.5685e-01, -1.0802e-02,\n",
            "         -7.3412e-02],\n",
            "        [ 2.5694e-01, -3.9981e-02, -5.8586e-03,  2.5751e-01, -9.3908e-02,\n",
            "         -2.6468e-01, -6.8448e-02,  5.6993e-02, -8.4784e-02,  1.0977e-01,\n",
            "         -9.5266e-03, -4.4528e-02,  5.6235e-02, -3.2194e-02, -1.3303e-01,\n",
            "          7.7084e-02],\n",
            "        [-1.2107e-01, -4.9455e-02, -1.0928e-01, -1.4182e-01, -2.1752e-01,\n",
            "         -1.3493e-01, -2.4229e-01,  6.4488e-02, -6.0827e-02, -9.6155e-02,\n",
            "          4.3416e-02, -1.0943e-01, -8.7071e-02,  1.0680e-01, -1.9074e-01,\n",
            "          1.8180e-01],\n",
            "        [ 1.1885e-01, -1.3959e-01,  1.3637e-01, -1.7812e-01,  1.7236e-01,\n",
            "          1.7322e-01,  1.5212e-01, -4.9635e-02,  1.2627e-02, -1.7094e-01,\n",
            "          5.6883e-02,  1.1782e-01,  2.6690e-01,  1.5117e-01, -1.3883e-01,\n",
            "         -7.5040e-02],\n",
            "        [ 1.7444e-02, -3.5884e-02, -1.8073e-01, -4.8182e-02,  5.1672e-02,\n",
            "         -2.1597e-01,  1.5905e-02, -2.4473e-01,  2.2163e-01,  6.5373e-02,\n",
            "          1.5828e-01,  4.6021e-02,  2.7998e-02,  1.6024e-01,  2.1393e-01,\n",
            "          1.7622e-02],\n",
            "        [-2.4343e-02,  1.6148e-02,  1.8917e-02,  4.6976e-02,  8.1570e-02,\n",
            "         -7.2976e-02, -8.2241e-02,  9.8812e-02,  1.2289e-01,  1.7245e-01,\n",
            "         -1.7320e-01, -2.4167e-01,  1.7690e-01, -7.3494e-02,  1.4066e-01,\n",
            "         -2.3803e-01],\n",
            "        [-1.9888e-01, -1.8503e-01,  8.4259e-02,  2.4136e-01,  5.5319e-02,\n",
            "         -1.5181e-01,  8.2156e-02, -1.8184e-01, -7.5127e-02, -2.1625e-01,\n",
            "          1.1747e-01, -4.3509e-02, -2.2899e-01,  9.2018e-02,  1.9127e-01,\n",
            "         -1.1790e-01]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.0860, -0.2362, -0.2142,  0.1716,  0.1689,  0.2149, -0.0474, -0.1591,\n",
            "        -0.1497, -0.0224, -0.1051, -0.1015,  0.1446,  0.1406,  0.2544, -0.1735,\n",
            "        -0.1481, -0.1807,  0.0241, -0.2079,  0.2115,  0.0250,  0.1389,  0.2335,\n",
            "         0.2197,  0.1802,  0.2623, -0.1372,  0.0478,  0.2256,  0.0486, -0.0534],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.1721,  0.0623, -0.0768,  0.1644,  0.0325, -0.0332, -0.0931,  0.0436,\n",
            "          0.0268, -0.0838, -0.1538, -0.0204,  0.0539, -0.0472,  0.1059, -0.0720,\n",
            "          0.1543, -0.0265,  0.0777,  0.1012,  0.1266,  0.0518, -0.1264, -0.1316,\n",
            "         -0.0902, -0.0639, -0.1390,  0.0065,  0.1957, -0.0794,  0.0822,  0.1624]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.1091], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working on prep"
      ],
      "metadata": {
        "id": "O36PGywGna2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Working on padding trajectories to allow for easier optimization\n",
        "\n",
        "# def pad_trajectories(pi_b):\n",
        "#     # Find the maximum length among all trajectories\n",
        "#     max_length = max(len(traj) for traj in pi_b)\n",
        "\n",
        "#     # Define the padding value\n",
        "#     padding_value = np.array([np.array([0, 0]), 0, 0, np.array([0, 0]), 0, 0], dtype=object)\n",
        "\n",
        "#     # Pad each trajectory to match the maximum length\n",
        "#     padded_pi_b = [traj + [padding_value] * (max_length - len(traj)) for traj in pi_b]\n",
        "\n",
        "#     return padded_pi_b"
      ],
      "metadata": {
        "id": "ErjhWCq3czmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pi_b_padded = pad_trajectories(pi_b)"
      ],
      "metadata": {
        "id": "31TWTP7Eb1sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_terms_tens(eval_policy, behav_policy, behavior_policies):\n",
        "  # Initialize lists to store axis data for each policy\n",
        "  timesteps = []\n",
        "  states = []\n",
        "  state_first = []\n",
        "  state_last = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  gamma_last = []\n",
        "  weight_last = []\n",
        "  weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "  psi = []\n",
        "\n",
        "  for index, policy in enumerate(behavior_policies):\n",
        "      policy_array = np.array(policy)\n",
        "      timesteps.append(policy_array[:, 4].astype(int))\n",
        "      # s.append(policy_array[:, 0])\n",
        "\n",
        "      # last timestep for gamma\n",
        "      gamma_last.append(len(policy))\n",
        "      # last importance weight\n",
        "      weight_last.append(weights[index][-1])\n",
        "\n",
        "\n",
        "      states.append(policy_array[:, 0][1:])\n",
        "      psi.append(policy_array[:,5][1:])\n",
        "      state_first.append(policy_array[:,0][0])\n",
        "      state_last.append(policy_array[:,0][-1])\n",
        "      actions.append(policy_array[:, 1])\n",
        "      rewards.append(policy_array[:, 2].astype(float))\n",
        "\n",
        "  weights_difference = []\n",
        "  for index, weight in enumerate(weights):\n",
        "    # diff = np.array(w[index][:-1]) - np.array(w[index][1:])\n",
        "    diff = np.array(weight[:-1]) - np.array(weight[1:])\n",
        "    weights_difference.append(diff)\n",
        "\n",
        "  return timesteps, states, state_first, state_last, actions, rewards, gamma_last, weight_last, weights, weights_difference"
      ],
      "metadata": {
        "id": "Kcx483JVfDWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t, s, s_f, s_l, a, r, g_l, w_l, w, w_diff = variance_terms_tens(P_pi_e, P_pi_b, pi_b)"
      ],
      "metadata": {
        "id": "q8d3KfusfsZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps, states, states_first, states_last, actions, rewards, gamma_last, weights_last, weights, weights_difference = variance_terms_tens(P_pi_e, P_pi_b, pi_b)"
      ],
      "metadata": {
        "id": "BAqNV3alEAjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def padding_IS_terms(timesteps, actions, rewards, weights):\n",
        "    # Find the maximum length among all lists\n",
        "    max_length = max(len(traj) for traj in timesteps)\n",
        "\n",
        "    # Define the padding values\n",
        "    zero_padding = 0\n",
        "\n",
        "    # Pad each list to match the maximum length\n",
        "    padded_timesteps = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in timesteps]\n",
        "    padded_rewards = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in rewards]\n",
        "    padded_actions = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in actions]\n",
        "    padded_weights = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in weights]\n",
        "\n",
        "    return padded_timesteps, padded_rewards, padded_actions, padded_weights"
      ],
      "metadata": {
        "id": "AuuBKl0joAEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_timesteps, padded_rewards, padded_actions, padded_weights = padding_IS_terms(timesteps, actions, rewards, weights)"
      ],
      "metadata": {
        "id": "xD675spBnrP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_states_weights_difference(states, weights_difference):\n",
        "    # Find the maximum length of trajectories\n",
        "    max_length = max(len(trajectory) for trajectory in states)\n",
        "\n",
        "    zero_padding = 0\n",
        "\n",
        "    # Pad each trajectory to make them all the same length\n",
        "    padded_states = [\n",
        "        [list(item) for item in trajectory] + [[0, 0]] * (max_length - len(trajectory))\n",
        "        for trajectory in states\n",
        "    ]\n",
        "\n",
        "    padded_weights_difference = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in weights_difference]\n",
        "\n",
        "    return padded_states, padded_weights_difference"
      ],
      "metadata": {
        "id": "4DekSrOTzLAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_states, padded_weights_difference = padding_states_weights_difference(states, weights_difference)"
      ],
      "metadata": {
        "id": "aX6DIhEQTCVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorize_padded_terms(padded_states, padded_weights_difference):\n",
        "  padded_state_tensors = torch.tensor(padded_states, dtype = torch.float64)\n",
        "  padded_weight_diff_tensors = torch.tensor(padded_weights_difference, dtype = torch.float64)\n",
        "  padded_weight_diff_tensors = padded_weight_diff_tensors.unsqueeze(-1)\n",
        "\n",
        "  return padded_state_tensors, padded_weight_diff_tensors\n"
      ],
      "metadata": {
        "id": "hHZhHnuUzHFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_state_tensors, padded_weight_diff_tensors = tensorize_padded_terms(padded_states, padded_weights_difference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5kb1vOxTZSu",
        "outputId": "6b5b983d-35ca-4d5f-9ff4-1cba9d74450b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-3c22136e8020>:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  padded_weight_diff_tensors = torch.tensor(padded_weights_difference, dtype = torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorize_last_and_first_terms(states_first, states_last, gamma_last, weights_last):\n",
        "  states_first_tensor = torch.tensor(states_first, dtype = torch.float64)\n",
        "  states_last_tensor = torch.tensor(states_last, dtype = torch.float64)\n",
        "  gamma_last_tensor = torch.tensor(gamma_last, dtype = torch.float64)\n",
        "  weights_last_tensor = torch.tensor(weights_last, dtype = torch.float64)\n",
        "\n",
        "  return states_first_tensor, states_last_tensor, gamma_last_tensor, weights_last_tensor"
      ],
      "metadata": {
        "id": "AqtoK6MEmPWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_first_tensor, states_last_tensor, gamma_last_tensor, weights_last_tensor = tensorize_last_and_first_terms(states_first, states_last, gamma_last, weights_last)"
      ],
      "metadata": {
        "id": "gvE-AddhDTkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_IS_terms(gamma, timesteps, rewards, weights):\n",
        "  gtrw = np.power(gamma, timesteps)*rewards*weights\n",
        "\n",
        "  IS_tensor = torch.sum(torch.tensor(gtrw, dtype = torch.float32), dim = 1, keepdim = True)\n",
        "\n",
        "  return IS_tensor\n"
      ],
      "metadata": {
        "id": "YVpa0bTwd3HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS_tensor = calc_IS_terms(0.9, padded_timesteps, padded_rewards, padded_weights)"
      ],
      "metadata": {
        "id": "xjRU7xlj89PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.finfo(torch.float64).max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGtcFc-YY9SK",
        "outputId": "ce82a7a6-848c-4034-f260-6a3bf7702324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7976931348623157e+308"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# samples_IS[0]"
      ],
      "metadata": {
        "id": "s_JAi7apXuJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gamma_weight_last(gamma, gamma_last, weights_last):\n",
        "  gamma_weight_last = np.power(gamma, gamma_last)*weights_last\n",
        "\n",
        "  gamma_weight_last_tensor = torch.tensor(gamma_weight_last, dtype = torch.float64).unsqueeze(-1)\n",
        "\n",
        "  return gamma_weight_last_tensor"
      ],
      "metadata": {
        "id": "8I3qisEpsYzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma_weights_last_tensor = calc_gamma_weight_last(0.9, gamma_last, weights_last)"
      ],
      "metadata": {
        "id": "XHDztxg4C3So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_IS_terms(IS_tensor, num_samples):\n",
        "  seed = 42\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  num_bootstraps = num_samples*len(IS_tensor)\n",
        "\n",
        "  # Sample indices with replacement\n",
        "  sampled_indices = torch.randint(0, len(IS_tensor), size=(num_bootstraps,), dtype=torch.long)\n",
        "\n",
        "  # new_size = (num_samples, IS_tensor.shape[0], IS_tensor.shape[1])\n",
        "  new_size = (num_samples, IS_tensor.shape[0])\n",
        "\n",
        "  IS_bootstraps = IS_tensor[sampled_indices].view(new_size)\n",
        "\n",
        "  # sampled_tensor = IS_bootstraps.view(new_size)\n",
        "\n",
        "  return IS_bootstraps"
      ],
      "metadata": {
        "id": "rdqtMRAT35lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_IS = bootstrap_IS_terms(IS_tensor, 10000)"
      ],
      "metadata": {
        "id": "MHOyNeJ-UdcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp_large_terms(input_term):\n",
        "  max_float64 = torch.finfo(torch.float64).max\n",
        "  return torch.clamp(input_term, min=-max_float64, max=max_float64)\n"
      ],
      "metadata": {
        "id": "ng9lLOfsGvdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp_large_terms(input_term):\n",
        "    max_float64 = torch.tensor(torch.finfo(torch.float64).max, dtype=torch.float64)\n",
        "    min_value = torch.tensor(-1e50, dtype=torch.float64)\n",
        "    return torch.clamp(input_term, min=min_value, max=max_float64)\n",
        "\n",
        "    # return torch.clamp(input_term, min=-max_float64, max=max_float64)\n"
      ],
      "metadata": {
        "id": "C1Su5BQ0cfUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomizableFeatureNet_d(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_prob=0.2, dtype=torch.float32):\n",
        "        super(CustomizableFeatureNet_d, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # Create the hidden layers based on the provided sizes\n",
        "        for in_dim, out_dim in zip([input_dim] + hidden_dims, hidden_dims):\n",
        "            self.hidden_layers.append(nn.Linear(in_dim, out_dim).to(dtype))\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim).to(dtype)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = F.relu(layer(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YFqvRa0LS7OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomizableFeatureNet_d(input_dim=2, hidden_dims=[16, 32], output_dim=1, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "udif1c2cTBzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 32], output_dim=1)"
      ],
      "metadata": {
        "id": "ct8DQIU6rUvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor):\n",
        "  # Get model outputs for states\n",
        "  states_output = model(padded_state_tensors)\n",
        "  states_first_output = model(states_first_tensor)\n",
        "  states_last_output = model(states_last_tensor)\n",
        "  return states_output, states_first_output, states_last_output"
      ],
      "metadata": {
        "id": "0JNHRYkBmZ5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_output, states_first_output, states_last_output = pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)"
      ],
      "metadata": {
        "id": "duNRe00YE34A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def states_weight_diff_sums(states_output, padded_weight_diff_tensors):\n",
        "  states_weight_diff = states_output * padded_weight_diff_tensors\n",
        "  sums_states_weight_diff = torch.sum(states_weight_diff, dim =1)\n",
        "\n",
        "  return sums_states_weight_diff"
      ],
      "metadata": {
        "id": "8nZkrdWPW5AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sums_states_weight_diff = states_weight_diff_sums(states_output, padded_weight_diff_tensors)"
      ],
      "metadata": {
        "id": "nlxvS4QpYW0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def last_first_terms_operations(gamma_weights_last_tensor, states_last_output, states_first_output):\n",
        "  gamma_weights_states_last_sub_states_first = gamma_weights_last_tensor*states_last_output -  states_first_output\n",
        "\n",
        "  return gamma_weights_states_last_sub_states_first"
      ],
      "metadata": {
        "id": "fDDD0NcUaxpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma_weights_states_last_sub_states_first = last_first_terms_operations(gamma_weights_last_tensor, states_last_output, states_first_output)"
      ],
      "metadata": {
        "id": "ScpV8dhaCqHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gamma_weights_states_last_sub_states_first.shape"
      ],
      "metadata": {
        "id": "oC-P2rDqFIwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_shaping_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, num_samples):\n",
        "\n",
        "  seed = 42\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  num_bootstraps = num_samples*sums_states_weight_diff.shape[0]\n",
        "\n",
        "  # Sample indices with replacement\n",
        "  sampled_indices = torch.randint(0, len(sums_states_weight_diff), size=(num_bootstraps,), dtype=torch.long)\n",
        "\n",
        "  # sizes\n",
        "  # size_states_weights_diff = (num_samples, states_output.shape[0], states_output.shape[1])\n",
        "  reshaped_size = (num_samples, sums_states_weight_diff.shape[0])\n",
        "\n",
        "  # Resize samples to shape num_samples x num_trajectories x length_padded_trajectories\n",
        "  # samples_states_output = states_output[sampled_indices].view(size_states_weights_diff)\n",
        "  # samples_weight_diff = padded_weight_diff_tensors[sampled_indices].view(size_states_weights_diff)\n",
        "\n",
        "  # Resize samples to shape num_samples x num_trajectories\n",
        "  sample_sums_states_weight_diff = sums_states_weight_diff[sampled_indices].view(reshaped_size)\n",
        "  # samples_states_first_output = sums_states_weight_diff[sampled_indices].view(reshaped_size)\n",
        "  # samples_states_last_output = states_last_output[sampled_indices].view(reshaped_size)\n",
        "  samples_gamma_weight_states_last_sub_states_first = gamma_weights_states_last_sub_states_first[sampled_indices].view(reshaped_size)\n",
        "\n",
        "\n",
        "  return sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first\n"
      ],
      "metadata": {
        "id": "d3UM9eUGeQdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first = bootstrap_shaping_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, 10000)"
      ],
      "metadata": {
        "id": "whCU0iyDCi8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def gtrw_plus_states_weight_diff_min_last_firsts_terms(gtrw_tensor, sums_states_weight_diff, gamma_weights_states_last_sub_states_first):\n",
        "#   sum_IS_and_shaping = gtrw_tensor + sums_states_weight_diff + gamma_weights_states_last_sub_states_first\n",
        "\n",
        "#   return sum_IS_and_shaping"
      ],
      "metadata": {
        "id": "sIw1SreEnUpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def bootstrap_IS_shaping_terms(gtrw_tensor, sums_states_weight_diff, gamma_weights_states_last_sub_states_first):\n",
        "#   seed = 42\n",
        "#   torch.manual_seed(seed)\n",
        "\n",
        "#   num_bootstraps = num_samples*sums_states_weight_diff.shape[0]\n",
        "\n",
        "#   # Sample indices with replacement\n",
        "#   sampled_indices = torch.randint(0, len(sums_states_weight_diff), size=(num_bootstraps,), dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "OZHZzE6d58xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTVuyCVBB4-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clamp_large_terms(torch.mean(clamp_large_terms(torch.mean(samples_IS, dim = 1))))"
      ],
      "metadata": {
        "id": "tdFIzbNUgx94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clamp_large_terms(torch.mean(clamp_large_terms(torch.mean(samples_IS, dim =1)**2)))"
      ],
      "metadata": {
        "id": "4dLA0DBLh-pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clamp_large_terms(torch.mean(clamp_large_terms(torch.mean(samples_IS, dim =1))**2) - torch.mean(clamp_large_terms(torch.mean(samples_IS, dim = 1))))"
      ],
      "metadata": {
        "id": "7pfgg5rZlakd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCOPE = sample_sums_states_weight_diff+samples_gamma_weight_states_last_sub_states_first"
      ],
      "metadata": {
        "id": "MWwuJS4En1ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.mean(SCOPE,dim =1).shape"
      ],
      "metadata": {
        "id": "ECEj-Lhooh8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(clamp_large_terms(samples_IS), dim = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6YXvZmXYUKd",
        "outputId": "d5a030e2-5c73-48bd-b58f-8e4970fbe046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([      -inf, 2.0056e+14,       -inf,  ...,       -inf,       -inf,\n",
              "        1.0028e+14])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_shaped_variance(samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first):\n",
        "\n",
        "\n",
        "  # states_output, states_first_output, states_last_output = pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)\n",
        "\n",
        "\n",
        "\n",
        "  # Begin calcs without clamping\n",
        "\n",
        "  # IS\n",
        "  E_IS_sq = torch.mean(torch.mean(clamp_large_terms(samples_IS), dim = 1)**2)\n",
        "  E_IS_all_sq = torch.mean(torch.mean(clamp_large_terms(samples_IS), dim = 1))**2\n",
        "\n",
        "  # states_weight_diff\n",
        "  E_s_wdiff_sq = torch.mean(torch.mean(clamp_large_terms(sample_sums_states_weight_diff), dim =1)**2)\n",
        "  E_s_wdiff_all_sq = torch.mean(torch.mean(clamp_large_terms(sample_sums_states_weight_diff), dim = 1))**2\n",
        "\n",
        "  # all terms\n",
        "  SCOPE = clamp_large_terms(sample_sums_states_weight_diff)+clamp_large_terms(samples_gamma_weight_states_last_sub_states_first)\n",
        "  E_IS_SCOPE = torch.mean(torch.mean(clamp_large_terms(samples_IS), dim =1) * torch.mean(SCOPE, dim =1))\n",
        "  E_IS_E_SCOPE = torch.mean(torch.mean(clamp_large_terms(samples_IS),dim = 1 ))*torch.mean(torch.mean(SCOPE, dim =1))\n",
        "\n",
        "  SCOPE_variance = E_IS_sq + 2*E_IS_SCOPE + E_s_wdiff_sq - E_IS_all_sq - 2*E_IS_E_SCOPE - E_IS_E_SCOPE\n",
        "\n",
        "  IS_variance = E_IS_sq - E_IS_all_sq\n",
        "\n",
        "  return IS_variance, SCOPE_variance\n"
      ],
      "metadata": {
        "id": "LLuzewmQ3DR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS_variance, SCOPE_variance = calculate_shaped_variance(samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first)"
      ],
      "metadata": {
        "id": "Nz9ApTk-qnx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(torch.mean(clamp_large_terms(samples_IS), dim = 1)**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN1zDrJWo4wZ",
        "outputId": "548dc965-3831-4791-9077-d8d8de7062d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(inf)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPE_variance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBjjOVEggKhd",
        "outputId": "fdeba7f8-0ed2-498d-f842-25fe37e45dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(nan, dtype=torch.float64, grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_variance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyMAdyWjgb4b",
        "outputId": "fcadf31c-726b-4846-b87e-576e2d0c1d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(nan)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def clamp_large_terms(input_term):\n",
        "    max_float64 = torch.tensor(torch.finfo(torch.float64).max, dtype=torch.float64)\n",
        "    # min_value = torch.tensor(-1e38, dtype=torch.float64)\n",
        "    min_value = torch.tensor(-9e-38, dtype=torch.float64)\n",
        "\n",
        "    # min_value = torch.tensor(-1.7e+308, dtype = torch.float64)\n",
        "    # min_value = torch.tensor(torch.finfo(torch.float64).min, dtype=torch.float64)\n",
        "\n",
        "\n",
        "    # Using torch.where to explicitly set values outside the desired range\n",
        "    clamped_result = torch.where(input_term < min_value, min_value, input_term)\n",
        "    clamped_result = torch.where(clamped_result > max_float64, max_float64, clamped_result)\n",
        "\n",
        "    return clamped_result\n",
        "clamp_large_terms(samples_IS)[0].min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDs0epOwdjjK",
        "outputId": "70d2de61-11b2-465f-a114-7527e815ead9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-9.0000e-38)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_IS[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGlLqnsIhLAr",
        "outputId": "dbacfd91-4d5b-48ae-8d2b-c36b24042d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00, -9.5567e-34, -1.1020e-12, -5.7703e-20,  0.0000e+00,\n",
              "        -6.6609e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.4562e-17,\n",
              "        -1.3518e-33, -6.3068e-29, -1.1566e-08,  0.0000e+00, -3.2522e-24,\n",
              "         0.0000e+00,  0.0000e+00, -4.0924e-02,  0.0000e+00,  0.0000e+00,\n",
              "        -6.0833e-21, -1.1020e-12,  0.0000e+00, -3.5032e-43, -3.4013e-37,\n",
              "        -7.9832e-23,  0.0000e+00, -2.2156e-12,  0.0000e+00, -7.3671e-27,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -3.0953e-12,  0.0000e+00, -7.1745e-11,  0.0000e+00,\n",
              "        -1.2184e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -4.8370e-19,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.7865e-25, -3.0953e-12, -3.1972e-14,\n",
              "         0.0000e+00, -3.9939e-21,  0.0000e+00, -4.2383e-17,  0.0000e+00,\n",
              "         0.0000e+00, -9.5567e-34,  0.0000e+00,  0.0000e+00, -5.4262e-07,\n",
              "        -3.7487e-37, -2.6165e-41,  0.0000e+00, -1.0701e-07,  0.0000e+00,\n",
              "         0.0000e+00, -6.6945e-22,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.3046e-34, -1.7190e-19,  0.0000e+00,\n",
              "         9.6867e-18, -4.6108e-20,  1.3313e-32, -4.5634e-22,  0.0000e+00,\n",
              "         0.0000e+00, -1.9068e-20, -3.2430e-22, -8.2104e-17,  0.0000e+00,\n",
              "        -1.0108e-29,  0.0000e+00, -8.4772e-20,  0.0000e+00, -2.0319e-39,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -6.3362e-38,  0.0000e+00,\n",
              "         4.7220e-16,  0.0000e+00, -1.5748e-17,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.3046e-34,  3.4275e-23,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2642e-11,\n",
              "        -2.0774e-08,  0.0000e+00, -2.4060e-40, -3.5032e-43,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -9.3884e-01, -8.4772e-20, -2.5036e-39,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.3247e-35,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -3.8730e-15, -1.0150e-03, -2.0038e-36,  0.0000e+00,\n",
              "         0.0000e+00, -8.4655e-19,  0.0000e+00, -7.3035e-20, -6.6609e-06,\n",
              "         0.0000e+00,  0.0000e+00, -1.6692e-14, -2.6013e-38,  0.0000e+00,\n",
              "         0.0000e+00, -5.6024e-35, -4.5634e-22,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8727e-10, -2.2156e-12,\n",
              "         0.0000e+00,  0.0000e+00,  5.1149e-40,  0.0000e+00,  0.0000e+00,\n",
              "        -1.3400e-08,  0.0000e+00, -2.6687e-08,  0.0000e+00, -4.5020e-30,\n",
              "        -1.0701e-07, -1.0052e-13,  0.0000e+00,  0.0000e+00, -4.5720e-36,\n",
              "        -2.7511e-36,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -9.6992e-23,  0.0000e+00,  0.0000e+00,  1.1169e-32,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2418e-12,  3.4265e-14,\n",
              "         0.0000e+00,  0.0000e+00, -1.6255e-43, -1.1568e-11,  0.0000e+00,\n",
              "        -2.5918e-04, -4.2958e-02, -1.4635e-26,  0.0000e+00, -8.7134e-11,\n",
              "         3.6282e-29,  0.0000e+00,  0.0000e+00, -1.2983e-17,  0.0000e+00,\n",
              "         0.0000e+00, -1.2418e-12, -1.6759e-21,  0.0000e+00,  0.0000e+00,\n",
              "        -1.9035e-38, -2.1801e-26,  0.0000e+00, -1.2909e-23, -4.1059e-16,\n",
              "        -2.4789e-27, -1.0616e-20,  0.0000e+00, -2.5688e-29, -5.9457e-04,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -1.4309e-17,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -3.0097e+06,  0.0000e+00, -3.9645e-25, -8.8058e-20,  1.9898e-43,\n",
              "        -2.2156e-12,  0.0000e+00,  0.0000e+00, -3.8730e-15,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -2.4202e-34,  0.0000e+00,  0.0000e+00, -3.2028e-23,\n",
              "         0.0000e+00,  0.0000e+00, -3.9735e-34,  0.0000e+00, -3.8966e-08,\n",
              "         0.0000e+00, -2.2030e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -3.5367e-40,  0.0000e+00,  0.0000e+00,\n",
              "        -7.4222e-38,  0.0000e+00,  0.0000e+00, -9.4159e-16,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -2.1972e-42, -8.4772e-20,  0.0000e+00,\n",
              "         0.0000e+00, -2.0903e-15,  0.0000e+00,  0.0000e+00, -2.5223e-44,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4013e-45,  0.0000e+00,\n",
              "        -1.7457e-24,  0.0000e+00, -1.0543e-37, -1.6182e-33, -3.5188e-28,\n",
              "        -8.2104e-17,  0.0000e+00, -3.8662e-06,  0.0000e+00,  0.0000e+00,\n",
              "        -2.4441e-09, -8.4772e-20,  0.0000e+00,  0.0000e+00, -9.1102e-12,\n",
              "         0.0000e+00, -3.0097e+06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -4.5720e-36,  0.0000e+00, -3.6430e-12,  0.0000e+00,  0.0000e+00,\n",
              "        -1.6816e-44,  1.9004e-41, -4.8050e-26, -2.8832e-06, -1.4013e-45,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4110e-04,\n",
              "         0.0000e+00,  0.0000e+00, -5.9390e-13,  0.0000e+00,  0.0000e+00,\n",
              "        -4.5634e-22,  0.0000e+00, -3.4013e-37,  0.0000e+00,  0.0000e+00,\n",
              "         1.9898e-43,  0.0000e+00, -4.2383e-17,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.2412e-14, -5.9457e-04,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.0701e-07,  5.1149e-40,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5719e-17,\n",
              "         0.0000e+00,  0.0000e+00, -6.4470e-14, -1.3247e-35,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -9.8613e-27, -3.9645e-25, -1.5112e-30, -1.1566e-08,  0.0000e+00,\n",
              "         0.0000e+00, -1.5719e-17,  0.0000e+00, -1.3828e-31,  0.0000e+00,\n",
              "        -4.5551e-22, -1.6255e-43, -1.9179e-27,  4.6289e-17, -6.6137e-04,\n",
              "        -8.4772e-20,  0.0000e+00,  0.0000e+00, -8.2205e-14, -1.2163e-08,\n",
              "         0.0000e+00,  0.0000e+00, -1.2418e-12, -1.0543e-37,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  2.8294e-34, -4.5634e-22,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -7.7118e-30,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.3518e-33,  0.0000e+00,  0.0000e+00,\n",
              "        -1.8627e-28, -3.9645e-25,  0.0000e+00,  0.0000e+00, -2.1972e-42,\n",
              "         0.0000e+00, -2.4485e-09, -1.0150e-03,  0.0000e+00, -1.7865e-25,\n",
              "         0.0000e+00,        -inf,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -6.7035e-32,  0.0000e+00,  0.0000e+00, -6.3710e-32,\n",
              "        -6.6137e-04, -6.5706e-01,  0.0000e+00, -1.5719e-17,  0.0000e+00,\n",
              "         0.0000e+00,  5.1149e-40,  0.0000e+00, -7.1308e-38, -1.3518e-33,\n",
              "         0.0000e+00,  0.0000e+00, -1.0150e-03,  0.0000e+00, -5.4345e-13,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.3034e-30,\n",
              "        -8.1634e-13,  0.0000e+00,  1.7343e-39,  0.0000e+00, -3.7737e+01,\n",
              "        -1.6692e-14,  0.0000e+00,  0.0000e+00, -1.1301e-18,  0.0000e+00,\n",
              "        -1.0095e-10, -8.0009e-07,  0.0000e+00, -2.4070e-08, -2.6165e-41,\n",
              "        -3.0533e-07,  0.0000e+00, -2.6325e-14,  0.0000e+00,  0.0000e+00,\n",
              "        -6.7686e-14,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.3188e+01,  0.0000e+00, -3.4013e-37,\n",
              "         0.0000e+00,  7.1993e-41, -2.2864e-35, -6.5861e-44, -4.9438e-23,\n",
              "        -2.0903e-15,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -4.6133e-21,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.8006e-24,\n",
              "         0.0000e+00,  0.0000e+00, -6.3710e-32,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -6.0236e-15,  0.0000e+00, -2.5559e-05, -3.8831e-13,\n",
              "         0.0000e+00,  0.0000e+00, -1.2184e+02,  0.0000e+00, -1.0090e+01,\n",
              "        -5.2582e-35, -1.3873e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         1.6419e-27, -8.8237e-12, -1.2808e-33, -4.6133e-21,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  9.6867e-18,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -2.4571e-31,  0.0000e+00,  1.3201e-38,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -4.1568e-11,  0.0000e+00,\n",
              "        -1.4013e-45, -1.0701e-07,  3.2638e-35,  0.0000e+00,  0.0000e+00,\n",
              "         3.4265e-14, -2.5223e-44,  0.0000e+00, -1.7958e-41, -2.4441e-09,\n",
              "         0.0000e+00, -8.0009e-07, -1.4125e-01, -1.0090e+01,  0.0000e+00,\n",
              "        -2.2156e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4070e-08,\n",
              "        -4.5634e-22, -1.3188e+01,  0.0000e+00,  0.0000e+00, -5.0588e-29,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3046e-34,\n",
              "        -2.3189e-15,  0.0000e+00, -2.9159e+00, -4.8370e-19, -1.0197e-32,\n",
              "         0.0000e+00, -4.2383e-17,  1.0241e-28,  0.0000e+00, -3.0533e-07,\n",
              "         0.0000e+00,  0.0000e+00, -1.0150e-03,  0.0000e+00, -9.1102e-12,\n",
              "         0.0000e+00, -2.6325e-14, -1.4617e-36, -3.8065e-38,  0.0000e+00,\n",
              "        -5.4262e-07, -1.0095e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -6.6729e-06,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -4.5634e-22,  0.0000e+00, -9.8748e-13,  0.0000e+00,\n",
              "        -1.8556e-23,  0.0000e+00, -5.4529e-33,  0.0000e+00, -3.2522e-24,\n",
              "         1.1032e-38, -1.5748e-17,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -7.9976e-23, -3.2028e-23, -9.3884e-01,  0.0000e+00,  0.0000e+00,\n",
              "        -3.4013e-37,  2.8026e-45,  0.0000e+00, -6.8054e-14,  0.0000e+00,\n",
              "        -1.0145e-30, -4.4114e-29, -6.0833e-21,  0.0000e+00, -1.2272e+09,\n",
              "         0.0000e+00, -8.2104e-17,  0.0000e+00, -6.1665e+15, -1.7958e-41,\n",
              "        -2.5223e-44, -6.8813e-12, -3.8615e-09,  0.0000e+00, -4.6133e-21,\n",
              "         0.0000e+00,  0.0000e+00, -3.5367e-40, -5.1789e+02, -7.3035e-20,\n",
              "         0.0000e+00, -2.3485e-09, -1.0284e-27, -4.5693e-33,  0.0000e+00,\n",
              "        -2.0038e-36, -1.5719e-17,  0.0000e+00, -5.8006e-24, -1.7944e-15,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.8730e-15,\n",
              "         0.0000e+00,  0.0000e+00, -1.0701e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -1.4013e-45, -7.9832e-23,  0.0000e+00, -1.3046e-34,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5112e-30,\n",
              "        -5.8855e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.7035e-32,\n",
              "         3.4275e-23,  0.0000e+00, -3.5486e-18,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4617e-36,  0.0000e+00,\n",
              "         0.0000e+00, -3.0533e-07, -1.4830e-20, -8.4772e-20,  0.0000e+00,\n",
              "        -6.8054e-14,  0.0000e+00, -2.0976e-10,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -1.0095e-10, -8.4655e-19, -9.5567e-34,  0.0000e+00,\n",
              "        -2.5559e-05, -1.4857e-20,  0.0000e+00,  0.0000e+00, -2.2156e-12,\n",
              "        -1.5748e-17,  0.0000e+00, -2.2156e-12, -2.8727e-10,  0.0000e+00,\n",
              "        -6.2816e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6129e-11,\n",
              "        -1.2412e-14,  0.0000e+00, -4.7570e-17,  0.0000e+00, -2.4571e-31,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.5551e-22,\n",
              "        -1.0701e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -5.6052e-45, -3.8615e-09,  0.0000e+00, -6.6945e-22,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4857e-20,  0.0000e+00,\n",
              "        -1.9068e-20, -1.4830e-20,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -1.4309e-17, -1.7958e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -4.9438e-23, -5.4529e-33,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -8.4772e-20,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -2.8727e-10,  0.0000e+00, -9.0216e-24, -1.2184e+02,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "        -4.2036e-30, -2.2960e-18,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -2.3133e-27,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -7.9832e-23,  0.0000e+00, -1.3400e-08, -2.4060e-40,\n",
              "        -9.5567e-34, -2.6038e-21,  0.0000e+00, -9.8748e-13,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -9.8091e-45,  0.0000e+00,\n",
              "         0.0000e+00, -1.3046e-34,  1.1169e-32,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -4.2383e-17, -2.2156e-12,  0.0000e+00,  0.0000e+00,\n",
              "        -3.7737e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.1789e+02,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -2.2141e-04, -7.1308e-38,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7511e-36,\n",
              "         0.0000e+00, -2.3189e-15,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9457e-04,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.5112e-30,  0.0000e+00, -1.4125e-01,\n",
              "        -3.2425e-18,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.0095e-10,  0.0000e+00, -7.4033e-39,\n",
              "        -1.7015e-18, -6.6945e-22,  0.0000e+00, -1.9035e-38, -4.2974e-25,\n",
              "        -1.1244e-36,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6816e-44,\n",
              "        -1.0052e-13,  0.0000e+00,  0.0000e+00, -2.5688e-29,  0.0000e+00,\n",
              "         0.0000e+00, -7.1308e-38, -1.3400e-08,  0.0000e+00, -8.7134e-11,\n",
              "         0.0000e+00, -1.7944e-15,  0.0000e+00, -2.5462e-42,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -1.2808e-33, -2.6687e-08,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -3.5322e-01, -2.0774e-08,\n",
              "        -1.4013e-45, -1.0095e-10,  0.0000e+00, -9.5567e-34,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -2.4202e-34, -3.3704e-28,  0.0000e+00,\n",
              "        -1.8556e-23, -3.0591e-21,  0.0000e+00,  0.0000e+00,  2.1787e-40,\n",
              "        -1.4635e-26,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4565e-24,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -4.2036e-30,  0.0000e+00,\n",
              "        -1.4309e-17,  0.0000e+00, -2.4565e-24,  0.0000e+00,  0.0000e+00,\n",
              "        -5.4262e-07,  0.0000e+00, -2.8903e-30,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4789e-27,\n",
              "        -2.4789e-27,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4645e-07,\n",
              "         0.0000e+00, -4.7570e-17,  0.0000e+00, -1.2418e-12,  0.0000e+00,\n",
              "        -2.6013e-38,  0.0000e+00, -4.5020e-30,  0.0000e+00,  0.0000e+00,\n",
              "        -1.1129e-17,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6108e-20,\n",
              "        -1.6266e-28,  0.0000e+00,  0.0000e+00, -9.0216e-24, -7.9832e-23,\n",
              "         0.0000e+00, -1.6816e-44,  0.0000e+00,  0.0000e+00, -3.5032e-43,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8558e-42, -4.0543e-28,\n",
              "        -1.6182e-33,  0.0000e+00, -1.3873e-42,  0.0000e+00, -3.0953e-12,\n",
              "         0.0000e+00,  0.0000e+00,  1.8187e-35, -1.3828e-31, -1.6692e-14])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clamp_large_terms(samples_IS)[0].min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg4Fy5iQcqnl",
        "outputId": "534b197d-a951-470c-d848-9b9d74a2121f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-9.0000e-38)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp_large_terms(tensor):\n",
        "    max_float64 = torch.finfo(torch.float64).max\n",
        "    min_float64 = torch.finfo(torch.float64).min\n",
        "\n",
        "    tensor[tensor == float('inf')] = 1e38  # Choose a large finite value\n",
        "    tensor[tensor == float('-inf')] = -1e38  # Choose a small finite value\n",
        "\n",
        "    return tensor\n",
        "\n",
        "# Example usage:\n",
        "# samples_IS_clamped = clamp_tensor(samples_IS)"
      ],
      "metadata": {
        "id": "5I1eVRAvlZK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# samples_IS_clamped[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "fMWanHX9l-kL",
        "outputId": "d8453853-77ac-49e1-fca3-d1760b015b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'samples_IS_clamped' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-5babce667c95>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples_IS_clamped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'samples_IS_clamped' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(torch.finfo(torch.float64).min, dtype=torch.float64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZvLUFyNeJBo",
        "outputId": "c35697a7-262f-427d-d331-dfc87a2c111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.7977e+308, dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MHWCgv3e3fSs",
        "outputId": "f1041452-2c44-40ad-fd87-8a903fee57eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet_d(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
              "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(state_tensors)"
      ],
      "metadata": {
        "id": "HkIlYqmg25KD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "651d0a8b-af25-4e2f-a721-0b694511e29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 must have the same dtype, but got Float and Double",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-602b7502521b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-0c890199681a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and Double"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "Tukv3iQZ3ekc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FltSKPtH8TAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 32], output_dim=1)"
      ],
      "metadata": {
        "id": "pG06s2t_0gZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PZWZk4GqndI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(pi_b, 0.3)"
      ],
      "metadata": {
        "id": "OFF9HKErgm34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS, state_tensors, w_diff, f, st_og, psi_res, sample_last_tensors = variance_terms_tens(P_pi_e, P_pi_b, phi_set)"
      ],
      "metadata": {
        "id": "inVdxVMRjT4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 32], output_dim=1)"
      ],
      "metadata": {
        "id": "qq1dLzLCjUp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = train_mse_var(model, 40, 0.001, 1, 1, IS, state_tensors, w_diff, f, sample_last_tensors, phi_set, st_og, psi_res)"
      ],
      "metadata": {
        "id": "TYKlf1urD34t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test class"
      ],
      "metadata": {
        "id": "72Fy4L1cuitK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from IS import calculate_importance_weights\n",
        "\n",
        "# import torch\n",
        "\n",
        "class SCOPE_variance(object):\n",
        "    def __init__(self, model, gamma, num_bootstraps, pi_b, P_pi_b, P_pi_e, dtype):\n",
        "        self.model = model\n",
        "        self.gamma = gamma\n",
        "        self.num_bootstraps = num_bootstraps\n",
        "        self.pi_b = pi_b\n",
        "        self.P_pi_b = P_pi_b\n",
        "        self.P_pi_e = P_pi_e\n",
        "        self.dtype = dtype\n",
        "\n",
        "\n",
        "    def prep_policies(self):\n",
        "        # Initialize lists to store axis data for each policy\n",
        "        timesteps = []\n",
        "        states = []\n",
        "        state_first = []\n",
        "        state_last = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        gamma_last = []\n",
        "        weight_last = []\n",
        "        weights = calculate_importance_weights(self.P_pi_e, self.P_pi_b, self.pi_b)\n",
        "        psi = []\n",
        "\n",
        "        for index, policy in enumerate(self.pi_b):\n",
        "            policy_array = np.array(policy)\n",
        "            timesteps.append(policy_array['timestep'].astype(int))\n",
        "            # s.append(policy_array[:, 0])\n",
        "\n",
        "            # last timestep for gamma\n",
        "            gamma_last.append(len(policy))\n",
        "            # last importance weight\n",
        "            weight_last.append(weights[index][-1])\n",
        "\n",
        "\n",
        "            states.append(policy_array['state'][1:])\n",
        "            psi.append(policy_array['psi'][1:])\n",
        "            state_first.append(policy_array['state'][0])\n",
        "            state_last.append(policy_array['state'][-1])\n",
        "            actions.append(policy_array['action'])\n",
        "            rewards.append(policy_array['reward'].astype(float))\n",
        "\n",
        "        weights_difference = []\n",
        "        for index, weight in enumerate(weights):\n",
        "            # diff = np.array(w[index][:-1]) - np.array(w[index][1:])\n",
        "            diff = np.array(weight[:-1]) - np.array(weight[1:])\n",
        "            weights_difference.append(diff)\n",
        "\n",
        "        return timesteps, states, state_first, state_last, actions, rewards, gamma_last, weight_last, weights, weights_difference\n",
        "\n",
        "    def padding_IS_terms(self,timesteps, actions, rewards, weights):\n",
        "        # Find the maximum length among all lists\n",
        "        max_length = max(len(traj) for traj in timesteps)\n",
        "\n",
        "        # Define the padding values\n",
        "        zero_padding = 0\n",
        "\n",
        "        # Pad each list to match the maximum length\n",
        "        padded_timesteps = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in timesteps]\n",
        "        padded_rewards = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in rewards]\n",
        "        padded_actions = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in actions]\n",
        "        padded_weights = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in weights]\n",
        "\n",
        "        return padded_timesteps, padded_rewards, padded_actions, padded_weights\n",
        "\n",
        "    def padding_states_weights_difference(self, states, weights_difference):\n",
        "        # Find the maximum length of trajectories\n",
        "        max_length = max(len(trajectory) for trajectory in states)\n",
        "\n",
        "        zero_padding = 0\n",
        "\n",
        "        # Pad each trajectory to make them all the same length\n",
        "        padded_states = [\n",
        "            [list(item) for item in trajectory] + [[0, 0]] * (max_length - len(trajectory))\n",
        "            for trajectory in states\n",
        "        ]\n",
        "\n",
        "        padded_weights_difference = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in weights_difference]\n",
        "\n",
        "        return padded_states, padded_weights_difference\n",
        "\n",
        "    def tensorize_padded_terms(self, padded_states, padded_weights_difference):\n",
        "        padded_state_tensors = torch.tensor(padded_states, dtype = self.dtype)\n",
        "        padded_weight_diff_tensors = torch.tensor(padded_weights_difference, dtype = self.dtype)\n",
        "        padded_weight_diff_tensors = padded_weight_diff_tensors.unsqueeze(-1)\n",
        "\n",
        "        return padded_state_tensors, padded_weight_diff_tensors\n",
        "\n",
        "    def tensorize_last_and_first_terms(self, states_first, states_last, gamma_last, weights_last):\n",
        "        states_first_tensor = torch.tensor(states_first, dtype = self.dtype)\n",
        "        states_last_tensor = torch.tensor(states_last, dtype = self.dtype)\n",
        "        gamma_last_tensor = torch.tensor(gamma_last, dtype = self.dtype)\n",
        "        weights_last_tensor = torch.tensor(weights_last, dtype = self.dtype)\n",
        "\n",
        "        return states_first_tensor, states_last_tensor, gamma_last_tensor, weights_last_tensor\n",
        "\n",
        "    def calc_IS_terms(self, gamma, timesteps, rewards, weights):\n",
        "        gtrw = np.power(gamma, timesteps)*rewards*weights\n",
        "\n",
        "        IS_tensor = torch.sum(torch.tensor(gtrw, dtype = self.dtype), dim = 1, keepdim = True)\n",
        "\n",
        "        return IS_tensor\n",
        "\n",
        "    def calc_gamma_weight_last(self, gamma, gamma_last, weights_last):\n",
        "        gamma_weight_last = np.power(gamma, gamma_last)*weights_last\n",
        "\n",
        "        gamma_weight_last_tensor = torch.tensor(gamma_weight_last, dtype = self.dtype).unsqueeze(-1)\n",
        "\n",
        "        return gamma_weight_last_tensor\n",
        "    def bootstrap_IS_terms(self, IS_tensor, num_samples):\n",
        "        seed = 42\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        num_bootstraps = num_samples*len(IS_tensor)\n",
        "\n",
        "        # Sample indices with replacement\n",
        "        sampled_indices = torch.randint(0, len(IS_tensor), size=(num_bootstraps,), dtype=torch.long)\n",
        "\n",
        "        # new_size = (num_samples, IS_tensor.shape[0], IS_tensor.shape[1])\n",
        "        new_size = (num_samples, IS_tensor.shape[0])\n",
        "\n",
        "        IS_bootstraps = IS_tensor[sampled_indices].view(new_size)\n",
        "\n",
        "        # sampled_tensor = IS_bootstraps.view(new_size)\n",
        "\n",
        "        return IS_bootstraps\n",
        "\n",
        "    def states_weight_diff_sums(self, states_output, padded_weight_diff_tensors):\n",
        "        states_weight_diff = states_output * padded_weight_diff_tensors\n",
        "        sums_states_weight_diff = torch.sum(states_weight_diff, dim =1)\n",
        "\n",
        "        return sums_states_weight_diff\n",
        "\n",
        "    def last_first_terms_operations(self, gamma_weights_last_tensor, states_last_output, states_first_output):\n",
        "        gamma_weights_states_last_sub_states_first = gamma_weights_last_tensor*states_last_output -  states_first_output\n",
        "\n",
        "        return gamma_weights_states_last_sub_states_first\n",
        "\n",
        "    def bootstrap_shaping_terms(self, sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor):\n",
        "\n",
        "        seed = 42\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        num_samples = self.num_bootstraps\n",
        "\n",
        "        num_bootstraps_lin = num_samples*sums_states_weight_diff.shape[0]\n",
        "\n",
        "        # Sample indices with replacement\n",
        "        sampled_indices = torch.randint(0, len(sums_states_weight_diff), size=(num_bootstraps_lin,), dtype=torch.long)\n",
        "\n",
        "        reshaped_size = (num_samples, sums_states_weight_diff.shape[0])\n",
        "\n",
        "        # Resize samples to shape num_samples x num_trajectories\n",
        "        sample_sums_states_weight_diff = sums_states_weight_diff[sampled_indices].view(reshaped_size)\n",
        "        samples_gamma_weight_states_last_sub_states_first = gamma_weights_states_last_sub_states_first[sampled_indices].view(reshaped_size)\n",
        "\n",
        "        # Sum states_weight_diff and gamma_weights-states_last_sub_states_first\n",
        "        sum_terms = sums_states_weight_diff + gamma_weights_states_last_sub_states_first\n",
        "\n",
        "        # sample IS terms\n",
        "\n",
        "        IS_SCOPE = IS_tensor * sum_terms\n",
        "\n",
        "        samples_IS_SCOPE = IS_SCOPE[sampled_indices].view(reshaped_size)\n",
        "\n",
        "\n",
        "        sample_all_shaping = sum_terms[sampled_indices].view(reshaped_size)\n",
        "\n",
        "        return sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, sample_all_shaping, samples_IS_SCOPE\n",
        "\n",
        "    def bootstrap_all_terms(self, sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor):\n",
        "        seed = 42\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # num_bootstraps = num_samples*len(IS_tensor)\n",
        "\n",
        "        # Sample indices with replacement\n",
        "        # sampled_indices = torch.randint(0, len(IS_tensor), size=(num_bootstraps,), dtype=torch.long)\n",
        "\n",
        "        # new_size = (num_samples, IS_tensor.shape[0], IS_tensor.shape[1])\n",
        "        # new_size = (num_samples, IS_tensor.shape[0])\n",
        "\n",
        "        # IS_bootstraps = IS_tensor[sampled_indices].view(new_size)\n",
        "\n",
        "        num_samples = self.num_bootstraps\n",
        "        num_bootstraps_lin = num_samples*sums_states_weight_diff.shape[0]\n",
        "\n",
        "\n",
        "        # Sample indices with replacement\n",
        "        sampled_indices = torch.randint(0, len(sums_states_weight_diff), size=(num_bootstraps_lin,), dtype=torch.long)\n",
        "\n",
        "        reshaped_size = (num_samples, sums_states_weight_diff.shape[0])\n",
        "\n",
        "        IS_bootstraps = IS_tensor[sampled_indices].view(reshaped_size)\n",
        "\n",
        "        # Resize samples to shape num_samples x num_trajectories\n",
        "        sample_sums_states_weight_diff = sums_states_weight_diff[sampled_indices].view(reshaped_size)\n",
        "        samples_gamma_weight_states_last_sub_states_first = gamma_weights_states_last_sub_states_first[sampled_indices].view(reshaped_size)\n",
        "\n",
        "        # Sum states_weight_diff and gamma_weights-states_last_sub_states_first\n",
        "        sum_terms = sums_states_weight_diff + gamma_weights_states_last_sub_states_first\n",
        "\n",
        "        # sample IS terms\n",
        "\n",
        "        IS_SCOPE = IS_tensor * sum_terms\n",
        "\n",
        "        samples_IS_SCOPE = IS_SCOPE[sampled_indices].view(reshaped_size)\n",
        "\n",
        "        sample_all_shaping = sum_terms[sampled_indices].view(reshaped_size)\n",
        "\n",
        "        return IS_bootstraps, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, sample_all_shaping, samples_IS_SCOPE\n",
        "\n",
        "\n",
        "\n",
        "    def pass_states(self,model, padded_state_tensors, states_first_tensor, states_last_tensor):\n",
        "        # Get model outputs for states\n",
        "        states_output = model(padded_state_tensors)\n",
        "        states_first_output = model(states_first_tensor)\n",
        "        states_last_output = model(states_last_tensor)\n",
        "        return states_output, states_first_output, states_last_output\n",
        "\n",
        "\n",
        "    # def prepare(self):\n",
        "    #     timesteps, states, states_first, states_last, actions, rewards, gamma_last, weights_last, weights, weights_difference = self.prep_policies()\n",
        "    #     padded_timesteps, padded_rewards, padded_actions, padded_weights = self.padding_IS_terms(timesteps, actions, rewards, weights)\n",
        "    #     padded_states, padded_weights_difference = self.padding_states_weights_difference(states, weights_difference)\n",
        "    #     padded_state_tensors, padded_weight_diff_tensors = self.tensorize_padded_terms(padded_states, padded_weights_difference)\n",
        "    #     states_first_tensor, states_last_tensor, gamma_last_tensor, weights_last_tensor = self.tensorize_last_and_first_terms(states_first, states_last, gamma_last, weights_last)\n",
        "    #     IS_tensor = self.calc_IS_terms(self.gamma, padded_timesteps, padded_rewards, padded_weights)\n",
        "    #     gamma_weights_last_tensor = self.calc_gamma_weight_last(self.gamma, gamma_last, weights_last)\n",
        "    #     samples_IS = self.bootstrap_IS_terms(IS_tensor, self.num_bootstraps)\n",
        "\n",
        "    #     return IS_tensor, samples_IS, padded_state_tensors, padded_weight_diff_tensors, gamma_weights_last_tensor, states_first_tensor, states_last_tensor\n",
        "\n",
        "\n",
        "    def prepare(self):\n",
        "        timesteps, states, states_first, states_last, actions, rewards, gamma_last, weights_last, weights, weights_difference = self.prep_policies()\n",
        "        padded_timesteps, padded_rewards, padded_actions, padded_weights = self.padding_IS_terms(timesteps, actions, rewards, weights)\n",
        "        padded_states, padded_weights_difference = self.padding_states_weights_difference(states, weights_difference)\n",
        "        padded_state_tensors, padded_weight_diff_tensors = self.tensorize_padded_terms(padded_states, padded_weights_difference)\n",
        "        states_first_tensor, states_last_tensor, gamma_last_tensor, weights_last_tensor = self.tensorize_last_and_first_terms(states_first, states_last, gamma_last, weights_last)\n",
        "        IS_tensor = self.calc_IS_terms(self.gamma, padded_timesteps, padded_rewards, padded_weights)\n",
        "        gamma_weights_last_tensor = self.calc_gamma_weight_last(self.gamma, gamma_last, weights_last)\n",
        "        # samples_IS = self.bootstrap_IS_terms(IS_tensor, self.num_bootstraps)\n",
        "\n",
        "        return IS_tensor, padded_state_tensors, padded_weight_diff_tensors, gamma_weights_last_tensor, states_first_tensor, states_last_tensor"
      ],
      "metadata": {
        "id": "1vZCWSiPum0K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9iS0QWve0s3",
        "outputId": "bd38c848-9a05-4e12-8396-3199b72bf38b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([([2., 9.], 3, 0., [1., 9.],  0, 3.),\n",
              "        ([1., 9.], 3, 0., [0., 9.],  1, 4.),\n",
              "        ([0., 9.], 1, 0., [0., 8.],  2, 5.),\n",
              "        ([0., 8.], 1, 0., [0., 7.],  3, 5.),\n",
              "        ([0., 7.], 1, 0., [0., 6.],  4, 5.),\n",
              "        ([0., 6.], 1, 0., [0., 5.],  5, 5.),\n",
              "        ([0., 5.], 1, 0., [0., 4.],  6, 5.),\n",
              "        ([0., 4.], 4, 0., [1., 4.],  7, 6.),\n",
              "        ([1., 4.], 1, 0., [1., 3.],  8, 5.),\n",
              "        ([1., 3.], 4, 0., [2., 3.],  9, 6.),\n",
              "        ([2., 3.], 4, 0., [3., 3.], 10, 5.),\n",
              "        ([3., 3.], 4, 0., [4., 3.], 11, 4.),\n",
              "        ([4., 3.], 1, 0., [4., 2.], 12, 3.),\n",
              "        ([4., 2.], 1, 0., [5., 2.], 13, 4.),\n",
              "        ([5., 2.], 4, 0., [6., 2.], 14, 3.),\n",
              "        ([6., 2.], 1, 0., [6., 1.], 15, 3.),\n",
              "        ([6., 1.], 1, 0., [7., 1.], 16, 4.),\n",
              "        ([7., 1.], 4, 0., [8., 1.], 17, 4.),\n",
              "        ([8., 1.], 0, 0., [8., 1.], 18, 4.),\n",
              "        ([8., 1.], 3, 0., [7., 1.], 19, 4.),\n",
              "        ([7., 1.], 4, 0., [8., 1.], 20, 4.),\n",
              "        ([8., 1.], 3, 0., [7., 1.], 21, 4.),\n",
              "        ([7., 1.], 1, 1., [7., 0.], 22, 4.)],\n",
              "       dtype=[('state', '<f8', (2,)), ('action', '<i8'), ('reward', '<f8'), ('state_next', '<f8', (2,)), ('timestep', '<i8'), ('psi', '<f8')]),\n",
              " array([([2., 9.], 1, 0., [2., 8.],  0, 3.),\n",
              "        ([2., 8.], 3, 0., [1., 8.],  1, 3.),\n",
              "        ([1., 8.], 1, 0., [1., 7.],  2, 4.),\n",
              "        ([1., 7.], 3, 0., [0., 7.],  3, 4.),\n",
              "        ([0., 7.], 1, 0., [0., 6.],  4, 5.),\n",
              "        ([0., 6.], 3, 0., [0., 6.],  5, 5.),\n",
              "        ([0., 6.], 1, 0., [0., 5.],  6, 5.),\n",
              "        ([0., 5.], 1, 0., [0., 4.],  7, 5.),\n",
              "        ([0., 4.], 4, 0., [1., 4.],  8, 6.),\n",
              "        ([1., 4.], 1, 0., [1., 3.],  9, 5.),\n",
              "        ([1., 3.], 2, 0., [1., 4.], 10, 6.),\n",
              "        ([1., 4.], 1, 0., [2., 4.], 11, 5.),\n",
              "        ([2., 4.], 4, 0., [3., 4.], 12, 4.),\n",
              "        ([3., 4.], 1, 0., [3., 3.], 13, 3.),\n",
              "        ([3., 3.], 1, 0., [3., 2.], 14, 4.),\n",
              "        ([3., 2.], 0, 0., [3., 2.], 15, 5.),\n",
              "        ([3., 2.], 1, 0., [3., 1.], 16, 5.),\n",
              "        ([3., 1.], 4, 0., [4., 1.], 17, 6.),\n",
              "        ([4., 1.], 0, 0., [4., 1.], 18, 5.),\n",
              "        ([4., 1.], 4, 0., [5., 1.], 19, 5.),\n",
              "        ([5., 1.], 1, 1., [5., 0.], 20, 4.)],\n",
              "       dtype=[('state', '<f8', (2,)), ('action', '<i8'), ('reward', '<f8'), ('state_next', '<f8', (2,)), ('timestep', '<i8'), ('psi', '<f8')]),\n",
              " array([([2., 9.], 3, 0., [1., 9.],  0, 3.),\n",
              "        ([1., 9.], 0, 0., [1., 9.],  1, 4.),\n",
              "        ([1., 9.], 3, 0., [0., 9.],  2, 4.),\n",
              "        ([0., 9.], 1, 0., [0., 8.],  3, 5.),\n",
              "        ([0., 8.], 3, 0., [0., 8.],  4, 5.),\n",
              "        ([0., 8.], 1, 0., [0., 7.],  5, 5.),\n",
              "        ([0., 7.], 1, 0., [0., 6.],  6, 5.),\n",
              "        ([0., 6.], 1, 0., [0., 5.],  7, 5.),\n",
              "        ([0., 5.], 1, 0., [0., 4.],  8, 5.),\n",
              "        ([0., 4.], 4, 0., [1., 4.],  9, 6.),\n",
              "        ([1., 4.], 1, 0., [1., 3.], 10, 5.),\n",
              "        ([1., 3.], 4, 0., [2., 3.], 11, 6.),\n",
              "        ([2., 3.], 3, 0., [1., 3.], 12, 5.),\n",
              "        ([1., 3.], 3, 0., [2., 3.], 13, 6.),\n",
              "        ([2., 3.], 1, 0., [2., 2.], 14, 5.),\n",
              "        ([2., 2.], 0, 0., [2., 2.], 15, 6.),\n",
              "        ([2., 2.], 4, 0., [3., 2.], 16, 6.),\n",
              "        ([3., 2.], 2, 0., [3., 3.], 17, 5.),\n",
              "        ([3., 3.], 1, 0., [3., 2.], 18, 4.),\n",
              "        ([3., 2.], 3, 0., [2., 2.], 19, 5.),\n",
              "        ([2., 2.], 1, 0., [2., 1.], 20, 6.),\n",
              "        ([2., 1.], 2, 0., [2., 2.], 21, 7.),\n",
              "        ([2., 2.], 4, 0., [3., 2.], 22, 6.),\n",
              "        ([3., 2.], 1, 0., [3., 1.], 23, 5.),\n",
              "        ([3., 1.], 4, 0., [4., 1.], 24, 6.),\n",
              "        ([4., 1.], 4, 0., [5., 1.], 25, 5.),\n",
              "        ([5., 1.], 0, 0., [5., 1.], 26, 4.),\n",
              "        ([5., 1.], 1, 0., [6., 1.], 27, 4.),\n",
              "        ([6., 1.], 1, 1., [6., 0.], 28, 4.)],\n",
              "       dtype=[('state', '<f8', (2,)), ('action', '<i8'), ('reward', '<f8'), ('state_next', '<f8', (2,)), ('timestep', '<i8'), ('psi', '<f8')]),\n",
              " array([([2., 9.], 2, 0., [2., 9.],  0, 3.),\n",
              "        ([2., 9.], 3, 0., [1., 9.],  1, 3.),\n",
              "        ([1., 9.], 3, 0., [0., 9.],  2, 4.),\n",
              "        ([0., 9.], 1, 0., [1., 9.],  3, 5.),\n",
              "        ([1., 9.], 3, 0., [2., 9.],  4, 4.),\n",
              "        ([2., 9.], 3, 0., [1., 9.],  5, 3.),\n",
              "        ([1., 9.], 4, 0., [2., 9.],  6, 4.),\n",
              "        ([2., 9.], 2, 0., [2., 9.],  7, 3.),\n",
              "        ([2., 9.], 3, 0., [1., 9.],  8, 3.),\n",
              "        ([1., 9.], 3, 0., [0., 9.],  9, 4.),\n",
              "        ([0., 9.], 4, 0., [1., 9.], 10, 5.),\n",
              "        ([1., 9.], 3, 0., [0., 9.], 11, 4.),\n",
              "        ([0., 9.], 0, 0., [0., 9.], 12, 5.),\n",
              "        ([0., 9.], 1, 0., [0., 8.], 13, 5.),\n",
              "        ([0., 8.], 1, 0., [0., 7.], 14, 5.),\n",
              "        ([0., 7.], 1, 0., [0., 6.], 15, 5.),\n",
              "        ([0., 6.], 3, 0., [1., 6.], 16, 5.),\n",
              "        ([1., 6.], 1, 0., [1., 6.], 17, 4.),\n",
              "        ([1., 6.], 3, 0., [0., 6.], 18, 4.),\n",
              "        ([0., 6.], 1, 0., [0., 5.], 19, 5.),\n",
              "        ([0., 5.], 1, 0., [0., 4.], 20, 5.),\n",
              "        ([0., 4.], 1, 0., [0., 3.], 21, 6.),\n",
              "        ([0., 3.], 2, 0., [0., 4.], 22, 7.),\n",
              "        ([0., 4.], 4, 0., [1., 4.], 23, 6.),\n",
              "        ([1., 4.], 1, 0., [1., 3.], 24, 5.),\n",
              "        ([1., 3.], 4, 0., [2., 3.], 25, 6.),\n",
              "        ([2., 3.], 1, 0., [2., 2.], 26, 5.),\n",
              "        ([2., 2.], 4, 0., [3., 2.], 27, 6.),\n",
              "        ([3., 2.], 1, 0., [4., 2.], 28, 5.),\n",
              "        ([4., 2.], 1, 0., [4., 1.], 29, 4.),\n",
              "        ([4., 1.], 4, 0., [5., 1.], 30, 5.),\n",
              "        ([5., 1.], 1, 1., [5., 0.], 31, 4.)],\n",
              "       dtype=[('state', '<f8', (2,)), ('action', '<i8'), ('reward', '<f8'), ('state_next', '<f8', (2,)), ('timestep', '<i8'), ('psi', '<f8')]),\n",
              " array([([2., 9.], 3, 0., [1., 9.],  0, 3.),\n",
              "        ([1., 9.], 1, 0., [1., 8.],  1, 4.),\n",
              "        ([1., 8.], 1, 0., [1., 7.],  2, 4.),\n",
              "        ([1., 7.], 1, 0., [1., 6.],  3, 4.),\n",
              "        ([1., 6.], 3, 0., [2., 6.],  4, 4.),\n",
              "        ([2., 6.], 3, 0., [1., 6.],  5, 3.),\n",
              "        ([1., 6.], 3, 0., [0., 6.],  6, 4.),\n",
              "        ([0., 6.], 1, 0., [0., 5.],  7, 5.),\n",
              "        ([0., 5.], 4, 0., [0., 5.],  8, 5.),\n",
              "        ([0., 5.], 1, 0., [0., 4.],  9, 5.),\n",
              "        ([0., 4.], 3, 0., [0., 4.], 10, 6.),\n",
              "        ([0., 4.], 4, 0., [1., 4.], 11, 6.),\n",
              "        ([1., 4.], 1, 0., [1., 3.], 12, 5.),\n",
              "        ([1., 3.], 4, 0., [2., 3.], 13, 6.),\n",
              "        ([2., 3.], 1, 0., [2., 2.], 14, 5.),\n",
              "        ([2., 2.], 4, 0., [3., 2.], 15, 6.),\n",
              "        ([3., 2.], 1, 0., [3., 1.], 16, 5.),\n",
              "        ([3., 1.], 4, 0., [4., 1.], 17, 6.),\n",
              "        ([4., 1.], 0, 0., [4., 1.], 18, 5.),\n",
              "        ([4., 1.], 4, 0., [5., 1.], 19, 5.),\n",
              "        ([5., 1.], 2, 0., [5., 2.], 20, 4.),\n",
              "        ([5., 2.], 1, 0., [5., 1.], 21, 3.),\n",
              "        ([5., 1.], 1, 1., [5., 0.], 22, 4.)],\n",
              "       dtype=[('state', '<f8', (2,)), ('action', '<i8'), ('reward', '<f8'), ('state_next', '<f8', (2,)), ('timestep', '<i8'), ('psi', '<f8')])]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_b[states[0],actions[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "995EfmiwgXi3",
        "outputId": "7bbf8238-3336-48bf-a69a-9eb776a8f1f6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 1, 1, 1, 1, 1, 4, 1, 4, 4, 4, 1, 1, 4, 1, 1, 4, 0, 3, 4, 3,\n",
              "       1])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class play"
      ],
      "metadata": {
        "id": "GKFk2XRHQeQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(5, env, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "pi_e = experiment_actions(5, env, P_pi_e)"
      ],
      "metadata": {
        "id": "TnCDINz7AoG1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 32], output_dim=1, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "gEgO1QxNLnCZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing = SCOPE_variance(model, 0.9, 10000, pi_b, P_pi_b, P_pi_e, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "I2S6fvyWQgjM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps, states, states_first, states_last, actions, rewards, gamma_last, weights_last, weights, weights_difference = testing.prep_policies()\n"
      ],
      "metadata": {
        "id": "VyU2oHd6Qyn2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b[tuple(np.append(pi_b[0]['state'][0].astype(int) , (pi_b[0]['action'][0],)))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WwYv6SGDudT",
        "outputId": "d86a8929-ad81-4903-f252-1e2a326e8183"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6799999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_e[tuple(np.append(pi_b[0]['state'][0].astype(int) , (pi_b[0]['action'][0],)))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHr8zugWSvWr",
        "outputId": "f9bb1fcf-0a37-4d84-913e-477d7e85b10c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.485"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_b[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LswZV_GfUHE0",
        "outputId": "8bb94af3-ee9c-4e71-e8e7-dd4c3fba153e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([([2., 9.], 3, 0., [1., 9.],  0, 3.),\n",
              "       ([1., 9.], 3, 0., [0., 9.],  1, 4.),\n",
              "       ([0., 9.], 1, 0., [0., 8.],  2, 5.),\n",
              "       ([0., 8.], 1, 0., [0., 7.],  3, 5.),\n",
              "       ([0., 7.], 1, 0., [0., 6.],  4, 5.),\n",
              "       ([0., 6.], 1, 0., [0., 5.],  5, 5.),\n",
              "       ([0., 5.], 1, 0., [0., 4.],  6, 5.),\n",
              "       ([0., 4.], 4, 0., [1., 4.],  7, 6.),\n",
              "       ([1., 4.], 1, 0., [1., 3.],  8, 5.),\n",
              "       ([1., 3.], 4, 0., [2., 3.],  9, 6.),\n",
              "       ([2., 3.], 4, 0., [3., 3.], 10, 5.),\n",
              "       ([3., 3.], 4, 0., [4., 3.], 11, 4.),\n",
              "       ([4., 3.], 1, 0., [4., 2.], 12, 3.),\n",
              "       ([4., 2.], 1, 0., [5., 2.], 13, 4.),\n",
              "       ([5., 2.], 4, 0., [6., 2.], 14, 3.),\n",
              "       ([6., 2.], 1, 0., [6., 1.], 15, 3.),\n",
              "       ([6., 1.], 1, 0., [7., 1.], 16, 4.),\n",
              "       ([7., 1.], 4, 0., [8., 1.], 17, 4.),\n",
              "       ([8., 1.], 0, 0., [8., 1.], 18, 4.),\n",
              "       ([8., 1.], 3, 0., [7., 1.], 19, 4.),\n",
              "       ([7., 1.], 4, 0., [8., 1.], 20, 4.),\n",
              "       ([8., 1.], 3, 0., [7., 1.], 21, 4.),\n",
              "       ([7., 1.], 1, 1., [7., 0.], 22, 4.)],\n",
              "      dtype=[('state', '<f8', (2,)), ('action', '<i8'), ('reward', '<f8'), ('state_next', '<f8', (2,)), ('timestep', '<i8'), ('psi', '<f8')])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7QFvGFYTxcf",
        "outputId": "93d2faad-8abf-454d-e773-8311e0598aef"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7132352941176471,\n",
              " 0.36282606414105434,\n",
              " 0.13164275282008847,\n",
              " 0.034066558251510434,\n",
              " 0.006287708340180658,\n",
              " 0.0008277312348312113,\n",
              " 7.771755426316267e-05,\n",
              " 5.204532588676245e-06,\n",
              " 2.48586285712858e-07,\n",
              " 8.468479182763697e-09,\n",
              " 3.6061492422665387e-11,\n",
              " 1.9195170816455204e-14,\n",
              " 7.287408118376271e-18,\n",
              " 1.973272342660357e-21,\n",
              " 6.678992851535058e-26,\n",
              " 1.612381306110978e-30,\n",
              " 2.776242228765327e-35,\n",
              " 5.9752622437758e-41,\n",
              " 7.796655348684653e-46,\n",
              " 7.255920717711713e-51,\n",
              " 8.440860713209574e-57,\n",
              " 7.003477936596074e-63,\n",
              " 4.1445141020010597e-69]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_difference[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM1-XMt-BziS",
        "outputId": "3647c2c1-f2f8-40ed-9b8a-468d73f4440f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.01516831e+01, -5.46309224e+01, -9.69844577e+01, -1.01159396e+02,\n",
              "       -2.37452773e+03, -1.53471859e+04, -6.90936339e+04, -2.13460139e+05,\n",
              "       -4.39202910e+05,  5.12160962e+05,  1.77653452e+05,  4.85730579e+04,\n",
              "        1.34311349e+03,  2.64266741e+01,  3.42329083e-01,  2.97261847e-02,\n",
              "        1.82655288e-03,  6.07082471e-05,  1.74411777e-05,  3.45811643e-06])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_timesteps, padded_rewards, padded_actions, padded_weights = testing.padding_IS_terms(timesteps, actions, rewards, weights)"
      ],
      "metadata": {
        "id": "dmpbvgzDRSlz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_states, padded_weights_difference = testing.padding_states_weights_difference(states, weights_difference)\n",
        "padded_state_tensors, padded_weight_diff_tensors = testing.tensorize_padded_terms(padded_states, padded_weights_difference)\n",
        "states_first_tensor, states_last_tensor, gamma_last_tensor, weights_last_tensor = testing.tensorize_last_and_first_terms(states_first, states_last, gamma_last, weights_last)\n",
        "IS_tensor = testing.calc_IS_terms(testing.gamma, padded_timesteps, padded_rewards, padded_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x68IJ4fcRYjU",
        "outputId": "008b5a61-e703-4882-8396-addc0248800e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-80280909a6e1>:89: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  padded_weight_diff_tensors = torch.tensor(padded_weights_difference, dtype = self.dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_weights_difference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aVBk7IoLPF8v",
        "outputId": "7b63dbfe-0314-4f0b-f0e4-591483cb6dd8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([3.50409230e-01, 2.31183311e-01, 9.75761946e-02, 2.77788499e-02,\n",
              "        5.45997711e-03, 7.50013681e-04, 7.25130217e-05, 4.95594630e-06,\n",
              "        2.40117807e-07, 8.43241769e-09, 3.60422973e-11, 1.91878834e-14,\n",
              "        7.28543485e-18, 1.97320555e-21, 6.67883161e-26, 1.61235354e-30,\n",
              "        2.77623625e-35, 5.97518428e-41, 7.79658279e-46, 7.25591228e-51,\n",
              "        8.44085371e-57, 7.00347379e-63, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
              " array([-2.01516831e+01, -5.46309224e+01, -9.69844577e+01, -1.01159396e+02,\n",
              "        -2.37452773e+03, -1.53471859e+04, -6.90936339e+04, -2.13460139e+05,\n",
              "        -4.39202910e+05,  5.12160962e+05,  1.77653452e+05,  4.85730579e+04,\n",
              "         1.34311349e+03,  2.64266741e+01,  3.42329083e-01,  2.97261847e-02,\n",
              "         1.82655288e-03,  6.07082471e-05,  1.74411777e-05,  3.45811643e-06,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
              " array([6.49647221e-001, 5.95446301e-002, 3.86005972e-003, 1.82343687e-004,\n",
              "        1.03542517e-006, 4.19155695e-009, 1.20981672e-011, 2.48996466e-014,\n",
              "        3.65448286e-017, 3.82506654e-020, 2.85527181e-023, 1.52054080e-026,\n",
              "        1.01176968e-030, 8.41493969e-036, 4.99176807e-041, 3.70139677e-047,\n",
              "        1.95753627e-053, 1.29408801e-060, 6.10169714e-068, 3.59623002e-076,\n",
              "        1.28497905e-083, 3.27474475e-091, 5.95239424e-099, 7.71682669e-107,\n",
              "        7.13540503e-115, 4.70577681e-123, 1.88146347e-130, 5.36528850e-138,\n",
              "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000]),\n",
              " array([1.13855699e-01, 1.04356568e-02, 6.76505313e-04, 3.10997151e-05,\n",
              "        1.01564315e-06, 2.39165308e-08, 6.91480655e-11, 2.49298183e-14,\n",
              "        6.41029676e-18, 1.17573014e-21, 2.69513385e-26, 4.40602593e-31,\n",
              "        4.36730784e-35, 3.08752048e-39, 1.55681207e-43, 5.59770490e-48,\n",
              "        1.21908761e-51, 1.61196687e-54, 1.52006735e-57, 1.02227717e-60,\n",
              "        4.89065859e-64, 1.42310506e-66, 2.95280742e-69, 4.36909841e-72,\n",
              "        4.61029186e-75, 3.46944580e-78, 1.86207736e-81, 7.12769288e-85,\n",
              "        1.94589493e-88, 3.78889758e-92, 5.26177432e-96]),\n",
              " array([-2.37078625e+00, -6.42716735e+00, -1.68318374e+02, -2.19357659e+03,\n",
              "        -2.01834857e+04, -1.30451080e+05, -5.87295888e+05,  2.92568647e+05,\n",
              "         2.54597491e+05,  1.82721763e+05,  1.00134608e+04,  3.89514042e+02,\n",
              "         1.07706121e+01,  2.11919140e-01,  2.96902327e-03,  2.96333769e-05,\n",
              "         2.10775843e-07,  1.03960823e-09,  3.21981020e-11,  7.18655419e-13,\n",
              "         1.96806745e-15,  3.84320860e-18,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00])]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states_output, states_first_output, states_last_output = testing.pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)\n",
        "sums_states_weight_diff = testing.states_weight_diff_sums(states_output, padded_weight_diff_tensors)\n",
        "\n",
        "gamma_weights_last_tensor = testing.calc_gamma_weight_last(testing.gamma, gamma_last, weights_last)\n",
        "\n",
        "gamma_weights_states_last_sub_states_first = testing.last_first_terms_operations(gamma_weights_last_tensor, states_last_output, states_first_output)\n"
      ],
      "metadata": {
        "id": "JTJBak1Gco7n"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WdLClfDFRICK",
        "outputId": "791465e6-3174-4ea9-8a89-5d10cde701e9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0020],\n",
              "        [ 0.8911],\n",
              "        [ 1.3753],\n",
              "        [ 1.3137],\n",
              "        [ 0.4047],\n",
              "        [ 0.7766],\n",
              "        [ 0.2557],\n",
              "        [ 0.8898],\n",
              "        [ 0.6326],\n",
              "        [ 0.1699],\n",
              "        [ 0.5175],\n",
              "        [ 0.1025],\n",
              "        [ 0.7043],\n",
              "        [ 0.1501],\n",
              "        [ 0.3807],\n",
              "        [ 1.2766],\n",
              "        [ 0.2123],\n",
              "        [ 0.3554],\n",
              "        [ 0.8854],\n",
              "        [ 1.1399],\n",
              "        [-0.5327],\n",
              "        [ 1.4007],\n",
              "        [ 0.0669],\n",
              "        [ 0.1753],\n",
              "        [ 0.0639],\n",
              "        [-0.0026],\n",
              "        [-0.0043],\n",
              "        [ 0.1673],\n",
              "        [ 0.0854],\n",
              "        [ 0.0112],\n",
              "        [-0.0640]], dtype=torch.float64, grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "swd = states_output*padded_weight_diff_tensors"
      ],
      "metadata": {
        "id": "LvW3ojK0Ruil"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swd[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WvF8FWxqSjY7",
        "outputId": "2934bd06-d5d9-4e56-d3b0-dab707b578c5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.5109e-01],\n",
              "        [ 2.0600e-01],\n",
              "        [ 1.3420e-01],\n",
              "        [ 3.6493e-02],\n",
              "        [ 2.2097e-03],\n",
              "        [ 5.8246e-04],\n",
              "        [ 1.8540e-05],\n",
              "        [ 4.4097e-06],\n",
              "        [ 1.5191e-07],\n",
              "        [ 1.4328e-09],\n",
              "        [ 1.8653e-11],\n",
              "        [ 1.9673e-15],\n",
              "        [ 5.1309e-18],\n",
              "        [ 2.9608e-22],\n",
              "        [ 2.5429e-26],\n",
              "        [ 2.0583e-30],\n",
              "        [ 5.8936e-36],\n",
              "        [ 2.1236e-41],\n",
              "        [ 6.9028e-46],\n",
              "        [ 8.2713e-51],\n",
              "        [-4.4964e-57],\n",
              "        [ 9.8098e-63],\n",
              "        [ 0.0000e+00],\n",
              "        [ 0.0000e+00],\n",
              "        [ 0.0000e+00],\n",
              "        [-0.0000e+00],\n",
              "        [-0.0000e+00],\n",
              "        [ 0.0000e+00],\n",
              "        [ 0.0000e+00],\n",
              "        [ 0.0000e+00],\n",
              "        [-0.0000e+00]], dtype=torch.float64, grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_weight_diff_tensors[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7xhuDISBRUde",
        "outputId": "66e77ad4-bc66-43d4-ae4b-9d70f5b307cf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.5041e-01],\n",
              "        [2.3118e-01],\n",
              "        [9.7576e-02],\n",
              "        [2.7779e-02],\n",
              "        [5.4600e-03],\n",
              "        [7.5001e-04],\n",
              "        [7.2513e-05],\n",
              "        [4.9559e-06],\n",
              "        [2.4012e-07],\n",
              "        [8.4324e-09],\n",
              "        [3.6042e-11],\n",
              "        [1.9188e-14],\n",
              "        [7.2854e-18],\n",
              "        [1.9732e-21],\n",
              "        [6.6788e-26],\n",
              "        [1.6124e-30],\n",
              "        [2.7762e-35],\n",
              "        [5.9752e-41],\n",
              "        [7.7966e-46],\n",
              "        [7.2559e-51],\n",
              "        [8.4409e-57],\n",
              "        [7.0035e-63],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00],\n",
              "        [0.0000e+00]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_states[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j-eLnG0hRjLM",
        "outputId": "b8008458-dfd6-4a46-d809-f840200f5ce6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0, 9.0],\n",
              " [0.0, 9.0],\n",
              " [0.0, 8.0],\n",
              " [0.0, 7.0],\n",
              " [0.0, 6.0],\n",
              " [0.0, 5.0],\n",
              " [0.0, 4.0],\n",
              " [1.0, 4.0],\n",
              " [1.0, 3.0],\n",
              " [2.0, 3.0],\n",
              " [3.0, 3.0],\n",
              " [4.0, 3.0],\n",
              " [4.0, 2.0],\n",
              " [5.0, 2.0],\n",
              " [6.0, 2.0],\n",
              " [6.0, 1.0],\n",
              " [7.0, 1.0],\n",
              " [8.0, 1.0],\n",
              " [8.0, 1.0],\n",
              " [7.0, 1.0],\n",
              " [8.0, 1.0],\n",
              " [7.0, 1.0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0],\n",
              " [0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sums_states_weight_diff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pMnYaZnlRCDP",
        "outputId": "25745896-74bf-4ebb-9c4c-38145c149cf8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7.3060e-01],\n",
              "        [-5.3051e+05],\n",
              "        [ 5.5980e-01],\n",
              "        [ 1.3621e-01],\n",
              "        [-8.3384e+04]], dtype=torch.float64, grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jma3P44WTp-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = SCOPE_variance(model, 0.9, 10000, pi_b, P_pi_b, P_pi_e)"
      ],
      "metadata": {
        "id": "1fEf2grNuo0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s,p_s, p_w, g_w_l, s_f, s_l = test1.prepare()"
      ],
      "metadata": {
        "id": "QLqIwtmVu6Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IS Play"
      ],
      "metadata": {
        "id": "hlu2XrJwhJFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "    \"\"\"\n",
        "    Calculate importance weights for behavior policies.\n",
        "\n",
        "    Parameters:\n",
        "    - eval_policy: Evaluation policy\n",
        "    - behav_policy: Behavior policy\n",
        "    - behavior_policies: List of behavior policies\n",
        "\n",
        "    Returns:\n",
        "    - all_weights: List of importance weights\n",
        "    \"\"\"\n",
        "    all_weights_temp = []\n",
        "    for trajectory in behavior_policies:\n",
        "        cum_ratio = 1\n",
        "        cumul_weights = []\n",
        "        for step in trajectory:\n",
        "            # eval_action_probs = get_quadrant_policy(step[0], eval_policy)\n",
        "            # behav_action_probs = get_quadrant_policy(step[0], behav_policy)\n",
        "\n",
        "            P_pi_b = behav_policy[tuple(np.append(step[0].astype(int) , (step[1],)))]\n",
        "            P_pi_e = eval_policy[tuple(np.append(step[0].astype(int) , (step[1],)))]\n",
        "\n",
        "            # ratio = (0.8*eval_action_probs[step[1]] +0.2*0.25)/ (0.8*behav_action_probs[step[1]]+0.2*0.25)\n",
        "            ratio = P_pi_e/P_pi_b\n",
        "            cum_ratio *= ratio\n",
        "            cumul_weights.append(cum_ratio)\n",
        "        all_weights_temp.append(cumul_weights)\n",
        "\n",
        "        all_weights = [list(np.cumprod(i)) for i in all_weights_temp]\n",
        "\n",
        "    return all_weights_temp, all_weights"
      ],
      "metadata": {
        "id": "3NvlDTygDhon"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_weights_temp, all_weights = calculate_importance_weights(P_pi_e, P_pi_b, pi_b)"
      ],
      "metadata": {
        "id": "eWX9DHMDDl-W"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_weights_temp[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqKsg2xDDtgF",
        "outputId": "b1ba3c6a-026d-4de6-8605-95a8381f831e"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7132352941176471,\n",
              " 0.5087045847750865,\n",
              " 0.36282606414105434,\n",
              " 0.2587803545711932,\n",
              " 0.18457128230445397,\n",
              " 0.1316427528200885,\n",
              " 0.09389225752609254,\n",
              " 0.06696727191199248,\n",
              " 0.0477634218784064,\n",
              " 0.03406655825151045,\n",
              " 0.004258319781438806,\n",
              " 0.0005322899726798508,\n",
              " 0.00037964799522018767,\n",
              " 0.0002707783495320456,\n",
              " 3.38472936915057e-05,\n",
              " 2.4141084471147448e-05,\n",
              " 1.721827348309781e-05,\n",
              " 2.1522841853872264e-06,\n",
              " 1.304822287391006e-05,\n",
              " 9.306453079185852e-06,\n",
              " 1.1633066348982315e-06,\n",
              " 8.297113498906504e-07,\n",
              " 5.917794186720081e-07]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b[2,9,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF6ddbvGikSp",
        "outputId": "68a15b7e-99b7-4e6e-fa18-3b544951caed"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states_test = pi_b[0]['state']"
      ],
      "metadata": {
        "id": "Hk9m_M6xkR4-"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX3c0AZhkg8X",
        "outputId": "729607f1-4d7c-4820-f5a5-86237fef2968"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions_test = pi_b[0]['action']"
      ],
      "metadata": {
        "id": "Sf7k0cxykaqE"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4QSrLMtkduw",
        "outputId": "5ab9179a-34dd-4406-cd73-e3c1faf6cb27"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23,)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract probability for each state-action pair\n",
        "num_steps, num_states, num_actions = P_pi_b.shape\n",
        "num_samples = states_test.shape[0]\n",
        "\n",
        "# Convert states_test to integers for indexing\n",
        "states_int = states_test.astype(int)\n",
        "\n",
        "# Extract probabilities using array indexing\n",
        "probs_pi_b = P_pi_b[states_int[:, 0], states_int[:, 1], actions_test]\n",
        "probs_pi_e = P_pi_e[states_int[:, 0], states_int[:, 1], actions_test]"
      ],
      "metadata": {
        "id": "psMIyTu1j0PS"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs_pi_b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrGoFbE3kx-K",
        "outputId": "f80b0739-0035-4628-a24b-a610ef62764d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.08,\n",
              "       0.08, 0.68, 0.68, 0.08, 0.68, 0.68, 0.08, 0.08, 0.68, 0.08, 0.68,\n",
              "       0.68])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs_pi_e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d9hkR7wk_mf",
        "outputId": "3edb3fdf-94bb-4a27-f7f0-3079dca07bec"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.485, 0.485, 0.485, 0.485, 0.485, 0.485, 0.485, 0.485, 0.485,\n",
              "       0.485, 0.01 , 0.01 , 0.485, 0.485, 0.01 , 0.485, 0.485, 0.01 ,\n",
              "       0.485, 0.485, 0.01 , 0.485, 0.485])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(probs_pi_e/probs_pi_b).cumprod(axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iVhb2RVlCI8",
        "outputId": "6af4e661-1c4e-4398-ff9b-170e9bc7e28e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.13235294e-01, 5.08704585e-01, 3.62826064e-01, 2.58780355e-01,\n",
              "       1.84571282e-01, 1.31642753e-01, 9.38922575e-02, 6.69672719e-02,\n",
              "       4.77634219e-02, 3.40665583e-02, 4.25831978e-03, 5.32289973e-04,\n",
              "       3.79647995e-04, 2.70778350e-04, 3.38472937e-05, 2.41410845e-05,\n",
              "       1.72182735e-05, 2.15228419e-06, 1.30482229e-05, 9.30645308e-06,\n",
              "       1.16330663e-06, 8.29711350e-07, 5.91779419e-07])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(probs_pi_e/probs_pi_b).prod(axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haf3CzhmnP-g",
        "outputId": "8a4cc96b-b454-43f3-d94c-e4546858ecd8"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.917794186720081e-07"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFeC8cDJlRqF",
        "outputId": "04d5b1ae-ed42-465b-eccd-b87ee7f98e05"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7132352941176471,\n",
              " 0.36282606414105434,\n",
              " 0.13164275282008847,\n",
              " 0.034066558251510434,\n",
              " 0.006287708340180658,\n",
              " 0.0008277312348312113,\n",
              " 7.771755426316267e-05,\n",
              " 5.204532588676245e-06,\n",
              " 2.48586285712858e-07,\n",
              " 8.468479182763697e-09,\n",
              " 3.6061492422665387e-11,\n",
              " 1.9195170816455204e-14,\n",
              " 7.287408118376271e-18,\n",
              " 1.973272342660357e-21,\n",
              " 6.678992851535058e-26,\n",
              " 1.612381306110978e-30,\n",
              " 2.776242228765327e-35,\n",
              " 5.9752622437758e-41,\n",
              " 7.796655348684653e-46,\n",
              " 7.255920717711713e-51,\n",
              " 8.440860713209574e-57,\n",
              " 7.003477936596074e-63,\n",
              " 4.1445141020010597e-69]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSZax_wTBuz2",
        "outputId": "10644bfa-fb7d-4b7d-ca1a-ecb9f63bd4c2"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.7132352941176471,\n",
              "  0.36282606414105434,\n",
              "  0.13164275282008847,\n",
              "  0.034066558251510434,\n",
              "  0.006287708340180658,\n",
              "  0.0008277312348312113,\n",
              "  7.771755426316267e-05,\n",
              "  5.204532588676245e-06,\n",
              "  2.48586285712858e-07,\n",
              "  8.468479182763697e-09,\n",
              "  3.6061492422665387e-11,\n",
              "  1.9195170816455204e-14,\n",
              "  7.287408118376271e-18,\n",
              "  1.973272342660357e-21,\n",
              "  6.678992851535058e-26,\n",
              "  1.612381306110978e-30,\n",
              "  2.776242228765327e-35,\n",
              "  5.9752622437758e-41,\n",
              "  7.796655348684653e-46,\n",
              "  7.255920717711713e-51,\n",
              "  8.440860713209574e-57,\n",
              "  7.003477936596074e-63,\n",
              "  4.1445141020010597e-69],\n",
              " [6.0625,\n",
              "  26.214183134191178,\n",
              "  80.84510557563685,\n",
              "  177.82956323277526,\n",
              "  278.98895939887166,\n",
              "  2653.5166914163347,\n",
              "  18000.702591032885,\n",
              "  87094.33652773884,\n",
              "  300554.4756778875,\n",
              "  739757.3859855295,\n",
              "  227596.42361250217,\n",
              "  49942.971995499654,\n",
              "  1369.9141292252755,\n",
              "  26.80063802941003,\n",
              "  0.37396395766511065,\n",
              "  0.03163487428795419,\n",
              "  0.001908689588734961,\n",
              "  8.213671358375234e-05,\n",
              "  2.1428466437824626e-05,\n",
              "  3.9872887390228824e-06,\n",
              "  5.291723054894178e-07],\n",
              " [0.7132352941176471,\n",
              "  0.06358807309688581,\n",
              "  0.004043443040174894,\n",
              "  0.00018338331548064946,\n",
              "  1.0396288528903333e-06,\n",
              "  4.203680049584427e-09,\n",
              "  1.2123103392960128e-11,\n",
              "  2.493622974018479e-14,\n",
              "  3.658310788273809e-17,\n",
              "  3.827923332764183e-20,\n",
              "  2.8567924540782985e-23,\n",
              "  1.5206419773333412e-26,\n",
              "  1.011778095713248e-30,\n",
              "  8.41498960820744e-36,\n",
              "  4.991771772741668e-41,\n",
              "  3.7013987228869067e-47,\n",
              "  1.957536400670582e-53,\n",
              "  1.2940880754943391e-60,\n",
              "  6.1016971764870074e-68,\n",
              "  3.596230150266961e-76,\n",
              "  1.2849790795932546e-83,\n",
              "  3.274744808368086e-91,\n",
              "  5.95239431745802e-99,\n",
              "  7.716826756593017e-107,\n",
              "  7.135405081264397e-115,\n",
              "  4.7057769931646824e-123,\n",
              "  1.881463521646853e-130,\n",
              "  5.365288613487482e-138,\n",
              "  1.0912473473433638e-145],\n",
              " [0.125,\n",
              "  0.011144301470588236,\n",
              "  0.0007086446565254968,\n",
              "  3.213934395021691e-05,\n",
              "  1.039628852890333e-06,\n",
              "  2.3985703812334665e-08,\n",
              "  6.917300171277249e-11,\n",
              "  2.4936229740184782e-14,\n",
              "  6.411472515531416e-18,\n",
              "  1.175757087011211e-21,\n",
              "  2.6951779101998296e-26,\n",
              "  4.40646269442255e-31,\n",
              "  4.367616605600319e-35,\n",
              "  3.087676164250621e-39,\n",
              "  1.5568680559372986e-43,\n",
              "  5.598925597374785e-48,\n",
              "  1.2207011013886082e-51,\n",
              "  1.6134879567799814e-54,\n",
              "  1.521090121055426e-57,\n",
              "  1.022767664998916e-60,\n",
              "  4.904919212915387e-64,\n",
              "  1.426062237134181e-66,\n",
              "  2.9571811322967266e-69,\n",
              "  4.3737121729807376e-72,\n",
              "  4.613763172345368e-75,\n",
              "  3.4713085877923017e-78,\n",
              "  1.8627903232948804e-81,\n",
              "  7.129639150096391e-85,\n",
              "  1.946273869590634e-88,\n",
              "  3.789423814439231e-92,\n",
              "  5.262295527618552e-96,\n",
              "  5.212068491909747e-100],\n",
              " [0.7132352941176471,\n",
              "  3.0840215451989623,\n",
              "  9.511188891251395,\n",
              "  177.82956323277526,\n",
              "  2371.406154890409,\n",
              "  22554.891877038845,\n",
              "  153005.9720237795,\n",
              "  740301.8604857799,\n",
              "  447733.2137675746,\n",
              "  193135.72310271577,\n",
              "  10413.960365070396,\n",
              "  400.49957250084367,\n",
              "  10.985530119573722,\n",
              "  0.21491800837354608,\n",
              "  0.0029988684932305667,\n",
              "  2.9845225280935253e-05,\n",
              "  2.1184837024610783e-07,\n",
              "  1.0725269631729829e-09,\n",
              "  3.291872929075463e-11,\n",
              "  7.206273348629824e-13,\n",
              "  1.9719160146074268e-15,\n",
              "  3.848565853595951e-18,\n",
              "  5.357254229013603e-21]]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_first = [trajectory[0] for trajectory in weights]\n"
      ],
      "metadata": {
        "id": "fsu9StgFC5UA"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_first"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmEx-WpwC-FU",
        "outputId": "eeed1cc6-afbf-4cf0-a28e-24aa8de7c279"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7132352941176471, 6.0625, 0.7132352941176471, 0.125, 0.7132352941176471]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states[0].astype(int)[:,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6k1h2M-j_ur",
        "outputId": "98f2d6a9-cc37-4828-9d0b-7abf66df9714"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 4, 5, 6, 6, 7, 8, 8, 7, 8, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    }
  ]
}