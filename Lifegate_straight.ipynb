{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOqc420Ym57FCBfVh6cEMjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Shaping/blob/main/Lifegate_straight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "zZ2S2CI8K_jT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jsHzrhmfpiTr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "from scipy.optimize import minimize\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import torch\n",
        "import sys\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deadend dependencies"
      ],
      "metadata": {
        "id": "UMj8NNrGfwu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/microsoft/med-deadend.git\n"
      ],
      "metadata": {
        "id": "KStXBTWK3f64"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lifegate class play"
      ],
      "metadata": {
        "id": "ek8__7lVidSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "import pygame\n",
        "import numpy as np\n",
        "import click\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tbWAPyxi04a",
        "outputId": "f3ac1299-7604-4eb6-ec7a-3b6a0a04ffc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB colors\n",
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "RED = (255, 0, 0)\n",
        "BLUE = (0, 100, 255)\n",
        "GREEN = (0, 255, 0)\n",
        "WALL = (80, 80, 80)\n",
        "YELLOW = (255, 255, 0)\n",
        "\n",
        "\n",
        "\n",
        "class LifeGate(object):\n",
        "    def __init__(self, state_mode, rng, death_drag, max_steps=100, fixed_life=True, rendering=False, image_saving=False, render_dir=None):\n",
        "        self.rng = rng\n",
        "        self.state_dtype = np.float32\n",
        "        self.frame_skip = 1  # for env consistency\n",
        "        self.fixed_life = fixed_life\n",
        "        self.blue = BLUE\n",
        "        self.death_drag = death_drag\n",
        "        self.legal_actions = [0, 1, 2, 3, 4]\n",
        "        self.action_meanings = ['no-op', 'up', 'down', 'left', 'right']\n",
        "        self.reward_scheme = {'death': -1.0, 'recovery': +1.0, 'step': 0.0, 'barrier': 0.0}\n",
        "        self.nb_actions = len(self.legal_actions)\n",
        "        self.player_pos_x = None\n",
        "        self.player_pos_y = None\n",
        "        self.agent_init_pos = None\n",
        "        self.state_mode = state_mode    # how the returned state look like ('pixel' or '1hot' or 'multi-head')\n",
        "        # self.scr_w = None\n",
        "        # self.scr_h = None\n",
        "        # self.possible_recoveries = []\n",
        "        self.recovery_observablity = True\n",
        "        # self.observability_switch_point = None  # where to turn observability off\n",
        "        # self.rendering_scale = None\n",
        "        # self.barriers = None\n",
        "        self.recoveries = None\n",
        "        self.deaths = None\n",
        "        # self.dead_ends = None\n",
        "        self._rendering = rendering\n",
        "        # self.state_shape = None\n",
        "        self.init_subclass()\n",
        "        if rendering:\n",
        "            self._init_pygame()\n",
        "        self.image_saving = image_saving\n",
        "        self.render_dir_main = render_dir\n",
        "        self.render_dir = None\n",
        "        self.state = None\n",
        "        self.step_id = 0\n",
        "        self.game_over = False\n",
        "\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def init_subclass(self):\n",
        "        # should implement sizes, barriers, recoveries, deaths, init_player(), and rendering_scale\n",
        "        self.scr_w, self.scr_h = 10, 10\n",
        "        self.tabular_state_shape = (self.scr_w, self.scr_h)\n",
        "        self.state_shape = [24]\n",
        "        self.rendering_scale = 30\n",
        "        self.barriers = [[0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [1, 5], [2, 5], [3, 5], [4, 5]]\n",
        "        self.possible_recoveries = [[5, 0], [6, 0], [7, 0]]\n",
        "        self.main_deaths = [[self.scr_w - 1, k] for k in range(self.scr_h)] + [[8,0]]\n",
        "        self.dead_ends = [[x, y] for x in range(self.scr_w // 2, self.scr_w - 1) for y in range(self.scr_w // 2, self.scr_w)]\n",
        "        self.observability_switch_point = [0, 5]\n",
        "\n",
        "    @property\n",
        "    def rendering(self):\n",
        "        return self._rendering\n",
        "\n",
        "    @rendering.setter\n",
        "    def rendering(self, flag):\n",
        "        if flag is True:\n",
        "            if self._rendering is False:\n",
        "                self._init_pygame()\n",
        "                self._rendering = True\n",
        "        else:\n",
        "            self.close()\n",
        "            self._rendering = False\n",
        "\n",
        "    def _init_pygame(self):\n",
        "        pygame.init()\n",
        "        size = [self.rendering_scale * self.scr_w, self.rendering_scale * self.scr_h]\n",
        "        self.screen = pygame.display.set_mode(size)\n",
        "        pygame.display.set_caption(\"LifeGate\")\n",
        "\n",
        "    def _init_rendering_folder(self):\n",
        "        if self.render_dir_main is None:\n",
        "            self.render_dir_main = 'render'\n",
        "        if not os.path.exists(os.path.join(os.getcwd(), self.render_dir_main)):\n",
        "            os.mkdir(os.path.join(os.getcwd(), self.render_dir_main))\n",
        "        i = 0\n",
        "        while os.path.exists(os.path.join(os.getcwd(), self.render_dir_main, 'render' + str(i))):\n",
        "            i += 1\n",
        "        self.render_dir = os.path.join(os.getcwd(), self.render_dir_main, 'render' + str(i))\n",
        "        os.mkdir(self.render_dir)\n",
        "\n",
        "    def reset(self):\n",
        "        if self.image_saving:\n",
        "            self._init_rendering_folder()\n",
        "        self.game_over = False\n",
        "        self.step_id = 0\n",
        "        self.recovery_observablity = True\n",
        "        self.blue = BLUE\n",
        "        state = self.init_episode()\n",
        "        return state\n",
        "\n",
        "    def init_episode(self):\n",
        "        # should implement reconfigurations at the beginning of each episode\n",
        "        self.player_pos_x, self.player_pos_y = 2, self.scr_h - 1\n",
        "        targets = deepcopy(self.possible_recoveries)\n",
        "        # if self.fixed_life == True:\n",
        "        #     rec = targets.pop(2)  # fixed life-gate for DQN\n",
        "        # else:\n",
        "        #     rec = targets.pop(self.rng.randint(len(targets)))\n",
        "        self.recoveries = targets #[rec]\n",
        "        self.deaths = self.main_deaths #+ targets\n",
        "        return self.get_obs(self.state_mode)\n",
        "\n",
        "    def render(self):\n",
        "        if not self.rendering:\n",
        "            return\n",
        "        pygame.event.pump()\n",
        "        self.screen.fill(BLACK)\n",
        "        size = [self.rendering_scale, self.rendering_scale]\n",
        "        for pos in self.dead_ends:\n",
        "            p = [self.rendering_scale * pos[0], self.rendering_scale * pos[1]]\n",
        "            rec1 = pygame.Rect(p[0], p[1], size[0], size[1])\n",
        "            pygame.draw.rect(self.screen, YELLOW, rec1)\n",
        "        player = pygame.Rect(self.rendering_scale * self.player_pos_x, self.rendering_scale * self.player_pos_y,\n",
        "                             size[0], size[1])\n",
        "        pygame.draw.rect(self.screen, WHITE, player)\n",
        "        for pos in self.deaths:\n",
        "            p = [self.rendering_scale * pos[0], self.rendering_scale * pos[1]]\n",
        "            rec1 = pygame.Rect(p[0], p[1], size[0], size[1])\n",
        "            pygame.draw.rect(self.screen, RED, rec1)\n",
        "        for pos in self.recoveries:\n",
        "            p = [self.rendering_scale * pos[0], self.rendering_scale * pos[1]]\n",
        "            rec1 = pygame.Rect(p[0], p[1], size[0], size[1])\n",
        "            pygame.draw.rect(self.screen, self.blue, rec1)  # self.blue will change if reach obs point\n",
        "        for pos in self.barriers:\n",
        "            p = [self.rendering_scale * pos[0], self.rendering_scale * pos[1]]\n",
        "            rec1 = pygame.Rect(p[0], p[1], size[0], size[1])\n",
        "            pygame.draw.rect(self.screen, WALL, rec1)\n",
        "        pygame.display.flip()\n",
        "\n",
        "        if self.image_saving:\n",
        "            self.save_image()\n",
        "\n",
        "    def save_image(self):\n",
        "        if self.rendering and self.render_dir is not None:\n",
        "            pygame.image.save(self.screen, self.render_dir + '/render' + str(self.step_id) + '.jpg')\n",
        "        else:\n",
        "            raise ValueError('env.rendering is False and/or environment has not been reset.')\n",
        "\n",
        "    def close(self):\n",
        "        if self.rendering:\n",
        "            pygame.quit()\n",
        "\n",
        "    def _move_player(self, action):\n",
        "        x, y = (self.player_pos_x, self.player_pos_y)\n",
        "        # dead-end:\n",
        "        if [x, y] in self.dead_ends:\n",
        "            if self.rng.binomial(1, 0.70):\n",
        "                action = 4  # forceful right\n",
        "            else:\n",
        "                action = 0  # no-op\n",
        "        else:\n",
        "            # natural risk of death\n",
        "            if self.rng.binomial(1, self.death_drag):  # say with 25% if death_drag==0.25\n",
        "                action = 4\n",
        "\n",
        "        if action == 4:    # right\n",
        "            x += 1\n",
        "        elif action == 3:  # left\n",
        "            x -= 1\n",
        "        elif action == 2:  # down\n",
        "            y += 1\n",
        "        elif action == 1:  # up\n",
        "            y -= 1\n",
        "        # updating the position\n",
        "        if [x, y] in self.barriers or x < 0 or y < 0 or y >= self.scr_h:\n",
        "            return\n",
        "        else:\n",
        "            self.player_pos_x, self.player_pos_y = x, y\n",
        "\n",
        "    def _get_status(self):\n",
        "        # check the current situation\n",
        "        if [self.player_pos_x, self.player_pos_y] in self.deaths:\n",
        "            return 'death'\n",
        "        elif [self.player_pos_x, self.player_pos_y] in self.recoveries:\n",
        "            return 'recovery'\n",
        "\n",
        "    def step(self, action):\n",
        "        assert action in self.legal_actions, 'Illegal action.'\n",
        "        if self.step_id >= self.max_steps - 1:\n",
        "            self.game_over = True\n",
        "            return self.get_obs(self.state_mode), 0., self.game_over, {}\n",
        "        self.step_id += 1\n",
        "        self._move_player(action)\n",
        "        if [self.player_pos_x, self.player_pos_y] == self.observability_switch_point and self.recovery_observablity == True:\n",
        "            self.recovery_observablity = False\n",
        "            self.blue = BLACK\n",
        "        status = self._get_status()\n",
        "        if status == 'death':\n",
        "            self.game_over = True\n",
        "            reward = self.reward_scheme['death']\n",
        "        elif status == 'recovery':\n",
        "            self.game_over = True\n",
        "            reward = self.reward_scheme['recovery']\n",
        "        else:\n",
        "            reward = self.reward_scheme['step']\n",
        "        return self.get_obs(self.state_mode), reward, self.game_over, {}\n",
        "\n",
        "    def get_lives(self):\n",
        "        if self.game_over == True:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.get_obs(self.state_mode)\n",
        "\n",
        "    def get_obs(self, method):\n",
        "        if method == 'vector':\n",
        "            return self._get_vec_obs()\n",
        "        elif method == 'pixel':\n",
        "            return self._get_pixel_obs()\n",
        "        elif method == 'tabular':\n",
        "            return self._get_tabular_obs()\n",
        "        else:\n",
        "            raise ValueError('Unknown observation method.')\n",
        "\n",
        "    def _get_vec_obs(self):\n",
        "        x = np.zeros(self.scr_w + self.scr_h + len(self.possible_recoveries), dtype=self.state_dtype)\n",
        "        x[self.player_pos_x] = 1.0\n",
        "        x[self.player_pos_y + self.scr_w] = 1.0\n",
        "        if self.recovery_observablity == True or self.fixed_life == True:\n",
        "            for k in self.recoveries:\n",
        "                x[k[0] - 5 + self.scr_w + self.scr_h] = 1.0\n",
        "        return x\n",
        "\n",
        "    def _get_tabular_obs(self):\n",
        "        return np.array([self.player_pos_x, self.player_pos_y])\n",
        "\n",
        "    def _get_pixel_obs(self):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "1p8iyu4BifbS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# shaping dependencies"
      ],
      "metadata": {
        "id": "AsZMw4C0f2K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ajagota7/Shaping.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu7X59dK3hqk",
        "outputId": "7e6484f5-f620-4b10-c7ab-8dc526877613"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Shaping'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 144 (delta 73), reused 107 (delta 43), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (144/144), 12.00 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Shaping"
      ],
      "metadata": {
        "id": "5SkpvWFqz631"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git pull origin main"
      ],
      "metadata": {
        "id": "8QRHhC-G0AiH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/"
      ],
      "metadata": {
        "id": "hZ8dnFnidxL_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/Shaping\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/Shaping/lifegate_1.zip', 'r') as zip_ref:\n",
        "    # zip_ref.extractall('/content/med-deadend/lifegate/results/lifegate_1')\n",
        "    zip_ref.extractall('/content/Shaping/')"
      ],
      "metadata": {
        "id": "2noY6FOTdsmY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.path.append('/content/med-deadend/lifegate')\n",
        "sys.path.append('/content/Shaping/')\n",
        "\n"
      ],
      "metadata": {
        "id": "id2reVHQ3heg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import q_networks"
      ],
      "metadata": {
        "id": "DgppFp3cDm4L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/med-deadend/lifegate\n",
        "\n",
        "\n",
        "# results_dir = 'results/lifegate_1/'\n",
        "results_dir = '/content/Shaping/'\n",
        "# Load the Q tables from the primary learning agent, Q_D and Q_R value functions\n",
        "with open(results_dir+'tabular_qnet.pkl', 'rb') as fq:\n",
        "    ai = pickle.load(fq)\n",
        "\n",
        "with open(results_dir+'tabular_qd.pkl', 'rb') as fd:\n",
        "    ai_d = pickle.load(fd)\n",
        "\n",
        "with open(results_dir+'tabular_qr.pkl', 'rb') as fr:\n",
        "    ai_r = pickle.load(fr)"
      ],
      "metadata": {
        "id": "7lv4ZIBkeLW3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((10, 10, 5))\n",
        "q_d = np.zeros_like(q_table)\n",
        "q_r = np.zeros_like(q_table)\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        for a in range(5):\n",
        "            key = tuple([j, i, a])\n",
        "            try:\n",
        "                q_table[i,j,a] = ai.q[key]\n",
        "                q_d[i,j,a] = ai_d.q[key]\n",
        "                q_r[i,j,a] = ai_r.q[key]\n",
        "            except:\n",
        "                pass"
      ],
      "metadata": {
        "id": "Rk0Z42sNebl4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import random\n",
        "# from lifegate import LifeGate\n",
        "params = yaml.safe_load(open(results_dir+'config.yaml', 'r'))\n",
        "np.random.seed(seed=params['random_seed'])\n",
        "random.seed(params['random_seed'])\n",
        "random_state = np.random.RandomState(params['random_seed'])"
      ],
      "metadata": {
        "id": "4uTTjsWNfK21"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# env"
      ],
      "metadata": {
        "id": "fjTGtdx8wCeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = LifeGate(max_steps=params['episode_max_len'], state_mode='tabular',\n",
        "                        rendering=True, image_saving=False, render_dir=None, rng=random_state, death_drag = 0.0)"
      ],
      "metadata": {
        "id": "XUoJuLN0fabn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_30 = LifeGate(max_steps=30, state_mode='tabular',\n",
        "                        rendering=True, image_saving=False, render_dir=None, rng=random_state, death_drag = 0.1)"
      ],
      "metadata": {
        "id": "7wG_zU6SM3xY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_50 = LifeGate(max_steps=50, state_mode='tabular',\n",
        "                        rendering=True, image_saving=False, render_dir=None, rng=random_state, death_drag = 0.1)"
      ],
      "metadata": {
        "id": "llhj96oRwFZ9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZp-8-f7far2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Shaping\n",
        "from Shaping import *\n",
        "# %cd /content/Shaping\n",
        "\n",
        "from choose_actions import action_probs_top_n_epsilon\n",
        "from shaping_features import *\n",
        "from gen_policies import *\n",
        "from IS import *\n",
        "from subset_policies import *\n",
        "from v_pi_e import *\n",
        "from optimization import *\n",
        "from neural_net import *\n",
        "from prep_variance import *\n",
        "from SCOPE_variance import SCOPE_variance"
      ],
      "metadata": {
        "id": "hS65UmL5Yu_K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JYUrYs3BCAMn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model with l2 reg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EQrnwRzAJTKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CustomizableFeatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_prob=0.2, l2_lambda=0.01, dtype=torch.float32):\n",
        "        super(CustomizableFeatureNet, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # Create the hidden layers based on the provided sizes\n",
        "        for in_dim, out_dim in zip([input_dim] + hidden_dims, hidden_dims):\n",
        "            layer = nn.Linear(in_dim, out_dim).to(dtype)\n",
        "            self.hidden_layers.append(layer)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim).to(dtype)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.l2_lambda = l2_lambda\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = F.relu(layer(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def l2_regularization(self):\n",
        "        l2_reg = torch.tensor(0., device=self.output_layer.weight.device)\n",
        "        for layer in self.hidden_layers:\n",
        "            l2_reg += torch.norm(layer.weight)\n",
        "        l2_reg += torch.norm(self.output_layer.weight)\n",
        "        return self.l2_lambda * l2_reg\n"
      ],
      "metadata": {
        "id": "FVdQ4HDTJTK1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCOPE straight"
      ],
      "metadata": {
        "id": "aIFnUpUIlyr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SCOPE_straight(object):\n",
        "\n",
        "  def __init__(self, model, gamma, num_bootstraps, pi_b, P_pi_b, P_pi_e, percent_to_estimate_phi, dtype):\n",
        "        self.model = model\n",
        "        self.gamma = gamma\n",
        "        self.num_bootstraps = num_bootstraps\n",
        "        self.pi_b = pi_b\n",
        "        self.P_pi_b = P_pi_b\n",
        "        self.P_pi_e = P_pi_e\n",
        "        self.dtype = dtype\n",
        "\n",
        "        self.percent_to_estimate_phi = percent_to_estimate_phi\n",
        "        # self.num_epochs = num_epochs\n",
        "\n",
        "  def subset_policies(self):\n",
        "    # seed_value = 0\n",
        "    # np.random.seed(seed_value)\n",
        "    num_policies = len(self.pi_b)\n",
        "    num_policies_to_estimate_phi = int(num_policies * self.percent_to_estimate_phi)\n",
        "\n",
        "    policies_for_scope = self.pi_b[num_policies_to_estimate_phi:]\n",
        "    policies_for_phi = self.pi_b[:num_policies_to_estimate_phi]\n",
        "\n",
        "    return policies_for_phi, policies_for_scope\n",
        "\n",
        "\n",
        "  # ---------------\n",
        "  # Pre-processing\n",
        "  # ---------------\n",
        "\n",
        "  def prep_policies(self, chosen_policies):\n",
        "      # Initialize lists to store axis data for each policy\n",
        "      timesteps = []\n",
        "      # states = []\n",
        "      # state_first = []\n",
        "      # state_last = []\n",
        "      actions = []\n",
        "      rewards = []\n",
        "      # gamma_last = []\n",
        "      # weight_last = []\n",
        "      # weight_first = []\n",
        "      # all_weights_temp, weights = calculate_importance_weights(P_pi_e, P_pi_b, pi_b)\n",
        "      weights = calculate_importance_weights(self.P_pi_e, self.P_pi_b, chosen_policies)\n",
        "      psi = []\n",
        "\n",
        "      states_current = []\n",
        "      states_next = []\n",
        "      states_all = []\n",
        "\n",
        "      states_last = []\n",
        "      psi_last = []\n",
        "\n",
        "      for index, policy in enumerate(chosen_policies):\n",
        "          policy_array = np.array(policy)\n",
        "\n",
        "          timesteps.append(policy_array['timestep'].astype(int))\n",
        "          actions.append(policy_array['action'])\n",
        "          rewards.append(policy_array['reward'].astype(float))\n",
        "\n",
        "          state_last = policy_array['state_next'][-1]\n",
        "          last_psi = smallest_distance_to_deadend(state_last, env)\n",
        "          states_last.append(state_last)\n",
        "          psi_last.append(last_psi)\n",
        "\n",
        "          # Concatenate psi array with last_psi\n",
        "          # all_psi = np.concatenate((policy_array['psi'], [last_psi]))\n",
        "          # psi.append(all_psi)\n",
        "          psi.append(policy_array['psi'])\n",
        "\n",
        "          states_next.append(policy_array['state_next'])\n",
        "          states_current.append(policy_array['state'])\n",
        "          # all_states = policy_array['state'] + policy_array['state_next'][-1]\n",
        "          all_states = np.vstack((policy_array['state'],policy_array['state_next'][-1]))\n",
        "          states_all.append(all_states)\n",
        "\n",
        "          # states_all.append(np.concatenate((policy_array['state'], policy_array['state_next'][-1])))\n",
        "\n",
        "\n",
        "\n",
        "      return timesteps, rewards, states_next, states_current, weights, actions, psi, states_last, psi_last\n",
        "\n",
        "  def padding_IS_terms(self, timesteps, actions, rewards, weights):\n",
        "\n",
        "    # Find the maximum length among all lists\n",
        "    max_length = max(len(traj) for traj in timesteps)\n",
        "\n",
        "    # Define the padding values\n",
        "    zero_padding = 0\n",
        "\n",
        "    # Pad each list to match the maximum length\n",
        "    padded_timesteps = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in timesteps]\n",
        "    padded_rewards = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in rewards]\n",
        "    padded_actions = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in actions]\n",
        "    padded_weights = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in weights]\n",
        "\n",
        "    return padded_timesteps, padded_rewards, padded_actions, padded_weights\n",
        "\n",
        "\n",
        "  def tensorize_IS_terms(self, padded_timesteps, padded_rewards, padded_weights):\n",
        "\n",
        "    padded_timestep_tensors = torch.tensor(padded_timesteps, dtype = self.dtype)\n",
        "    padded_reward_tensors = torch.tensor(padded_rewards, dtype = self.dtype)\n",
        "    padded_weight_tensors = torch.tensor(padded_weights, dtype = self.dtype)\n",
        "\n",
        "    return padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors\n",
        "\n",
        "  def padding_states_all(self, states_all, psi):\n",
        "    max_length = max(len(trajectory) for trajectory in states_all)\n",
        "\n",
        "    zero_padding = 0\n",
        "\n",
        "    # Pad each trajectory to make them all the same length\n",
        "    padded_states_all = [\n",
        "        [list(item) for item in trajectory] + [[0, 0]] * (max_length - len(trajectory))\n",
        "        for trajectory in states_all\n",
        "    ]\n",
        "\n",
        "    padded_psi = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in psi]\n",
        "    mask = [[1] * len(trajectory) + [0] * (max_length - len(trajectory)) for trajectory in states_all]\n",
        "\n",
        "    return padded_states_all, padded_psi, mask\n",
        "\n",
        "\n",
        "\n",
        "  def padding_states(self, states_next, states_current, psi):\n",
        "    # Find the maximum length of trajectories\n",
        "    max_length = max(len(trajectory) for trajectory in states_current)\n",
        "\n",
        "    zero_padding = 0\n",
        "\n",
        "    # Pad each trajectory to make them all the same length\n",
        "    padded_states_next = [\n",
        "        [list(item) for item in trajectory] + [[0, 0]] * (max_length - len(trajectory))\n",
        "        for trajectory in states_next\n",
        "    ]\n",
        "\n",
        "    # Pad each trajectory to make them all the same length\n",
        "    padded_states_current = [\n",
        "        [list(item) for item in trajectory] + [[0, 0]] * (max_length - len(trajectory))\n",
        "        for trajectory in states_current\n",
        "    ]\n",
        "\n",
        "    padded_psi = [np.concatenate([traj, [zero_padding] * (max_length - len(traj))]) for traj in psi]\n",
        "\n",
        "    # Create mask\n",
        "    mask = [[1] * len(trajectory) + [0] * (max_length - len(trajectory)) for trajectory in states_current]\n",
        "\n",
        "    return padded_states_next, padded_states_current, padded_psi, mask\n",
        "\n",
        "\n",
        "  def tensorize_padded_terms(self, padded_states_next, padded_states_current, padded_psi,mask):\n",
        "    padded_states_next_tensors = torch.tensor(padded_states_next, dtype = self.dtype)\n",
        "    padded_states_current_tensors = torch.tensor(padded_states_current, dtype = self.dtype)\n",
        "    padded_psi_tensors = torch.tensor(padded_psi, dtype = self.dtype)\n",
        "\n",
        "    mask_tensor = torch.tensor(mask, dtype = self.dtype)\n",
        "    return padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor\n",
        "\n",
        "  def tensorize_all_states_psi(self, padded_states_all, padded_psi, mask):\n",
        "    padded_states_all_tensors = torch.tensor(padded_states_all, dtype = self.dtype)\n",
        "    padded_psi_tensors = torch.tensor(padded_psi, dtype = self.dtype)\n",
        "    mask_tensor = torch.tensor(mask, dtype = self.dtype)\n",
        "\n",
        "    return padded_states_all_tensors, padded_psi_tensors, mask_tensor\n",
        "\n",
        "  def tensorize_last_states_psi(self, states_last, psi_last):\n",
        "    states_last_tensor = torch.tensor(states_last, dtype = self.dtype)\n",
        "    psi_last_tensor = torch.tensor(psi_last, dtype = self.dtype)\n",
        "\n",
        "    return states_last_tensor, psi_last_tensor\n",
        "\n",
        "  #-----------------------\n",
        "  # Preparation Functions\n",
        "  # ----------------------\n",
        "\n",
        "  def prepare_IS(self):\n",
        "    timesteps, rewards, states_next, states_current, weights, actions,_,_,_ = self.prep_policies(self.pi_b)\n",
        "    padded_timesteps, padded_rewards, padded_actions, padded_weights = self.padding_IS_terms(timesteps, actions, rewards, weights)\n",
        "    padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors = self.tensorize_IS_terms(padded_timesteps, padded_rewards, padded_weights)\n",
        "\n",
        "    return padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors\n",
        "\n",
        "  def prepare_SCOPE(self, policies):\n",
        "    timesteps, rewards, states_next, states_current, weights, actions, psi,states_last, psi_last = self.prep_policies(policies)\n",
        "    padded_timesteps, padded_rewards, padded_actions, padded_weights = self.padding_IS_terms(timesteps, actions, rewards, weights)\n",
        "    padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors = self.tensorize_IS_terms(padded_timesteps, padded_rewards, padded_weights)\n",
        "    padded_states_next, padded_states_current, padded_psi, mask = self.padding_states(states_next, states_current, psi)\n",
        "    padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor = self.tensorize_padded_terms(padded_states_next, padded_states_current, padded_psi, mask)\n",
        "    states_last_tensor, psi_last_tensor = self.tensorize_last_states_psi(states_last, psi_last)\n",
        "    return padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor, states_last_tensor, psi_last_tensor\n",
        "\n",
        "  def prepare_SCOPE_phi(self):\n",
        "    phi_set,_ = self.subset_policies()\n",
        "    padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor, states_last_tensor, psi_last_tensor = self.prepare_SCOPE(phi_set)\n",
        "\n",
        "    return padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor, states_last_tensor, psi_last_tensor\n",
        "\n",
        "  def prepare_SCOPE_test(self):\n",
        "    _, scope_set = self.subset_policies()\n",
        "    padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors,_,_,_,_ = self.prepare_SCOPE(scope_set)\n",
        "\n",
        "    return padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors\n",
        "\n",
        "\n",
        "  # ----------------\n",
        "  # IS Calculations\n",
        "  # ----------------\n",
        "\n",
        "\n",
        "  def bootstrap_IS(self, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors):\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    num_samples = self.num_bootstraps\n",
        "    num_bootstraps_lin = num_samples*padded_timestep_tensors.shape[0]\n",
        "\n",
        "    # Sample indices with replacement\n",
        "    sampled_indices = torch.randint(0, len(padded_timestep_tensors), size=(num_bootstraps_lin,), dtype=torch.long)\n",
        "\n",
        "    reshaped_size = (num_samples, padded_timestep_tensors.shape[0], padded_timestep_tensors.shape[1])\n",
        "\n",
        "    padded_IS = self.gamma**(padded_timestep_tensors)*padded_weight_tensors*padded_reward_tensors\n",
        "\n",
        "    IS_bootstraps = padded_IS[sampled_indices].view(reshaped_size)\n",
        "\n",
        "    # timestep_bootstraps = padded_timestep_tensors[sampled_indices].view(reshaped_size)\n",
        "    # rewards_bootstraps = padded_reward_tensors[sampled_indices].view(reshaped_size)\n",
        "    # weights_bootstraps = padded_weight_tensors[sampled_indices].view(reshaped_size)\n",
        "    # return timestep_bootstraps, rewards_bootstraps, weights_bootstraps, IS_bootstraps\n",
        "    return IS_bootstraps\n",
        "\n",
        "\n",
        "  def calc_var_IS(self, IS_bootstraps):\n",
        "    # Step 1: Sum along the third dimension\n",
        "    sum_IS_trajectories = torch.sum(IS_bootstraps, dim=2)  # Shape: [1000, 1000]\n",
        "\n",
        "    # Step 2: Take the mean along the second dimension\n",
        "    mean_IS_sum = torch.mean(sum_IS_trajectories, dim=1)  # Shape: [1000]\n",
        "\n",
        "    # Step 3: Calculate the variance across the first dimension\n",
        "    IS_variance = torch.var(mean_IS_sum)  # A single scalar value\n",
        "\n",
        "    IS_mean = torch.mean(mean_IS_sum) # A single scalar value\n",
        "\n",
        "    return IS_mean, IS_variance\n",
        "\n",
        "\n",
        "  def IS_pipeline(self):\n",
        "    padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS = self.prepare_IS()\n",
        "    # timestep_bootstraps_IS, rewards_bootstraps_IS, weights_bootstraps_IS = self.bootstrap_IS(padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS)\n",
        "    IS_bootstraps = self.bootstrap_IS(padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS)\n",
        "    # IS_mean, IS_variance = self.calc_variance_IS(timestep_bootstraps_IS, rewards_bootstraps_IS, weights_bootstraps_IS)\n",
        "    IS_mean, IS_variance = self.calc_var_IS(IS_bootstraps)\n",
        "\n",
        "    return IS_mean, IS_variance\n",
        "\n",
        "\n",
        "\n",
        "  # ---------------------\n",
        "  # SCOPE calculations\n",
        "  # ---------------------\n",
        "\n",
        "  def pass_states(self, padded_states_next_tensors, padded_states_current_tensors):\n",
        "    states_next_output = self.model(padded_states_next_tensors)\n",
        "    states_current_output = self.model(padded_states_current_tensors)\n",
        "\n",
        "    return states_next_output.squeeze(), states_current_output.squeeze()\n",
        "\n",
        "  def bootstrap_straight(self, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output):\n",
        "      seed = 42\n",
        "      torch.manual_seed(seed)\n",
        "\n",
        "      num_samples = self.num_bootstraps\n",
        "      num_bootstraps_lin = num_samples*padded_timestep_tensors.shape[0]\n",
        "\n",
        "      # Sample indices with replacement\n",
        "      sampled_indices = torch.randint(0, len(padded_timestep_tensors), size=(num_bootstraps_lin,), dtype=torch.long)\n",
        "\n",
        "      reshaped_size = (num_samples, padded_timestep_tensors.shape[0], padded_timestep_tensors.shape[1])\n",
        "\n",
        "      padded_scope = self.gamma**(padded_timestep_tensors)*padded_weight_tensors*(padded_reward_tensors +self.gamma*states_next_output - states_current_output)\n",
        "      scope_bootstraps = padded_scope[sampled_indices].view(reshaped_size)\n",
        "\n",
        "      return scope_bootstraps\n",
        "\n",
        "  def pass_then_boostraps(self, padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors):\n",
        "    states_next_output, states_current_output = self.pass_states(padded_states_next_tensors, padded_states_current_tensors)\n",
        "    # timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = self.bootstrap_straight(padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output)\n",
        "    scope_bootstraps = self.bootstrap_straight(padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output)\n",
        "    # return timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps\n",
        "    return scope_bootstraps\n",
        "\n",
        "  def calc_var_straight(self, scope_bootstraps):\n",
        "\n",
        "    # Step 1: Sum along the third dimension\n",
        "    sum_scope_trajectories = torch.sum(scope_bootstraps, dim=2)  # Shape: [1000, 1000]\n",
        "\n",
        "    # Step 2: Take the mean along the second dimension\n",
        "    mean_scope_sum = torch.mean(sum_scope_trajectories, dim=1)  # Shape: [1000]\n",
        "\n",
        "    # Step 3: Calculate the variance across the first dimension\n",
        "    scope_variance = torch.var(mean_scope_sum)  # A single scalar value\n",
        "\n",
        "    scope_mean = torch.mean(mean_scope_sum) # A single scalar value\n",
        "\n",
        "    return scope_mean, scope_variance\n",
        "\n",
        "  def train_var_scope(self, num_epochs, learning_rate, scope_weight=1, mse_weight=1):\n",
        "\n",
        "      # IS terms for comparison to SCOPE\n",
        "      padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS = self.prepare_IS()\n",
        "      # timestep_bootstraps_IS, rewards_bootstraps_IS, weights_bootstraps_IS = self.bootstrap_IS(padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS)\n",
        "      # IS_mean, IS_variance = self.calc_variance_IS(timestep_bootstraps_IS, rewards_bootstraps_IS, weights_bootstraps_IS)\n",
        "\n",
        "      IS_bootstraps = self.bootstrap_IS(padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS)\n",
        "      IS_mean, IS_variance = self.calc_var_IS(IS_bootstraps)\n",
        "\n",
        "      # SCOPE terms for training phi\n",
        "      padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor, states_last_tensor, psi_last_tensor = self.prepare_SCOPE_phi()\n",
        "\n",
        "\n",
        "      self.model.train()\n",
        "\n",
        "      # Enable anomaly detection\n",
        "      torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "      optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          total_loss = 0\n",
        "\n",
        "\n",
        "          # timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = self.pass_then_boostraps(padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors)\n",
        "\n",
        "          states_next_output, states_current_output = self.pass_states(padded_states_next_tensors, padded_states_current_tensors)\n",
        "          # timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = self.bootstrap_straight(padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output)\n",
        "          # SCOPE_mean, SCOPE_variance = self.calc_variance_straight(timestep_bootstraps, weights_bootstraps, rewards_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps)\n",
        "\n",
        "          scope_bootstraps = self.bootstrap_straight(padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output)\n",
        "          SCOPE_mean, SCOPE_variance = self.calc_var_straight(scope_bootstraps)\n",
        "\n",
        "          # mse_loss = F.mse_loss(states_current_output, 0.2*padded_psi_tensors)\n",
        "          mse_loss = F.mse_loss(states_current_output, 0.1*padded_psi_tensors, reduction='none')\n",
        "          masked_mse_loss = mse_loss * mask_tensor\n",
        "\n",
        "          states_last_output = self.model(states_last_tensor)\n",
        "          mse_states_last_loss = F.mse_loss(states_last_output.squeeze(),0.1*psi_last_tensor, reduction = 'none')\n",
        "\n",
        "          # mean_mse_loss = masked_mse_loss.mean()\n",
        "          sum_mse_loss = torch.sum(masked_mse_loss, dim = 1)\n",
        "\n",
        "          mean_mse_loss = torch.mean(sum_mse_loss + mse_states_last_loss)\n",
        "\n",
        "\n",
        "          print(f\"Epoch {epoch+1}\")\n",
        "          print(\"IS variance: \", IS_variance)\n",
        "          print(\"SCOPE Var loss: \", SCOPE_variance)\n",
        "          print(\"MSE loss: \", mean_mse_loss.item())\n",
        "\n",
        "          # Testing evaluaton\n",
        "          scope_mean, scope_var = self.evaluate_scope()\n",
        "          print(f\"SCOPE mean: {scope_mean}, SCOPE var: {scope_var}\")\n",
        "          self.model.train()\n",
        "\n",
        "\n",
        "          # tot = SCOPE_variance\n",
        "          # tot = SCOPE_variance + mse_loss\n",
        "          tot = scope_weight*SCOPE_variance + mse_weight*mean_mse_loss\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Retain the graph to avoid clearing it before backward pass\n",
        "          tot.backward(retain_graph=True)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += tot.item()\n",
        "\n",
        "          print(f\"Total Loss: {total_loss}\")\n",
        "          print(\"-\" * 40)\n",
        "\n",
        "      # Disable anomaly detection after running the code\n",
        "      torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "      for name, param in self.model.named_parameters():\n",
        "          if param.requires_grad:\n",
        "              print(f\"Parameter name: {name}\")\n",
        "              print(f\"Weights: {param.data}\")\n",
        "\n",
        "      return self.model #, sum_mse_loss, mse_states_last_loss\n",
        "\n",
        "  def evaluate_scope(self):\n",
        "    self.model.eval()\n",
        "    padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors = self.prepare_SCOPE_test()\n",
        "    # timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = self.pass_then_boostraps(padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors)\n",
        "    # SCOPE_mean, SCOPE_variance = self.calc_variance_straight(timestep_bootstraps, weights_bootstraps, rewards_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps)\n",
        "\n",
        "    scope_bootstraps = self.pass_then_boostraps(padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors)\n",
        "    SCOPE_mean, SCOPE_variance = self.calc_var_straight(scope_bootstraps)\n",
        "\n",
        "    return SCOPE_mean, SCOPE_variance\n",
        "\n",
        "\n",
        "  # -----------------------\n",
        "  # Heatmaps for lifegate\n",
        "  # -----------------------\n",
        "  def get_model_output_dict(self):\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    # Initialize an empty dictionary to store data\n",
        "    data = {}\n",
        "\n",
        "    # Loop through all combinations from [0,0] to [9,9]\n",
        "    for i in range(10):\n",
        "      for j in range(10):\n",
        "          # Prepare input data\n",
        "          input_data = torch.tensor([i, j], dtype=torch.float64)\n",
        "\n",
        "          # Pass input through the self.model\n",
        "          output = self.model(input_data)\n",
        "\n",
        "          # Store data in the dictionary\n",
        "          data[(i, j)] = output.item()\n",
        "\n",
        "    return data\n",
        "\n",
        "  def plot_heatmap(self, data):\n",
        "    values = np.zeros((10, 10))\n",
        "    for (x, y), value in data.items():\n",
        "        values[y, x] = value\n",
        "\n",
        "    # Create the heatmap\n",
        "    fig = go.Figure(data=go.Heatmap(z=values, colorscale='viridis'))\n",
        "\n",
        "    # Add colorbar\n",
        "    fig.update_layout(coloraxis_colorbar=dict(title='Values',\n",
        "                                              ticks='outside',\n",
        "                                              tickvals=[np.min(values), np.max(values)],\n",
        "                                              ticktext=[np.min(values), np.max(values)]))\n",
        "\n",
        "    # Add labels and title\n",
        "    fig.update_layout(xaxis=dict(tickvals=np.arange(10), ticktext=list(range(10)), title='X'),\n",
        "                      yaxis=dict(tickvals=np.arange(9, -1, -1), ticktext=list(range(9, -1, -1)), title='Y', autorange=\"reversed\"),\n",
        "                      title='Heatmap')\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "  def get_heatmap(self):\n",
        "    data = self.get_model_output_dict()\n",
        "    self.plot_heatmap(data)\n",
        "\n",
        "  # ---------------------\n",
        "  # State Visitation Heatmap\n",
        "  # ---------------------\n",
        "\n",
        "  def count_state_visits(self):\n",
        "    state_visit_counts = {}\n",
        "    for trajectory in self.pi_b:\n",
        "        for data_point in trajectory:\n",
        "            state = tuple(data_point['state'])\n",
        "            if state not in state_visit_counts:\n",
        "                state_visit_counts[state] = 0\n",
        "            state_visit_counts[state] += 1\n",
        "\n",
        "        # Include last state_next of the trajectory\n",
        "        last_state_next = tuple(trajectory[-1]['state_next'])\n",
        "        if last_state_next not in state_visit_counts:\n",
        "            state_visit_counts[last_state_next] = 0\n",
        "        state_visit_counts[last_state_next] += 1\n",
        "\n",
        "    return state_visit_counts\n",
        "\n",
        "  def create_state_visit_dict(self):\n",
        "      state_visit_dict = {}\n",
        "      for i in range(10):\n",
        "          for j in range(10):\n",
        "              state_visit_dict[(i, j)] = 0\n",
        "      return state_visit_dict\n",
        "\n",
        "  def fill_state_visit_dict(self,state_visit_counts):\n",
        "      state_visit_dict = self.create_state_visit_dict()\n",
        "      for state, count in state_visit_counts.items():\n",
        "          state_visit_dict[state] = count\n",
        "      return state_visit_dict\n",
        "\n",
        "\n",
        "  def plot_state_visitations_heatmap(self, state_visit_dict):\n",
        "    # Create lists to store x, y, and z values\n",
        "    x = []\n",
        "    y = []\n",
        "    z = []\n",
        "\n",
        "    # Iterate through the state visit dictionary\n",
        "    for state, count in state_visit_dict.items():\n",
        "        x.append(state[0])\n",
        "        y.append(9 - state[1])  # Flip y-axis to have (0, 0) at the bottom-left\n",
        "        z.append(count)\n",
        "\n",
        "    # Create the heatmap trace\n",
        "    trace = go.Heatmap(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        z=z,\n",
        "        colorscale='Viridis',  # Choose a colorscale\n",
        "        colorbar=dict(title='Visits'),\n",
        "        zmin=0,\n",
        "        zmax=max(z)  # Set maximum value for the color scale\n",
        "    )\n",
        "\n",
        "    # Create layout\n",
        "    layout = go.Layout(\n",
        "        title='State Visitations Heatmap',\n",
        "        xaxis=dict(title='X-axis'),\n",
        "        yaxis=dict(title='Y-axis', tickvals=list(range(10)), ticktext=list(range(9, -1, -1))),\n",
        "    )\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure(data=[trace], layout=layout)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "  def get_state_visitation_heatmap(self):\n",
        "\n",
        "    # Count state visits\n",
        "    state_visit_counts = self.count_state_visits()\n",
        "\n",
        "    # Fill state visit dictionary\n",
        "    state_visit_dict = self.fill_state_visit_dict(state_visit_counts)\n",
        "\n",
        "    # Assuming state_visit_dict is your dictionary with state visitations\n",
        "    self.plot_state_visitations_heatmap(state_visit_dict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P8buR941l0JC"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test class"
      ],
      "metadata": {
        "id": "P6c1J5GDaf86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.possible_recoveries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dN8X3qzxqG1",
        "outputId": "aec683cc-38f6-446b-e7da-9940b08aef3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 0], [6, 0], [7, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(200, env_50, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e = experiment_actions(1000, env, P_pi_e)\n",
        "model_200_0p99 = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64)\n",
        "test_200_0p99 = SCOPE_straight(model_200_0p99, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "0ueKh2Cfahjf"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps, rewards, states_next, states_current, weights, actions, psi, states_last, psi_last = test_200_0p99.prep_policies(pi_b)"
      ],
      "metadata": {
        "id": "aoDGlkVEB3Tx"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, masked_mean_set, last_set = test_200_0p99.train_var_scope(2, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6SNC1SJOuTQ",
        "outputId": "e62113e0-2f39-4185-92c3-2c2a97092b2b"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(6.6645e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0617, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "SCOPE mean: 0.03283755622152981, SCOPE var: 0.005437360955003497\n",
            "Total Loss: 1.0616657317194855\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(6.6645e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0278, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "SCOPE mean: 0.04225211428680145, SCOPE var: 0.005846893896544516\n",
            "Total Loss: 1.0278019486643444\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.5256, -0.1831],\n",
            "        [ 0.4874, -0.2204],\n",
            "        [ 0.3107,  0.1398],\n",
            "        [ 0.0890, -0.3095],\n",
            "        [ 0.6320,  0.2276],\n",
            "        [-0.2607, -0.6321],\n",
            "        [-0.7033, -0.3404],\n",
            "        [-0.5834, -0.1342],\n",
            "        [ 0.0957, -0.5969],\n",
            "        [-0.4036,  0.4857],\n",
            "        [-0.4621, -0.6912],\n",
            "        [ 0.6774, -0.3109],\n",
            "        [ 0.2194, -0.6755],\n",
            "        [ 0.4790,  0.3212],\n",
            "        [ 0.6565, -0.0203],\n",
            "        [-0.1400, -0.2687]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.5916,  0.5307,  0.6659,  0.1979,  0.4248,  0.1721,  0.3640,  0.2282,\n",
            "         0.4869,  0.0936,  0.2460, -0.6385,  0.6843, -0.5377, -0.2398,  0.6074],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 2.0028e-01, -1.9150e-01, -2.4069e-02,  1.6687e-01,  2.1879e-01,\n",
            "          6.3637e-02,  3.5722e-02, -1.2352e-01, -1.8295e-01, -1.9474e-01,\n",
            "          1.7001e-01,  2.1148e-01, -9.0445e-03,  1.4426e-02, -1.1128e-01,\n",
            "         -5.9484e-02],\n",
            "        [-1.2590e-01, -1.1654e-01,  2.3379e-01,  1.1440e-01, -2.0756e-01,\n",
            "         -2.2469e-01,  3.2124e-02, -1.1786e-01, -1.8041e-01, -3.8652e-02,\n",
            "         -1.8120e-01, -1.6980e-01,  2.4513e-01,  2.0847e-01,  5.5338e-02,\n",
            "          1.1932e-01],\n",
            "        [ 3.1758e-02, -1.3842e-01, -1.3210e-01,  3.3371e-03,  1.8243e-01,\n",
            "         -8.9647e-02,  1.0802e-01,  8.8250e-02, -1.3918e-01,  8.0631e-03,\n",
            "         -2.4271e-02, -2.2108e-01, -1.5716e-01, -3.1119e-02,  1.2950e-02,\n",
            "          6.4989e-02],\n",
            "        [-1.3412e-01,  6.1866e-02,  9.4260e-02,  1.7324e-01,  8.8476e-02,\n",
            "          1.6742e-02,  6.0140e-02,  1.6866e-01, -4.2428e-02, -1.8848e-01,\n",
            "         -2.2014e-01, -1.8238e-01,  5.9408e-02, -2.4772e-01,  1.5292e-02,\n",
            "          1.0134e-02],\n",
            "        [ 2.0735e-01,  1.0819e-01,  2.1480e-01, -2.2879e-01, -1.9573e-01,\n",
            "         -2.4803e-01,  1.3377e-02, -4.9486e-02, -1.0148e-01, -1.5297e-01,\n",
            "          2.0011e-01,  1.5937e-01,  1.5667e-01, -1.8065e-01,  5.4387e-04,\n",
            "         -7.4076e-02],\n",
            "        [ 1.8545e-01, -4.1313e-02, -1.1475e-01,  4.3038e-02,  1.4214e-02,\n",
            "          1.5483e-01,  1.3361e-01,  1.3709e-01, -2.0066e-01,  2.0942e-01,\n",
            "          1.0681e-01,  1.0933e-01,  2.1655e-01,  9.0622e-02, -1.6535e-01,\n",
            "          8.7913e-02],\n",
            "        [-9.4016e-02,  7.5155e-02, -3.9988e-02,  2.2944e-02, -2.2417e-01,\n",
            "          6.1380e-02, -2.0414e-01, -8.1497e-02,  2.1949e-01,  1.3603e-01,\n",
            "          2.4309e-01,  1.5763e-01,  2.4262e-01,  6.3869e-02,  1.9673e-01,\n",
            "         -2.4734e-01],\n",
            "        [ 2.9435e-02,  7.0832e-02, -4.5244e-02,  2.4120e-01,  1.0661e-01,\n",
            "         -1.6126e-01,  1.8163e-01, -1.7269e-01,  1.3298e-01, -7.2866e-02,\n",
            "         -5.8181e-02, -5.0263e-02, -1.2517e-01, -1.0126e-01, -3.6885e-02,\n",
            "          1.2789e-01],\n",
            "        [ 1.1730e-01, -8.5566e-02, -1.6726e-01,  3.8499e-02,  2.3128e-01,\n",
            "         -1.2013e-01,  1.3873e-01,  1.5010e-01,  1.5989e-01, -1.9258e-01,\n",
            "         -1.8497e-01, -1.4312e-01, -1.4654e-01, -1.6513e-01,  2.1496e-01,\n",
            "          1.9387e-01],\n",
            "        [ 9.6692e-02, -2.0030e-01, -9.6532e-02,  1.4838e-01,  1.9529e-01,\n",
            "          1.6996e-01,  1.5586e-01, -1.0916e-01,  2.0352e-01, -7.0839e-02,\n",
            "         -2.4478e-01, -1.9085e-01, -1.8525e-01,  7.3185e-02, -2.4701e-01,\n",
            "          4.4840e-03],\n",
            "        [-6.0516e-02, -2.2240e-01,  2.1518e-01, -1.1809e-03,  2.3576e-01,\n",
            "         -2.0614e-01,  2.1780e-01, -1.8906e-01,  1.3330e-01, -1.9266e-01,\n",
            "          2.2091e-01,  8.0886e-02, -1.9792e-01,  1.4748e-01,  1.7092e-01,\n",
            "         -1.1895e-01],\n",
            "        [ 1.8442e-01, -1.4585e-01,  2.3946e-01, -1.0020e-01,  9.4425e-02,\n",
            "          1.5864e-01,  1.8815e-01,  7.4287e-03,  6.6943e-02, -5.0384e-02,\n",
            "         -1.3912e-04,  1.9527e-01, -7.8683e-03,  1.0256e-01,  2.1907e-01,\n",
            "         -1.9509e-01],\n",
            "        [ 7.2188e-02, -3.6822e-02,  7.0376e-02, -2.4671e-01,  1.7451e-01,\n",
            "         -2.0078e-01, -9.1050e-02,  3.8632e-02, -1.4463e-01, -2.5094e-01,\n",
            "         -1.4911e-02, -8.6221e-02, -7.9463e-02,  1.6998e-01, -1.6597e-02,\n",
            "          1.4905e-02],\n",
            "        [ 1.1007e-01, -4.2570e-02, -2.0597e-01,  5.6226e-02,  5.2142e-02,\n",
            "         -9.3782e-02,  1.5367e-01, -8.0826e-02,  1.4859e-01,  2.1185e-01,\n",
            "         -1.3212e-01,  1.2519e-02, -1.8085e-01, -1.0261e-01,  7.8652e-02,\n",
            "          5.8627e-02],\n",
            "        [-1.6615e-01,  2.3874e-01,  2.4147e-01, -2.2809e-02, -3.0182e-02,\n",
            "          4.9254e-02, -2.2479e-02,  1.1894e-01,  3.1404e-02,  1.4483e-01,\n",
            "          7.3961e-02,  2.4550e-03,  1.2213e-02, -2.1975e-01, -1.0782e-02,\n",
            "          1.6575e-01],\n",
            "        [-9.7037e-02,  1.1045e-01,  1.9928e-01,  6.1813e-02, -3.3081e-02,\n",
            "          7.1479e-02, -1.6773e-01,  2.0375e-01, -3.5086e-02,  9.5235e-02,\n",
            "         -1.7433e-01,  2.3101e-01,  1.4270e-01,  8.7972e-02, -1.5400e-02,\n",
            "         -1.2504e-01]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.1398, -0.0435, -0.0543,  0.0579,  0.2480,  0.2436,  0.1703,  0.0795,\n",
            "         0.0290,  0.1877, -0.2180,  0.0456, -0.0336,  0.0196,  0.0726,  0.2450],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.0218,  0.0405,  0.0153,  0.1337, -0.0724,  0.1623,  0.1891, -0.1632,\n",
            "         -0.2339,  0.1723, -0.1185, -0.0502,  0.0947, -0.2086,  0.0915,  0.0249]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([-0.0745], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_mean_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u8r2x9FQ9s9",
        "outputId": "cc53f841-d2b3-4230-9e94-22ff597728c6"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9.1995,  5.6653,  6.6862,  5.5858, 10.7349, 11.2185,  8.0980, 11.1943,\n",
              "         7.5683,  5.2833, 10.5943,  8.1137,  8.2862,  7.7130,  7.1801,  5.7873,\n",
              "        13.8663,  6.1387,  4.9187,  6.9743,  9.1391, 10.0840,  6.5897,  4.5218,\n",
              "         7.6901,  4.7775, 14.2380,  8.5109,  4.9762,  6.2620,  7.8673,  5.6961,\n",
              "         5.3566,  5.9950,  1.8361,  6.7066, 11.8596,  4.8783, 10.2837,  5.2763,\n",
              "         5.0988,  3.5132,  5.1391,  9.1433,  6.4432,  5.8137,  5.3222,  7.7351,\n",
              "         8.4064,  5.9786,  9.1857,  6.0923,  5.1522,  8.4583,  3.8289,  6.1480,\n",
              "         8.8692,  9.6334, 14.3951,  3.6950], dtype=torch.float64,\n",
              "       grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mean(masked_mean_set+last_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTgLOn_-QgnC",
        "outputId": "b26f3ad3-1537-4efd-b71a-066053141012"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.7470, dtype=torch.float64, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N23fx3xTQWnn",
        "outputId": "2199f0ba-ca9f-49eb-c929-9f72cf7db81f"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.2488e-02, 1.0773e-01, 6.2056e-01, 4.2670e-01, 7.4674e-01, 8.5925e-02,\n",
              "        4.5368e-01, 6.7436e-01, 7.9104e-03, 4.9821e-02, 1.0558e+00, 6.7005e-01,\n",
              "        1.4596e-01, 4.0665e-01, 6.6734e-01, 3.5449e-01, 1.8626e-01, 6.1782e-01,\n",
              "        1.3178e+00, 2.2973e-01, 6.7766e-01, 3.6193e-01, 1.4151e-02, 1.1771e-01,\n",
              "        3.3000e-01, 9.4572e-02, 1.7433e-02, 9.4038e-02, 2.8302e-01, 5.1651e-01,\n",
              "        3.3207e-01, 3.1182e-01, 1.6128e-03, 1.7661e-01, 2.1079e-01, 2.4753e-01,\n",
              "        3.5967e-01, 8.3684e-01, 3.0507e-04, 1.6805e+00, 1.0348e+00, 2.6982e-01,\n",
              "        2.9373e-01, 8.5336e-01, 4.3011e-01, 3.8731e-01, 1.8092e-01, 1.7744e-01,\n",
              "        2.8587e-01, 1.6206e-01, 1.3928e-01, 5.9762e-01, 1.2352e+00, 1.3601e-02,\n",
              "        8.0091e-02, 4.7974e-01, 1.9591e-01, 2.5214e-01, 6.2668e-01, 1.6981e-01],\n",
              "       dtype=torch.float64, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_200_0p99(torch.tensor(states_last)).squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERreTGZMNvjM",
        "outputId": "91e130b5-a1b3-4905-9969-5261c1b9cb1c"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3.5931e-01,  3.1535e-01,  1.1579e-02,  4.1809e-01,  5.3519e-01,\n",
              "         8.3596e-02, -2.7350e-01, -7.7953e-02,  1.6937e-01,  7.5656e-02,\n",
              "        -5.9883e-01,  1.6519e-01,  5.4058e-01,  1.2196e-01,  7.0372e-03,\n",
              "        -1.7420e-02, -3.2525e-01,  3.3452e-01,  2.4492e-01,  3.1173e-01,\n",
              "        -9.0382e-02, -1.4154e-01, -1.5056e-01, -2.8426e-02, -4.4593e-01,\n",
              "        -2.8394e-01,  3.5727e-01, -5.7754e-01, -1.0026e-01, -9.1278e-02,\n",
              "         3.4936e-02,  3.1954e-01, -4.4189e-01, -1.0707e-01,  7.4240e-02,\n",
              "        -2.0949e-01, -1.4405e-01,  1.8865e-01, -2.4926e-01,  8.7673e-03,\n",
              "         2.9567e-02,  2.9281e-01,  1.1581e-01, -7.9609e-04,  2.1255e-01,\n",
              "        -6.5637e-02,  5.3519e-01,  3.3169e-01, -3.1210e-01,  2.9727e-01,\n",
              "         2.4818e-02,  4.5850e-01, -6.3380e-01,  1.9436e-01, -1.9333e-01,\n",
              "         2.6602e-02,  1.5739e-01,  1.6537e-01,  3.7224e-01, -1.5389e-01,\n",
              "         4.4906e-01, -1.0166e-01,  5.0598e-01, -3.7695e-01,  4.6480e-01,\n",
              "         2.2792e-01,  7.8922e-02, -4.3059e-01,  7.1211e-02, -4.5229e-02,\n",
              "         2.7552e-01,  5.3519e-01,  1.9450e-01, -1.4356e-01,  4.0551e-01,\n",
              "         2.4917e-01, -1.4612e-02, -3.8988e-02, -2.1193e-01,  5.3519e-01,\n",
              "         2.0199e-01,  4.1186e-01,  1.4292e-01,  3.0363e-01,  1.7007e-01,\n",
              "         2.5294e-01, -4.5981e-01, -2.6739e-01,  2.4955e-01,  2.0282e-01,\n",
              "        -1.1858e-01,  5.3519e-01,  1.5739e-01,  1.0239e-01, -8.6099e-01,\n",
              "         1.3699e-01,  4.0551e-01,  1.0043e-01,  5.4835e-01, -3.4195e-01,\n",
              "         6.7293e-01,  3.2125e-01, -6.8539e-02, -1.0237e-01, -8.8974e-02,\n",
              "         9.2004e-02,  2.9503e-01, -2.2997e-01,  3.1942e-01,  2.5019e-01,\n",
              "         3.7599e-01, -2.5283e-01, -6.0688e-01,  8.1666e-01,  2.6025e-01,\n",
              "         5.2121e-01, -3.2835e-01,  2.2098e-01, -3.9652e-03,  5.9655e-01,\n",
              "        -1.7180e-02, -5.6058e-01, -4.5479e-02, -6.7444e-01, -1.4953e-01,\n",
              "         2.0352e-01,  2.2393e-01,  5.2509e-01,  1.8153e-01,  2.8436e-01,\n",
              "        -1.1266e-01, -2.1353e-01, -6.9970e-01,  5.4749e-01, -3.8390e-02,\n",
              "         2.2460e-01, -4.4985e-01, -4.6775e-01,  4.4160e-04,  3.6183e-01,\n",
              "        -1.8838e-01,  6.1184e-01, -2.5283e-01,  4.4746e-01,  3.7109e-01,\n",
              "        -3.6335e-01,  1.5228e-01,  5.3519e-01, -8.2850e-02, -6.6999e-01,\n",
              "         3.6431e-01,  6.2073e-01,  6.9929e-02,  2.7198e-01,  6.3369e-01,\n",
              "         3.4418e-01, -2.5603e-01,  3.0231e-01,  1.0483e-01, -3.7420e-01,\n",
              "        -3.9696e-01, -4.6040e-01,  1.8942e-01, -6.2336e-02, -4.7331e-01,\n",
              "        -3.8913e-01,  6.1400e-01,  1.5141e-01,  5.7370e-01,  4.2501e-01,\n",
              "         5.0276e-02,  1.4292e-01,  1.4216e-01,  1.2180e-01, -3.4243e-02,\n",
              "         3.7539e-01,  6.6186e-01,  3.2594e-01,  9.8536e-02, -8.9099e-02,\n",
              "         3.4764e-01, -3.1273e-01,  2.9250e-01, -5.0072e-01, -2.6978e-01,\n",
              "         6.2904e-01, -3.6279e-01, -5.2317e-02,  4.1254e-01, -5.2884e-01,\n",
              "         1.3391e-01, -1.9145e-01,  5.0684e-01,  2.9481e-01, -2.6739e-01,\n",
              "        -2.0109e-01,  2.5747e-01,  1.6373e-01, -7.2330e-02, -2.0585e-02],\n",
              "       dtype=torch.float64, grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "psi_last"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOiInM2RHQGf",
        "outputId": "ac943fcd-2728-42fb-edca-d395e4105368"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0,\n",
              " 5.0]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(states_current[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjzmqAFfIjaf",
        "outputId": "7ae75b3b-a7c3-4dcf-df48-f906367a8b32"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(states_all[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-_WT5cLLasO",
        "outputId": "da07a72c-2dcf-4250-ef60-71df67b2b32c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(psi[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxRs7RyMLeht",
        "outputId": "ab8af6a4-05d2-4541-bf57-7763f6d920b1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-qng8KXLknk",
        "outputId": "53781bb5-548d-4eae-c7ac-70d40de5487e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 8.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [0., 2.],\n",
              "        [0., 1.],\n",
              "        [0., 2.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 9.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 2.],\n",
              "        [0., 2.],\n",
              "        [0., 1.],\n",
              "        [0., 2.],\n",
              "        [0., 2.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [2., 8.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [4., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [4., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [4., 9.],\n",
              "        [3., 9.],\n",
              "        [4., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 7.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 4.],\n",
              "        [4., 4.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [0., 2.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [7., 1.],\n",
              "        [7., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [7., 1.],\n",
              "        [7., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [2., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [2., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [2., 6.],\n",
              "        [3., 6.],\n",
              "        [3., 7.],\n",
              "        [2., 7.],\n",
              "        [2., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [2., 6.],\n",
              "        [3., 6.],\n",
              "        [2., 6.],\n",
              "        [2., 7.],\n",
              "        [2., 6.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [0., 2.],\n",
              "        [0., 1.],\n",
              "        [0., 2.],\n",
              "        [0., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [3., 7.],\n",
              "        [3., 6.],\n",
              "        [2., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [6., 2.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 2.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 2.],\n",
              "        [6., 3.],\n",
              "        [5., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [4., 4.],\n",
              "        [5., 4.],\n",
              "        [4., 4.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [4., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [6., 2.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [4., 2.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 4.],\n",
              "        [3., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [2., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 2.],\n",
              "        [5., 2.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 1.],\n",
              "        [2., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 3.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [3., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [2., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 2.],\n",
              "        [4., 2.],\n",
              "        [4., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 2.],\n",
              "        [6., 2.],\n",
              "        [6., 1.],\n",
              "        [6., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [3., 3.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [2., 9.],\n",
              "        [1., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [1., 7.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [3., 9.],\n",
              "        [3., 8.],\n",
              "        [4., 8.],\n",
              "        [3., 8.],\n",
              "        [2., 8.],\n",
              "        [1., 8.],\n",
              "        [1., 7.],\n",
              "        [1., 7.],\n",
              "        [1., 6.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 4.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]]),\n",
              " array([[2., 9.],\n",
              "        [1., 9.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 8.],\n",
              "        [0., 9.],\n",
              "        [0., 8.],\n",
              "        [0., 7.],\n",
              "        [0., 6.],\n",
              "        [0., 5.],\n",
              "        [0., 4.],\n",
              "        [0., 4.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [1., 2.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [0., 3.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [0., 4.],\n",
              "        [1., 4.],\n",
              "        [1., 3.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [2., 3.],\n",
              "        [2., 2.],\n",
              "        [3., 2.],\n",
              "        [3., 1.],\n",
              "        [4., 1.],\n",
              "        [5., 1.],\n",
              "        [5., 0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states_next[0][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBtz8QT8KiSM",
        "outputId": "fb5eee64-38c8-4440-cb3b-2c3f7255f47a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.vstack((states_current[0],states_next[0][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oZSzB_LKJba",
        "outputId": "3e5e0e8d-32f5-47b3-f91f-eb088ad62738"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 9.],\n",
              "       [2., 8.],\n",
              "       [1., 8.],\n",
              "       [1., 7.],\n",
              "       [0., 7.],\n",
              "       [0., 7.],\n",
              "       [0., 6.],\n",
              "       [0., 5.],\n",
              "       [0., 5.],\n",
              "       [0., 4.],\n",
              "       [1., 4.],\n",
              "       [1., 3.],\n",
              "       [2., 3.],\n",
              "       [2., 3.],\n",
              "       [2., 4.],\n",
              "       [2., 4.],\n",
              "       [2., 4.],\n",
              "       [2., 3.],\n",
              "       [2., 3.],\n",
              "       [2., 2.],\n",
              "       [3., 2.],\n",
              "       [4., 2.],\n",
              "       [4., 1.],\n",
              "       [5., 1.],\n",
              "       [5., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "psi[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBOckqaxIME-",
        "outputId": "d0d2cb34-5bd3-45ad-ce0b-a7504e88bf41"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8.,  8.,  9.,  9., 10., 10., 10., 10., 10., 11., 10., 11., 10.,\n",
              "       10.,  9.,  9.,  9., 10., 10., 11., 10.,  9., 10.,  9.])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(states_next[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmZa67xGh0F",
        "outputId": "64432f6a-a788-4d3f-a09d-84ec3f9a8e09"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS = test_200_0p99.prepare_IS()\n",
        "timestep_bootstraps_IS, rewards_bootstraps_IS, weights_bootstraps_IS, IS_boostraps = test_200_0p99.bootstrap_IS(padded_timestep_tensors_IS, padded_reward_tensors_IS, padded_weight_tensors_IS)\n",
        "test_200_0p99.calc_variance_IS(timestep_bootstraps_IS, rewards_bootstraps_IS, weights_bootstraps_IS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW7zKTJ_DBHR",
        "outputId": "5354153a-87ad-48a2-c418-8435477186b5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1634, dtype=torch.float64), tensor(0.0052, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_0p99.calc_var_IS(IS_boostraps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olV3qblhET_6",
        "outputId": "c84fcd84-b7ca-4b55-b96e-0bfe41d03abd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1634, dtype=torch.float64), tensor(0.0052, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors, padded_psi_tensors, mask_tensor = test_200_0p99.prepare_SCOPE_phi()\n",
        "\n",
        "states_next_output, states_current_output = test_200_0p99.pass_states(padded_states_next_tensors, padded_states_current_tensors)\n",
        "timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps, scope_bootstraps = test_200_0p99.bootstrap_straight(padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output)\n",
        "test_200_0p99.calc_variance_straight(timestep_bootstraps, weights_bootstraps, rewards_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l92kGwjEGKCG",
        "outputId": "1ef58d54-6844-4e77-cf44-036680aa5441"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2590, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0194, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_0p99.calc_var_straight(scope_bootstraps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "zIA5o7oiGmZI",
        "outputId": "49a176cb-302b-407d-d9fd-7ca73b4981f0"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scope_bootstraps' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-c0298a2939f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_200_0p99\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_var_straight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_bootstraps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'scope_bootstraps' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_variance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAxZ9xNCD9ef",
        "outputId": "faaa96ba-a2bd-4728-9933-7c1fa3b14955"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0052, dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QrNhNl4FEN7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_0p99.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "rbQIhOmDerdg",
        "outputId": "994c6891-f557-4de1-fae3-4a110cb60746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"9b404d8c-a962-44ce-ba75-acabb0838fb6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9b404d8c-a962-44ce-ba75-acabb0838fb6\")) {                    Plotly.newPlot(                        \"9b404d8c-a962-44ce-ba75-acabb0838fb6\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,0,12,42,295,344,344,332,254,268,0,1,33,233,266,0,104,214,155,316,0,25,194,238,70,0,33,57,70,365,0,146,192,81,27,0,8,13,25,101,0,176,89,32,7,0,0,1,4,18,82,146,30,9,1,0,0,0,3,4,17,35,6,2,0,0,0,0,2,5,3,4,1,0,0,0,0,0,2,6,0,0,0,0,0,0,0,0,6,5,0,0,0,0,0,0,0,0,2,4],\"zmax\":365,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9b404d8c-a962-44ce-ba75-acabb0838fb6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_0p99.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqq1JfG4oFEw",
        "outputId": "e1c3659f-c459-4944-ec70-565fcfbfa060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0040, dtype=torch.float64), tensor(5.4213e-06, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_e = experiment_actions(1000, env_30, P_pi_e)\n"
      ],
      "metadata": {
        "id": "mqgINIVonyQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5Qjc3OHnlwK",
        "outputId": "06a3cbd9-e6db-4e9b-d1e3-dc0dee39a86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02618155036170724"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_0p99.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvFwsJRZoT4N",
        "outputId": "379cb0f9-40f4-4d32-a0a6-7f0fb96eb008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1136, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6bkJmw-NDCM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, masked_mean_set, last_set = test_200_0p99.train_var_scope(2, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "VlkljTT0j4JH",
        "outputId": "d82d7661-57bb-4f9f-e5d6-c98f4969a803"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SCOPE_straight.pass_states() missing 1 required positional argument: 'states_last_tensor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-8b94e0865e19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_mean_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_200_0p99\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_var_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-0704e7a7b845>\u001b[0m in \u001b[0;36mtrain_var_scope\u001b[0;34m(self, num_epochs, learning_rate, scope_weight, mse_weight)\u001b[0m\n\u001b[1;32m    334\u001b[0m           \u001b[0;31m# timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = self.pass_then_boostraps(padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m           \u001b[0mstates_next_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_current_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_last_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_states_next_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_states_current_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m           \u001b[0;31m# timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = self.bootstrap_straight(padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, states_next_output, states_current_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m           \u001b[0;31m# SCOPE_mean, SCOPE_variance = self.calc_variance_straight(timestep_bootstraps, weights_bootstraps, rewards_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SCOPE_straight.pass_states() missing 1 required positional argument: 'states_last_tensor'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(masked_mean_set, dim = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMRG1lr8L3F5",
        "outputId": "bd1a0fad-a795-4ab8-f3d9-4399b7e2b5ee"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 8.5748,  4.6997,  9.4879,  5.7368,  8.7992, 12.2325,  9.0167,  7.4956,\n",
              "         8.9889,  5.5547,  7.1841,  6.8971, 11.3291,  4.0391, 10.0364,  7.8897,\n",
              "        10.0546,  5.6250,  4.2488,  5.0116,  6.4645,  8.6754,  9.2830,  5.0178,\n",
              "         7.2064,  4.4252,  5.2096,  9.4458,  5.3339, 12.6139, 14.0995,  5.3023,\n",
              "         5.8995,  5.9573,  7.1260,  6.6979, 12.6700,  8.7708, 14.1373,  8.8810,\n",
              "         7.4070,  5.6319,  1.4903,  6.4208,  6.5101,  5.2310,  5.2161, 14.1081,\n",
              "         6.2500,  7.6385,  8.8202,  5.3136,  4.6834,  4.8666,  4.5748,  8.1876,\n",
              "         7.9586,  6.6502,  6.2471,  4.7426], dtype=torch.float64,\n",
              "       grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUKoS77NMVcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_0p99.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "V2sqlHKDE65f",
        "outputId": "72717d56-56de-41a5-b049-33848bc2c434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e2a185e0-65c0-4651-a516-5b47c3065a23\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e2a185e0-65c0-4651-a516-5b47c3065a23\")) {                    Plotly.newPlot(                        \"e2a185e0-65c0-4651-a516-5b47c3065a23\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[1.2573835235247308,1.2121116654714608,1.1587351689698597,1.0890393940416507,1.0143056557679528,0.9313090597505751,0.8617729024957921,0.7990076466821899,0.7863456886466722,0.7935801778973488],[1.2085112588915081,1.1447958576670236,1.0763351059032564,1.0085342461033484,0.9261172825975943,0.8430389301719439,0.7746020564704048,0.741816423590127,0.7446123538621187,0.7474082841341105],[1.1608238213578468,1.0686309357924135,0.999458143835136,0.9209255054446135,0.8378471530189628,0.7588457113284159,0.7119414787474538,0.7139412153216829,0.7159409518959121,0.7179406884701411],[1.102010651711113,0.9940693603348467,0.9217957703562543,0.8329516342763816,0.773211480386571,0.716345506869289,0.7112311541046606,0.7061168013400325,0.7010024485754043,0.696073439815705],[1.043197482064379,0.9195077848772799,0.8423047753818742,0.7539253516512923,0.7228384307397022,0.7335278061688246,0.7325797350048119,0.7274653822401836,0.7238386239661239,0.7252619876969071],[0.9843843124176452,0.8526603800237074,0.7623998416838216,0.6848788916865534,0.6905980953960853,0.7203209153389134,0.7455386055634718,0.7674313287201477,0.7688546924509307,0.770278056181714],[0.9255711427709115,0.792202055974766,0.6757299752566394,0.6422186844752651,0.6598373101034973,0.7010700377790475,0.7316068938838929,0.7819053791481336,0.8138707609357378,0.8152941246665208],[0.8742660305588956,0.7417301144605792,0.615215816155854,0.6190760999009567,0.6499261689755642,0.6841347317017943,0.7209907204988244,0.7679736674685547,0.8182721527327954,0.860310193151328],[0.8430430198758123,0.7016719275101029,0.6162651307470943,0.6132226274070283,0.6484645336032338,0.6826644829368832,0.7191854482513631,0.7560414370483931,0.8043404410532164,0.8546389263174571],[0.8406265836133788,0.7374925532162699,0.6446115994179221,0.6239035536900341,0.6439497755453332,0.6795331901853141,0.7173801760039018,0.7542361648009319,0.791092153597962,0.8407072146378782]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[0.6132226274070283,1.2573835235247308],\"ticktext\":[0.6132226274070283,1.2573835235247308]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e2a185e0-65c0-4651-a516-5b47c3065a23');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test random policy"
      ],
      "metadata": {
        "id": "1enrp4izvB3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(200, env_30, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 5, 0.05)\n",
        "# pi_e = experiment_actions(200, env_30, P_pi_e)\n",
        "model_200_random = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64)\n",
        "test_200_random = SCOPE_straight(model_200_random, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "o1Yatyj_vBQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fjpFSy9G3lEH",
        "outputId": "4526e9aa-acc8-4a51-a708-c567d838a230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b6b91938-caae-430b-8d88-c7dbe4d911de\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b6b91938-caae-430b-8d88-c7dbe4d911de\")) {                    Plotly.newPlot(                        \"b6b91938-caae-430b-8d88-c7dbe4d911de\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,1,10,50,268,338,363,345,239,264,0,4,35,214,223,0,118,213,144,300,0,24,167,233,64,0,57,46,79,391,0,160,177,65,16,0,17,14,32,122,0,194,64,19,8,0,4,1,10,27,66,127,23,5,0,0,1,0,4,17,14,30,15,1,0,0,1,0,3,11,4,11,5,0,0,0,1,0,5,11,1,1,1,0,0,0,1,0,3,18,0,0,1,0,0,0,1,0,3,9],\"zmax\":391,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b6b91938-caae-430b-8d88-c7dbe4d911de');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random.train_var_scope(200, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haUOaqCUwE1Z",
        "outputId": "61573e7c-84a8-4e29-a3d1-43b4ddf79a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005218583187486271\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8199e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.819862798753365e-06\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0250e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.024958128773479e-05\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3286e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.328558541237725e-06\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3393e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.339343710615368e-06\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6211e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.6211430442459475e-06\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2653e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.265335151122525e-06\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0510e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.051047600597096e-06\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8702e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.870209703818152e-06\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7126e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.712563882034527e-06\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5836e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.583572711832976e-06\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4577e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.4576827574062896e-06\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3125e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.3124976815032185e-06\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0066e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.006596834226985e-06\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6917e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.69172072462017e-06\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4814e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.481446272610602e-06\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4082e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.408237363289505e-06\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3609e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.360898500363375e-06\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2893e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.289300722576919e-06\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1854e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.185418847790483e-06\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0697e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.069714783891458e-06\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9426e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.942574680761142e-06\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7829e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.7829417752681105e-06\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5794e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.57935947901364e-06\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3608e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.360810140060177e-06\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1808e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.1807924057078715e-06\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0601e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.060120898765123e-06\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9717e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.9717408236799345e-06\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8730e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.872999221427799e-06\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.7516e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.7516474596638177e-06\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.6306e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.6305590452069293e-06\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5235e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.523523543249152e-06\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4194e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.4194498364047354e-06\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.3129e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.3128939880993014e-06\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2141e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.214062016990148e-06\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1367e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.1367109298641893e-06\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0838e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.083815783841163e-06\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0376e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.0375979569506807e-06\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9805e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9805008954865997e-06\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9141e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.914083240956638e-06\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8499e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.8498630487635488e-06\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7934e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.79340060248865e-06\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7418e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7417869716767264e-06\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6932e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.693229088537973e-06\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6521e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.652083330895247e-06\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6183e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6182684387381757e-06\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5885e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.588513485400126e-06\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5554e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5554372379718102e-06\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5175e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5175046438766148e-06\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4801e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.480053377309589e-06\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4473e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.447324435934277e-06\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4183e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.418310600076487e-06\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3903e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.390265070521835e-06\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3637e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3637086263004655e-06\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3383e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3383400083150835e-06\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3125e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.31250659928688e-06\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2847e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2846957422101712e-06\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2560e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2559596512447755e-06\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2286e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.228577050500867e-06\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2033e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2033493843786976e-06\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1797e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1797144203416527e-06\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1562e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.156154608006684e-06\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1327e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.132736368288874e-06\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1098e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1098159459796553e-06\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0874e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.087387176553559e-06\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0654e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0653765430733716e-06\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0440e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0440016768329383e-06\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0238e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.023774640913568e-06\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0046e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.004644345872273e-06\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9855e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9855444021115104e-06\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9664e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9664473496712084e-06\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9480e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9479648155239455e-06\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9302e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.930246841945334e-06\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9135e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.913469938622366e-06\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8966e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8966349425248998e-06\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8803e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8802689506063122e-06\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8646e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8646384465865746e-06\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8494e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8494220482641309e-06\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8342e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.834248940106725e-06\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8194e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8193945827479823e-06\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8050e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8050239952374909e-06\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7911e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7910790319948859e-06\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7775e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.777472533367116e-06\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7643e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7642696400697195e-06\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7514e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7513736303655654e-06\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7386e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7386393443419527e-06\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7261e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.72605917588156e-06\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7137e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7137111172087957e-06\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7017e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.70168246177967e-06\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6898e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6898050135129278e-06\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6781e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.678111787146935e-06\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6667e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.666665924812708e-06\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6554e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6554311453160312e-06\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6443e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6443288200276103e-06\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6333e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6332934110030025e-06\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6224e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6224489114033852e-06\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6118e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6118267902468544e-06\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6014e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6013985499298854e-06\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5912e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.591187594489545e-06\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5811e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5811012101060652e-06\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5711e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.57113862838106e-06\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5613e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.56128487380117e-06\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5516e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5515787813700968e-06\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5420e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5420405119450996e-06\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5326e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5325912797062999e-06\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5233e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5232583624091237e-06\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5140e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5140374537668416e-06\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5049e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5049193258219863e-06\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4959e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.495938102032153e-06\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4872e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4871653763708906e-06\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4785e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4785098772109553e-06\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4700e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4699533544220917e-06\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4615e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4614845358204992e-06\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4531e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4530988801505674e-06\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4448e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.444794670666121e-06\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4366e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4366200492836897e-06\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4286e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4285575526343965e-06\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4206e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4206169498874845e-06\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4128e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.412758364642678e-06\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4049e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4049271362319458e-06\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3972e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3971676686681605e-06\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3895e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3895229745871703e-06\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3820e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3819691075902093e-06\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3745e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3744673334075072e-06\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3670e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3670360353007032e-06\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3597e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3596744211206814e-06\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3523e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.352348448508024e-06\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3450e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3450428355096257e-06\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3378e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3377695234585822e-06\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3305e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.33053801186768e-06\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3234e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3233702453599088e-06\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3163e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3163022681213861e-06\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3094e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3093599621481132e-06\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3023e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3023491527108688e-06\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2955e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2954738530788846e-06\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2886e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2886319027444451e-06\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2818e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2818478290293494e-06\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2752e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2752080422592504e-06\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2688e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2687634407935566e-06\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2623e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.262301999489526e-06\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2560e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.255998893826394e-06\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2495e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2494783983748676e-06\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2430e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.242969612112399e-06\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2367e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2366863057883822e-06\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2305e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.230498586588437e-06\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2243e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2243098508692354e-06\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2181e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2181108655731147e-06\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2119e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2119121829076057e-06\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2057e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2057381808493915e-06\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1998e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1997800473889394e-06\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1937e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1937131089699019e-06\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1876e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1876458393676022e-06\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1818e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.181773606693398e-06\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1759e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1758905989545526e-06\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1700e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1699926729808505e-06\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1640e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.163994028472344e-06\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1581e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1581407484250805e-06\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1522e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1522375862878014e-06\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1464e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1464352317216299e-06\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1407e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1406573996968808e-06\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1349e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1348688616956476e-06\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1291e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1291225899890357e-06\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1234e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1234246321396223e-06\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1177e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1176959485926347e-06\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1120e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1119844601246873e-06\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1065e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.106545404528081e-06\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1010e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1009598616667592e-06\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0953e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0952879991166997e-06\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0898e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.089796881799858e-06\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0843e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0843320026362492e-06\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0789e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0788984591451425e-06\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0735e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.073479139144111e-06\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0681e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0681016344893633e-06\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0628e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.062755341972131e-06\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0574e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0574296328530208e-06\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0522e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0522455339942676e-06\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0470e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0469772202589901e-06\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0418e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0417855918657226e-06\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0367e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0366911934089256e-06\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0316e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0316145825176385e-06\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0265e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0265281392125857e-06\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0214e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.021445206457885e-06\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0164e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.016406392588562e-06\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0115e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0114971322467864e-06\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0066e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.006569971077742e-06\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0018e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0018493962329524e-06\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9700e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.970021111012322e-07\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9221e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.92208605463252e-07\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8756e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.87559806023695e-07\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8290e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.828956505026449e-07\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7822e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.782213794239675e-07\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7367e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.736746673102684e-07\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6909e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.690934024226122e-07\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6459e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.645924509598353e-07\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6012e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.60123381744426e-07\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5564e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.55641787749128e-07\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5122e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.512165528154774e-07\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4685e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.46854749711329e-07\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4257e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.425665982668925e-07\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(8.9415e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3830e-07, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.382997772972921e-07\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.4160, -0.5639],\n",
            "        [-0.0252,  0.5018],\n",
            "        [-0.2653, -0.4130],\n",
            "        [-0.4997,  0.5405],\n",
            "        [-0.1369,  0.1777],\n",
            "        [-0.5486,  0.5722],\n",
            "        [-0.5864,  0.2929],\n",
            "        [ 0.0592, -0.6384],\n",
            "        [ 0.1675, -0.5224],\n",
            "        [ 0.5592, -0.0803],\n",
            "        [-0.3333,  0.2056],\n",
            "        [-0.1957,  0.1838],\n",
            "        [ 0.1333,  0.1802],\n",
            "        [-0.2681, -0.1993],\n",
            "        [-0.1323, -0.1753],\n",
            "        [ 0.4818,  0.4107]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.5324,  0.6085,  0.4263, -0.1060,  0.0831, -0.1741, -0.2859, -0.2251,\n",
            "        -0.2082, -0.1651,  0.4732,  0.3228,  0.3390,  0.5883, -0.2635, -0.4474],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 0.1070, -0.2702, -0.0483, -0.2136,  0.1762,  0.1867, -0.1716,  0.1543,\n",
            "         -0.0730, -0.2434,  0.0650,  0.1376, -0.1726, -0.0664, -0.0461, -0.0685],\n",
            "        [ 0.2396, -0.1336, -0.0745, -0.0321, -0.0140,  0.1095,  0.1047, -0.1067,\n",
            "         -0.0900,  0.3086, -0.1483, -0.1186, -0.2119,  0.0208,  0.2383, -0.0535],\n",
            "        [-0.2005, -0.1586,  0.0359, -0.2290, -0.2166, -0.0359, -0.2091,  0.0215,\n",
            "         -0.0640,  0.0093, -0.1375,  0.0683,  0.0392,  0.0846, -0.0960, -0.1645],\n",
            "        [ 0.0424, -0.0669,  0.0631, -0.1053, -0.1674, -0.0228,  0.0862, -0.0299,\n",
            "          0.0131, -0.2525, -0.1101,  0.2235, -0.1300,  0.0178, -0.0321,  0.0657],\n",
            "        [-0.0862, -0.1556, -0.0223,  0.1965,  0.2476,  0.0061,  0.2042, -0.0860,\n",
            "          0.0050, -0.0380,  0.1883,  0.1462, -0.0586,  0.1278, -0.2156, -0.0348],\n",
            "        [-0.0610, -0.0633, -0.0139, -0.1922,  0.0268,  0.2602, -0.1128, -0.1555,\n",
            "         -0.0180, -0.2591,  0.1349, -0.1974, -0.2445,  0.0949,  0.1655,  0.0896],\n",
            "        [-0.1709, -0.0613,  0.1730,  0.1006,  0.0804,  0.1342, -0.1666, -0.0862,\n",
            "         -0.1633,  0.0841, -0.2249, -0.2067,  0.0528,  0.1978, -0.0231, -0.1150],\n",
            "        [ 0.0101, -0.1598,  0.0618, -0.1438,  0.1242,  0.1703, -0.1244,  0.1451,\n",
            "         -0.0706, -0.0748, -0.1685, -0.0056, -0.3079,  0.1874, -0.1245, -0.1703],\n",
            "        [ 0.2167, -0.0338,  0.2416, -0.1259,  0.1644,  0.0219,  0.2940, -0.0612,\n",
            "          0.1897,  0.2130, -0.1672, -0.1017, -0.0618,  0.0045, -0.1073, -0.0891],\n",
            "        [ 0.0406,  0.0587,  0.1533,  0.0328, -0.2861, -0.1048,  0.2664, -0.1565,\n",
            "         -0.0930, -0.3120,  0.0886, -0.0790, -0.1761,  0.0170, -0.1685, -0.1635],\n",
            "        [-0.0381,  0.1997, -0.1271, -0.1610, -0.1112,  0.0758,  0.2706, -0.1461,\n",
            "          0.0094,  0.0642, -0.2784, -0.2383,  0.1415, -0.0176,  0.2465,  0.0517],\n",
            "        [-0.0303,  0.1057, -0.1684, -0.1017,  0.1226, -0.0618, -0.1092, -0.0255,\n",
            "         -0.0422, -0.2011, -0.0173, -0.2302,  0.1458, -0.0878, -0.2043,  0.0288],\n",
            "        [ 0.2119,  0.0598, -0.1821, -0.0253, -0.1287, -0.0026, -0.1990,  0.2326,\n",
            "          0.0518, -0.2119,  0.1062, -0.0222,  0.0961,  0.2374,  0.1947,  0.0329],\n",
            "        [-0.1676,  0.0646, -0.1637, -0.1722,  0.1660, -0.0766,  0.1817,  0.2113,\n",
            "         -0.0733, -0.2040,  0.0988, -0.2459, -0.1081, -0.1914,  0.0588, -0.1499],\n",
            "        [ 0.1433,  0.1387,  0.1297,  0.1363, -0.1367, -0.1227, -0.1628, -0.2121,\n",
            "         -0.0241,  0.2838,  0.0140,  0.0870, -0.1988, -0.1736,  0.0209, -0.0470],\n",
            "        [ 0.1765,  0.1835, -0.0616,  0.0656,  0.2218,  0.1011,  0.0832, -0.0714,\n",
            "         -0.2270,  0.1006, -0.0272, -0.0073, -0.2399,  0.0144,  0.1693,  0.1717]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.2081, -0.1037, -0.0968, -0.1189, -0.0342, -0.1509, -0.1145,  0.1007,\n",
            "         0.0441,  0.0807, -0.2352,  0.0309, -0.2310, -0.0054,  0.0056, -0.0388],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.2270,  0.1609, -0.0928, -0.2061, -0.0097, -0.0516,  0.0728,  0.1352,\n",
            "         -0.0922, -0.0372, -0.0109,  0.0101, -0.0600,  0.1787, -0.0322,  0.0047]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.2554], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
              "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "FWrKK2gq1saa",
        "outputId": "b4ffd8e0-55b8-4603-f6aa-af1509382657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2b109429-c4fd-4b6c-9c66-42c9b997c534\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2b109429-c4fd-4b6c-9c66-42c9b997c534\")) {                    Plotly.newPlot(                        \"2b109429-c4fd-4b6c-9c66-42c9b997c534\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[0.23332330352492367,0.24258862148102012,0.23326124374902843,0.22878496516599747,0.23283042956419,0.23671336700501938,0.2405963044458487,0.24447924188667805,0.2483621793275074,0.2522451167683367],[0.25413643178137496,0.2543538585060284,0.24413886937359366,0.23220636506747233,0.23015959799339447,0.23620391182244574,0.2402891258198424,0.2443743398172391,0.24845955381463578,0.2525447678120325],[0.25369140656555744,0.2549150708830124,0.2503195817929705,0.24058524914151114,0.22915152788461285,0.22200536514036334,0.23151309079690852,0.24102081645345372,0.24770807247629067,0.25179328647368737],[0.2535487848738608,0.2553627495507404,0.25164981117617097,0.24607433853601487,0.23715388889797126,0.22609669070175334,0.21520767261430818,0.2233588579438774,0.23286658360042256,0.24237430925696773],[0.2535897042622119,0.2563298737031791,0.2532750516346418,0.2473929340825205,0.24183266378065255,0.2337225286544314,0.22304185351889383,0.2121123596374682,0.21520462509084626,0.22471235074739143],[0.2531524882894386,0.2566668811877895,0.25424189966710403,0.24901817454099137,0.24313605698887003,0.23779751312068914,0.23029116841089153,0.21989192794329337,0.20905752245460868,0.2070503922378151],[0.2527152723166653,0.255055520532074,0.25459055723578977,0.25064341499946224,0.2447612974473409,0.2388791798952196,0.23376236246072576,0.22685980816735168,0.21654106325645045,0.20600268527174923],[0.2517107972563284,0.2539434013785719,0.25493921480447557,0.25050390949868095,0.24638653790581178,0.24050442035369043,0.23482181940783137,0.2297272118007624,0.2234284479238118,0.21319019856960758],[0.2505986781028264,0.2528312822250699,0.25528787237316136,0.2508983942747231,0.24801177836428265,0.2421296608121613,0.23624754326004,0.23085721048438684,0.22573812491142803,0.21999708768027199],[0.2494865589493243,0.25171916307156783,0.25398403144691317,0.25129287905076525,0.24669240342662943,0.24375490127063218,0.23787278371851084,0.2320116871339011,0.2268926015609423,0.2217735159879835]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[0.20600268527174923,0.2566668811877895],\"ticktext\":[0.20600268527174923,0.2566668811877895]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2b109429-c4fd-4b6c-9c66-42c9b997c534');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3d71c3-4913-4be4-c72a-80e2a6c028f6",
        "id": "bMNdZyX0wP74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0130, dtype=torch.float64), tensor(8.9415e-05, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_e = experiment_actions(1000, env_30, P_pi_e)\n"
      ],
      "metadata": {
        "id": "dNqOUUhNwP8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86508de-5c8a-4490-9ee5-80cf50f2ec8f",
        "id": "_BFL3TclwP8L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.12203056843017494"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a06dbe-d37a-40d3-ca90-e867b34f5706",
        "id": "XXIslB1ZwP8M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0521, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test random pi_b"
      ],
      "metadata": {
        "id": "AT8DEpH257IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(200, env_50, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "pi_e = experiment_actions(1000, env_50, P_pi_e)\n",
        "model_200_random_pi_b = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64)\n",
        "test_200_random_pi_b = SCOPE_straight(model_200_random_pi_b, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "WMPIrLYl5-8M"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random_pi_b.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQo5eTV3wy2q",
        "outputId": "046fc409-1017-4d57-b105-7346f1b4560b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2.6041, dtype=torch.float64), tensor(6.1715, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random_pi_b.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Eappd8gl338Y",
        "outputId": "24fe4a9f-fa8e-4cac-b421-ce6f8fb3086c"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"92d136b8-32cf-44d9-9a4e-72fd1b077c5e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"92d136b8-32cf-44d9-9a4e-72fd1b077c5e\")) {                    Plotly.newPlot(                        \"92d136b8-32cf-44d9-9a4e-72fd1b077c5e\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,1,19,74,392,348,301,304,242,250,0,9,73,360,347,0,43,108,103,258,0,52,302,331,78,0,14,19,54,304,0,287,302,48,10,0,5,2,9,30,0,342,92,16,7,0,1,0,0,3,177,253,27,5,1,0,0,0,0,0,19,41,11,0,0,0,0,0,0,0,4,4,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"zmax\":392,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('92d136b8-32cf-44d9-9a4e-72fd1b077c5e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random_pi_b.train_var_scope(300, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbp_-3l56PPN",
        "outputId": "2d06049b-f526-4044-8f4c-a76cfad2aa10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1993, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04945267717328408\n",
            "Total Loss: 0.24875341891612565\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04854473065112503\n",
            "Total Loss: 0.05144933898137734\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04864399770972223\n",
            "Total Loss: 0.05152271293468217\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04866784514199147\n",
            "Total Loss: 0.051536243945871946\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.048615524104309624\n",
            "Total Loss: 0.051456531593814775\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04849324975461501\n",
            "Total Loss: 0.05129381081879754\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.048312368946103\n",
            "Total Loss: 0.051068472854216375\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04808510664483826\n",
            "Total Loss: 0.05080233676055136\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.047823059414614455\n",
            "Total Loss: 0.05051311905677309\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04753419416889371\n",
            "Total Loss: 0.05021146838496162\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04722807551361388\n",
            "Total Loss: 0.049901004960354024\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04691277498134304\n",
            "Total Loss: 0.04958687463320923\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04659304279898141\n",
            "Total Loss: 0.04926749648956022\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.046272942741294266\n",
            "Total Loss: 0.04894278287548763\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04595591695553651\n",
            "Total Loss: 0.04861457584941882\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04564403838554215\n",
            "Total Loss: 0.04828432210083874\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04533760245366354\n",
            "Total Loss: 0.047956796039588356\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04503700286409159\n",
            "Total Loss: 0.04763495530437685\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04474146323908528\n",
            "Total Loss: 0.04732146475834983\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.044450754256739944\n",
            "Total Loss: 0.047021420606675204\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04416397203018826\n",
            "Total Loss: 0.04673580603533978\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04388233225981902\n",
            "Total Loss: 0.04645699445010106\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.043603678756781755\n",
            "Total Loss: 0.046177584960981274\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04332833023998148\n",
            "Total Loss: 0.04589407181185849\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.043055934588074954\n",
            "Total Loss: 0.04560548184441549\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04278634332973145\n",
            "Total Loss: 0.04531169247385323\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04252074241438567\n",
            "Total Loss: 0.045018647089610724\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.042259248346763596\n",
            "Total Loss: 0.04472366945423052\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04200227647433682\n",
            "Total Loss: 0.04443084477144617\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04174682517805569\n",
            "Total Loss: 0.04414081163147765\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04149124233419884\n",
            "Total Loss: 0.0438529593292618\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04123434443318772\n",
            "Total Loss: 0.043565842125013396\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04097669582950763\n",
            "Total Loss: 0.043280626042309506\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.040719604644652116\n",
            "Total Loss: 0.04298870713722543\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0404622600231216\n",
            "Total Loss: 0.04268962867704999\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04020670588951857\n",
            "Total Loss: 0.042389470681551765\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.039953007689818965\n",
            "Total Loss: 0.042090959869297\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0397040310695286\n",
            "Total Loss: 0.04179458260694084\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03945700799764345\n",
            "Total Loss: 0.04149656599511773\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03921001819622812\n",
            "Total Loss: 0.041196694948528616\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.038964229225037325\n",
            "Total Loss: 0.040899319300946424\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03872097989747095\n",
            "Total Loss: 0.04060832670097764\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03848278548872226\n",
            "Total Loss: 0.0403241845095867\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03824822096660062\n",
            "Total Loss: 0.04004731451541948\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03801701581472115\n",
            "Total Loss: 0.03979722262232877\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03778867378836817\n",
            "Total Loss: 0.0395482813830392\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03756496156811893\n",
            "Total Loss: 0.039285037111903565\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03734492444370271\n",
            "Total Loss: 0.039011574138313763\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.037129042224008285\n",
            "Total Loss: 0.038739952850723\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03691556313961234\n",
            "Total Loss: 0.03848513703915277\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.036700684809237716\n",
            "Total Loss: 0.038238687767602725\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.036483439675281816\n",
            "Total Loss: 0.0379941689911375\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.036264010987566216\n",
            "Total Loss: 0.037747021232492864\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03604080241836738\n",
            "Total Loss: 0.03749578083462312\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0358162979321748\n",
            "Total Loss: 0.037246438748284316\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03559218420161251\n",
            "Total Loss: 0.037002976865311295\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.035370390248440416\n",
            "Total Loss: 0.03676658070812876\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.035152061095278464\n",
            "Total Loss: 0.03653376487947622\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03493610572152552\n",
            "Total Loss: 0.03629806893712749\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03472570303094824\n",
            "Total Loss: 0.03606165002877402\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.034519636998314465\n",
            "Total Loss: 0.03582657829037465\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03431632428327262\n",
            "Total Loss: 0.035595242534100416\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.034114294170846465\n",
            "Total Loss: 0.03536911710903119\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03391246894580996\n",
            "Total Loss: 0.035147546204267806\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.033709584183012564\n",
            "Total Loss: 0.03492709409053595\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03350555363665455\n",
            "Total Loss: 0.034706740752203494\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0333009000091261\n",
            "Total Loss: 0.034487743203362134\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03309689775671131\n",
            "Total Loss: 0.034271352164935646\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03289522618742891\n",
            "Total Loss: 0.034062508386576394\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.032698693004125146\n",
            "Total Loss: 0.03386262322128427\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03250933494312577\n",
            "Total Loss: 0.03365961509764954\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03232705621774532\n",
            "Total Loss: 0.03345325069867564\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03214945208228399\n",
            "Total Loss: 0.03325197904279367\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.031969078706416874\n",
            "Total Loss: 0.03305762498034988\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03178739609351097\n",
            "Total Loss: 0.03286448614424302\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03160472176512849\n",
            "Total Loss: 0.03267168734564331\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03142147250828222\n",
            "Total Loss: 0.0324792427332623\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.031239001048223396\n",
            "Total Loss: 0.032290971257610435\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.031058938853701586\n",
            "Total Loss: 0.03210830545150763\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030881709462510655\n",
            "Total Loss: 0.0319247712636638\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030707599043085832\n",
            "Total Loss: 0.031741027063447586\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030535611333163414\n",
            "Total Loss: 0.03156104411393954\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03036501589774746\n",
            "Total Loss: 0.0313828796600467\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030195169304680438\n",
            "Total Loss: 0.031205299707098597\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030025291484208495\n",
            "Total Loss: 0.03102849273117923\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029855973233586783\n",
            "Total Loss: 0.03085360700165191\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029688322326705638\n",
            "Total Loss: 0.030679704498985282\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029522313421967126\n",
            "Total Loss: 0.03050663339229394\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029357520446253317\n",
            "Total Loss: 0.030334439851922655\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029193983492424292\n",
            "Total Loss: 0.030164749258931325\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029031378635397998\n",
            "Total Loss: 0.02999515556860708\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02886991893201097\n",
            "Total Loss: 0.029826190218432688\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028710135310381867\n",
            "Total Loss: 0.029659221510265604\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028551954666903544\n",
            "Total Loss: 0.029492721676264128\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02839502616873332\n",
            "Total Loss: 0.029327462048411038\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028238517286046868\n",
            "Total Loss: 0.029163277022912566\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028082790030764113\n",
            "Total Loss: 0.029000013441074005\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02792759675527582\n",
            "Total Loss: 0.028838349819270664\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.027773428428421246\n",
            "Total Loss: 0.028677382854680983\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.027620516521045278\n",
            "Total Loss: 0.028516518683088465\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.027468542595094975\n",
            "Total Loss: 0.02835628364797853\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.027317185749429176\n",
            "Total Loss: 0.028197331777617837\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02716582609778817\n",
            "Total Loss: 0.028038695597995447\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.027014689126839842\n",
            "Total Loss: 0.0278808720377992\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.026864176878806035\n",
            "Total Loss: 0.02772375878473636\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.026714805646693766\n",
            "Total Loss: 0.027567548305359908\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02656702989021822\n",
            "Total Loss: 0.02741289476529418\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02641708097707591\n",
            "Total Loss: 0.027256217359741132\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.026266769470711302\n",
            "Total Loss: 0.027099692438693864\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02611685139399621\n",
            "Total Loss: 0.02694299815501798\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025967603552981878\n",
            "Total Loss: 0.026786539126156566\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02581879605170001\n",
            "Total Loss: 0.02663085931650037\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02567001183692824\n",
            "Total Loss: 0.026475992590889594\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02552131577460556\n",
            "Total Loss: 0.026321930088510265\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025372187661414906\n",
            "Total Loss: 0.02616811325179297\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025223521625193937\n",
            "Total Loss: 0.02601492293057072\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025075899057256077\n",
            "Total Loss: 0.02586207994085572\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024928946913592157\n",
            "Total Loss: 0.025709095918681948\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024782714154479035\n",
            "Total Loss: 0.025556639953846782\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024636689311154154\n",
            "Total Loss: 0.025403911643382945\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024491107000194842\n",
            "Total Loss: 0.025251771428352433\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024346112744800676\n",
            "Total Loss: 0.025100100330748804\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02420163809422769\n",
            "Total Loss: 0.024949223754352254\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024057708651825906\n",
            "Total Loss: 0.024799328705755918\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02391394973432977\n",
            "Total Loss: 0.024650076159729178\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023770269677040975\n",
            "Total Loss: 0.024501261018107\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023626902489020014\n",
            "Total Loss: 0.024352795677952205\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023483804499146013\n",
            "Total Loss: 0.0242032911478199\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023341077732581553\n",
            "Total Loss: 0.024050943829703762\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02319844588324014\n",
            "Total Loss: 0.023896065895412304\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02305600452927503\n",
            "Total Loss: 0.02373993458877167\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02291385188451716\n",
            "Total Loss: 0.02358737615670228\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02277197953593389\n",
            "Total Loss: 0.023435638765637826\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022630394207658916\n",
            "Total Loss: 0.023284544459237604\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02249015582654771\n",
            "Total Loss: 0.02313502408884195\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02235041349099149\n",
            "Total Loss: 0.02298556398237943\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022211090250914115\n",
            "Total Loss: 0.022836133758181054\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022072247043313453\n",
            "Total Loss: 0.022687144884497197\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021934465294753253\n",
            "Total Loss: 0.022542329399190344\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021798498285674466\n",
            "Total Loss: 0.022397342657655587\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02166191945542206\n",
            "Total Loss: 0.022249508584162202\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021525280209144192\n",
            "Total Loss: 0.02210483112483976\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021389352054586124\n",
            "Total Loss: 0.02196416798089218\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021253974267399778\n",
            "Total Loss: 0.021824601551170386\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021120850286590286\n",
            "Total Loss: 0.02168441186765273\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020987718536063054\n",
            "Total Loss: 0.021546739801023847\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02085311236470807\n",
            "Total Loss: 0.0214078663440855\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020718954044245837\n",
            "Total Loss: 0.021270912276076446\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02058690087342954\n",
            "Total Loss: 0.021136285112275017\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02045775534825959\n",
            "Total Loss: 0.02100216680873213\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020330190137579304\n",
            "Total Loss: 0.0208698705508048\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020197140308575685\n",
            "Total Loss: 0.020731807704300054\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020062241278119154\n",
            "Total Loss: 0.020592682366378467\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019927463018715518\n",
            "Total Loss: 0.020454680438533678\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01979351640854209\n",
            "Total Loss: 0.020316764306595698\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01966054098539313\n",
            "Total Loss: 0.02017931360037044\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019527906512076504\n",
            "Total Loss: 0.020043168719918934\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01939521361598454\n",
            "Total Loss: 0.01990753045288724\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01926288571959303\n",
            "Total Loss: 0.01977316762293595\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01913202838129444\n",
            "Total Loss: 0.01964009825718444\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019003398440630134\n",
            "Total Loss: 0.019508561382744524\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01887637680942546\n",
            "Total Loss: 0.019377273547066823\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01875035549605594\n",
            "Total Loss: 0.019246582093013283\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018624976409901736\n",
            "Total Loss: 0.019117610549579226\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01850059513348684\n",
            "Total Loss: 0.01899008427589519\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018377572233956488\n",
            "Total Loss: 0.01886342090867842\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018255778662891218\n",
            "Total Loss: 0.018737894035363068\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018134558028595347\n",
            "Total Loss: 0.018613257611096258\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018013838385722552\n",
            "Total Loss: 0.01849010738535888\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017894590619172368\n",
            "Total Loss: 0.01836880460856639\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017777315219740197\n",
            "Total Loss: 0.0182494231468843\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01766167315909077\n",
            "Total Loss: 0.018131402495362943\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01754727086810803\n",
            "Total Loss: 0.018014756427061015\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017433496108014982\n",
            "Total Loss: 0.017899247067561644\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017320518588694885\n",
            "Total Loss: 0.017785096111284603\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017205491311864753\n",
            "Total Loss: 0.01766895171409266\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017091667480834207\n",
            "Total Loss: 0.017553115494410836\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016979503662563976\n",
            "Total Loss: 0.017438594826889402\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01686833682788095\n",
            "Total Loss: 0.017325587607140615\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016758430019867376\n",
            "Total Loss: 0.01721392885677529\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016650262899030665\n",
            "Total Loss: 0.017103675650959042\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016543944173240444\n",
            "Total Loss: 0.01699467889074927\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016439138435330233\n",
            "Total Loss: 0.016887173435300516\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016335404788414727\n",
            "Total Loss: 0.01678102386440071\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016233221795650135\n",
            "Total Loss: 0.01667592676662776\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016132539584324906\n",
            "Total Loss: 0.016572200698318242\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016033301006540754\n",
            "Total Loss: 0.016469816708098783\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01593613478448741\n",
            "Total Loss: 0.016369145190262856\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01584039860073178\n",
            "Total Loss: 0.016270365133684878\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015746841265441052\n",
            "Total Loss: 0.01617302013281707\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015654888727292288\n",
            "Total Loss: 0.01607755402412153\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015564484685481517\n",
            "Total Loss: 0.015983646242436977\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01547947675783937\n",
            "Total Loss: 0.015895064351121466\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015396680467479823\n",
            "Total Loss: 0.01580828774850752\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015315644884431984\n",
            "Total Loss: 0.01572310519000634\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015236126383292626\n",
            "Total Loss: 0.015639589161985094\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015158268628996076\n",
            "Total Loss: 0.015557873559152464\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015082857087817791\n",
            "Total Loss: 0.015478224587482147\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015010025379354405\n",
            "Total Loss: 0.015400916794470452\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014938818188047492\n",
            "Total Loss: 0.015325209451458342\n",
            "----------------------------------------\n",
            "Epoch 201\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014868993314741813\n",
            "Total Loss: 0.015251071714328266\n",
            "----------------------------------------\n",
            "Epoch 202\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014799983689822244\n",
            "Total Loss: 0.015178168533072035\n",
            "----------------------------------------\n",
            "Epoch 203\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014731986544678131\n",
            "Total Loss: 0.01510614020718345\n",
            "----------------------------------------\n",
            "Epoch 204\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014664589226793114\n",
            "Total Loss: 0.015035070424161984\n",
            "----------------------------------------\n",
            "Epoch 205\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014598040720822384\n",
            "Total Loss: 0.014964683298632734\n",
            "----------------------------------------\n",
            "Epoch 206\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014533087416442193\n",
            "Total Loss: 0.014896111151875058\n",
            "----------------------------------------\n",
            "Epoch 207\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014469820941312174\n",
            "Total Loss: 0.014829469821920406\n",
            "----------------------------------------\n",
            "Epoch 208\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01440727071806753\n",
            "Total Loss: 0.014763235623713641\n",
            "----------------------------------------\n",
            "Epoch 209\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014345616524046286\n",
            "Total Loss: 0.014699337709401055\n",
            "----------------------------------------\n",
            "Epoch 210\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014284451479959274\n",
            "Total Loss: 0.014635807181736543\n",
            "----------------------------------------\n",
            "Epoch 211\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014223482420036184\n",
            "Total Loss: 0.014573760694954547\n",
            "----------------------------------------\n",
            "Epoch 212\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014163007897253225\n",
            "Total Loss: 0.014511600081783876\n",
            "----------------------------------------\n",
            "Epoch 213\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014102570646818012\n",
            "Total Loss: 0.0144505834774519\n",
            "----------------------------------------\n",
            "Epoch 214\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014043639352624246\n",
            "Total Loss: 0.01439050123052608\n",
            "----------------------------------------\n",
            "Epoch 215\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013985527529671844\n",
            "Total Loss: 0.01433138438839587\n",
            "----------------------------------------\n",
            "Epoch 216\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013927346079011818\n",
            "Total Loss: 0.01427284548483857\n",
            "----------------------------------------\n",
            "Epoch 217\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013870212244171388\n",
            "Total Loss: 0.014215418939479706\n",
            "----------------------------------------\n",
            "Epoch 218\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013814411850707347\n",
            "Total Loss: 0.01415915445062968\n",
            "----------------------------------------\n",
            "Epoch 219\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013759581541837401\n",
            "Total Loss: 0.014103588112831864\n",
            "----------------------------------------\n",
            "Epoch 220\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013705394997101357\n",
            "Total Loss: 0.014049173483573084\n",
            "----------------------------------------\n",
            "Epoch 221\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013651267454540983\n",
            "Total Loss: 0.013995291051233871\n",
            "----------------------------------------\n",
            "Epoch 222\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013597186985154311\n",
            "Total Loss: 0.013942670102243405\n",
            "----------------------------------------\n",
            "Epoch 223\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013544775761818052\n",
            "Total Loss: 0.013890902770545683\n",
            "----------------------------------------\n",
            "Epoch 224\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013493429937544366\n",
            "Total Loss: 0.013840550816181886\n",
            "----------------------------------------\n",
            "Epoch 225\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013442693022176681\n",
            "Total Loss: 0.013790891823237596\n",
            "----------------------------------------\n",
            "Epoch 226\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013392373846478081\n",
            "Total Loss: 0.013741841196605025\n",
            "----------------------------------------\n",
            "Epoch 227\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013342329653764445\n",
            "Total Loss: 0.013693541184358893\n",
            "----------------------------------------\n",
            "Epoch 228\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013292517967985255\n",
            "Total Loss: 0.013645763254456414\n",
            "----------------------------------------\n",
            "Epoch 229\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013243721323154196\n",
            "Total Loss: 0.013598922955954329\n",
            "----------------------------------------\n",
            "Epoch 230\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01319642473907032\n",
            "Total Loss: 0.013553549037279844\n",
            "----------------------------------------\n",
            "Epoch 231\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01314935268567442\n",
            "Total Loss: 0.013509100113060685\n",
            "----------------------------------------\n",
            "Epoch 232\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013102916979143824\n",
            "Total Loss: 0.013464393631526869\n",
            "----------------------------------------\n",
            "Epoch 233\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013057256956307745\n",
            "Total Loss: 0.013419999624719774\n",
            "----------------------------------------\n",
            "Epoch 234\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013012310170395251\n",
            "Total Loss: 0.013377306707058945\n",
            "----------------------------------------\n",
            "Epoch 235\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012968228957065836\n",
            "Total Loss: 0.0133356576246721\n",
            "----------------------------------------\n",
            "Epoch 236\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01292475498779753\n",
            "Total Loss: 0.013294287789280474\n",
            "----------------------------------------\n",
            "Epoch 237\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012881854351393732\n",
            "Total Loss: 0.013253751519725536\n",
            "----------------------------------------\n",
            "Epoch 238\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012842339818246026\n",
            "Total Loss: 0.013215967594694763\n",
            "----------------------------------------\n",
            "Epoch 239\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01280310767988597\n",
            "Total Loss: 0.013178614015054503\n",
            "----------------------------------------\n",
            "Epoch 240\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012764034855767763\n",
            "Total Loss: 0.013141315958140295\n",
            "----------------------------------------\n",
            "Epoch 241\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01272489886064019\n",
            "Total Loss: 0.013104117590237597\n",
            "----------------------------------------\n",
            "Epoch 242\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012685868070710654\n",
            "Total Loss: 0.013068078904315493\n",
            "----------------------------------------\n",
            "Epoch 243\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012647804127420071\n",
            "Total Loss: 0.013032196596014579\n",
            "----------------------------------------\n",
            "Epoch 244\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012610830712544116\n",
            "Total Loss: 0.01299698549106762\n",
            "----------------------------------------\n",
            "Epoch 245\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012575455063957915\n",
            "Total Loss: 0.012961369240760356\n",
            "----------------------------------------\n",
            "Epoch 246\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012540561669039062\n",
            "Total Loss: 0.01292665985295561\n",
            "----------------------------------------\n",
            "Epoch 247\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012505572976652278\n",
            "Total Loss: 0.01289281725044122\n",
            "----------------------------------------\n",
            "Epoch 248\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012470509886089918\n",
            "Total Loss: 0.012859104533168646\n",
            "----------------------------------------\n",
            "Epoch 249\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012435546332462084\n",
            "Total Loss: 0.012825416708455467\n",
            "----------------------------------------\n",
            "Epoch 250\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012400568613000712\n",
            "Total Loss: 0.012792439429185842\n",
            "----------------------------------------\n",
            "Epoch 251\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012366144967897234\n",
            "Total Loss: 0.012760059746030736\n",
            "----------------------------------------\n",
            "Epoch 252\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012332400951338146\n",
            "Total Loss: 0.012727802362968216\n",
            "----------------------------------------\n",
            "Epoch 253\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012299648056258517\n",
            "Total Loss: 0.012694955730906838\n",
            "----------------------------------------\n",
            "Epoch 254\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01226782062704371\n",
            "Total Loss: 0.012662515991584643\n",
            "----------------------------------------\n",
            "Epoch 255\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012237596697114592\n",
            "Total Loss: 0.012630827784575466\n",
            "----------------------------------------\n",
            "Epoch 256\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0122068041367195\n",
            "Total Loss: 0.012599290166787364\n",
            "----------------------------------------\n",
            "Epoch 257\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012175465510140879\n",
            "Total Loss: 0.012567318694442258\n",
            "----------------------------------------\n",
            "Epoch 258\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012144028724291234\n",
            "Total Loss: 0.012535804835000481\n",
            "----------------------------------------\n",
            "Epoch 259\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012112825140652709\n",
            "Total Loss: 0.012504427132176834\n",
            "----------------------------------------\n",
            "Epoch 260\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012083381045631959\n",
            "Total Loss: 0.012473607080062314\n",
            "----------------------------------------\n",
            "Epoch 261\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012054383440307327\n",
            "Total Loss: 0.012443304699772886\n",
            "----------------------------------------\n",
            "Epoch 262\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012025269253708098\n",
            "Total Loss: 0.012413192910216938\n",
            "----------------------------------------\n",
            "Epoch 263\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011997386348012208\n",
            "Total Loss: 0.012383726788645289\n",
            "----------------------------------------\n",
            "Epoch 264\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011970244776785825\n",
            "Total Loss: 0.012354536765766817\n",
            "----------------------------------------\n",
            "Epoch 265\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011942646451179935\n",
            "Total Loss: 0.012325891854777622\n",
            "----------------------------------------\n",
            "Epoch 266\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011916036220296425\n",
            "Total Loss: 0.012297851567228183\n",
            "----------------------------------------\n",
            "Epoch 267\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011888522946316614\n",
            "Total Loss: 0.012269270747108846\n",
            "----------------------------------------\n",
            "Epoch 268\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011860308034248148\n",
            "Total Loss: 0.01224140903268112\n",
            "----------------------------------------\n",
            "Epoch 269\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01183322963115964\n",
            "Total Loss: 0.01221367677483205\n",
            "----------------------------------------\n",
            "Epoch 270\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011806126006786927\n",
            "Total Loss: 0.012186288705446973\n",
            "----------------------------------------\n",
            "Epoch 271\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011779176998902935\n",
            "Total Loss: 0.012158982622858049\n",
            "----------------------------------------\n",
            "Epoch 272\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011751707437816337\n",
            "Total Loss: 0.012131690476960243\n",
            "----------------------------------------\n",
            "Epoch 273\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011724595764374452\n",
            "Total Loss: 0.012104528977475968\n",
            "----------------------------------------\n",
            "Epoch 274\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011698167050723027\n",
            "Total Loss: 0.012077932487465286\n",
            "----------------------------------------\n",
            "Epoch 275\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011672420247403021\n",
            "Total Loss: 0.01205171605878809\n",
            "----------------------------------------\n",
            "Epoch 276\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011646244155497951\n",
            "Total Loss: 0.012025748893562317\n",
            "----------------------------------------\n",
            "Epoch 277\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011621200119995907\n",
            "Total Loss: 0.01199965022403778\n",
            "----------------------------------------\n",
            "Epoch 278\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011596080310801454\n",
            "Total Loss: 0.01197364925522822\n",
            "----------------------------------------\n",
            "Epoch 279\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011570647292675129\n",
            "Total Loss: 0.011946249414019948\n",
            "----------------------------------------\n",
            "Epoch 280\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011544660740362077\n",
            "Total Loss: 0.011919393016219247\n",
            "----------------------------------------\n",
            "Epoch 281\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011517403861943597\n",
            "Total Loss: 0.011892058771155973\n",
            "----------------------------------------\n",
            "Epoch 282\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0114902477278543\n",
            "Total Loss: 0.01186518430516524\n",
            "----------------------------------------\n",
            "Epoch 283\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011464252651922179\n",
            "Total Loss: 0.01183768812445604\n",
            "----------------------------------------\n",
            "Epoch 284\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01143802721270292\n",
            "Total Loss: 0.011810577374388972\n",
            "----------------------------------------\n",
            "Epoch 285\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011413214372842706\n",
            "Total Loss: 0.011783595099677698\n",
            "----------------------------------------\n",
            "Epoch 286\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011388737533849497\n",
            "Total Loss: 0.011756901316123105\n",
            "----------------------------------------\n",
            "Epoch 287\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011363389998337938\n",
            "Total Loss: 0.011730067367567803\n",
            "----------------------------------------\n",
            "Epoch 288\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011337958635767012\n",
            "Total Loss: 0.011703597630423406\n",
            "----------------------------------------\n",
            "Epoch 289\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011313673089174445\n",
            "Total Loss: 0.011676135978752136\n",
            "----------------------------------------\n",
            "Epoch 290\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01128918423020416\n",
            "Total Loss: 0.011649015511352796\n",
            "----------------------------------------\n",
            "Epoch 291\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011266710870765298\n",
            "Total Loss: 0.011621665378479407\n",
            "----------------------------------------\n",
            "Epoch 292\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011243619163872749\n",
            "Total Loss: 0.011594806749663835\n",
            "----------------------------------------\n",
            "Epoch 293\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01121866028075188\n",
            "Total Loss: 0.011567404483257306\n",
            "----------------------------------------\n",
            "Epoch 294\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01119455499374248\n",
            "Total Loss: 0.011540788200252545\n",
            "----------------------------------------\n",
            "Epoch 295\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01117022311793842\n",
            "Total Loss: 0.01151411436049818\n",
            "----------------------------------------\n",
            "Epoch 296\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011145524611575383\n",
            "Total Loss: 0.01148949682616665\n",
            "----------------------------------------\n",
            "Epoch 297\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011124226248315156\n",
            "Total Loss: 0.011463121588096127\n",
            "----------------------------------------\n",
            "Epoch 298\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011101697395952915\n",
            "Total Loss: 0.011437488894749055\n",
            "----------------------------------------\n",
            "Epoch 299\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011077850858885662\n",
            "Total Loss: 0.011412487157661652\n",
            "----------------------------------------\n",
            "Epoch 300\n",
            "IS variance:  tensor(1.6278e-19, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011054548343907997\n",
            "Total Loss: 0.011387159076841876\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.2923,  0.2339],\n",
            "        [ 0.3194,  0.2682],\n",
            "        [-0.2634, -0.0373],\n",
            "        [ 0.5838, -0.0355],\n",
            "        [-0.4373,  0.4338],\n",
            "        [-0.6965, -0.0170],\n",
            "        [-0.7056, -0.1768],\n",
            "        [ 0.4269, -0.5237],\n",
            "        [-0.6634,  0.4686],\n",
            "        [-0.2940,  0.0116],\n",
            "        [-0.0910, -0.0699],\n",
            "        [ 0.4628, -0.5533],\n",
            "        [-0.4880,  0.2012],\n",
            "        [-0.3394, -0.3179],\n",
            "        [-0.0898,  0.2170],\n",
            "        [ 0.2648,  0.0378]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.0980, -0.5624,  0.7502, -0.7612,  0.3475, -0.6895,  0.8333,  0.4590,\n",
            "        -0.4031,  0.7324,  0.7440, -0.1095,  0.3259, -0.4964, -0.1910, -0.2662],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[-7.9441e-02, -2.2863e-02,  3.3402e-01, -1.0591e-01,  1.5772e-01,\n",
            "         -6.6172e-02,  4.3558e-01,  1.7566e-01, -1.8136e-01, -7.8936e-02,\n",
            "         -6.3431e-03,  1.5941e-01,  8.8961e-02, -4.1249e-02, -1.5283e-01,\n",
            "         -2.6457e-02],\n",
            "        [ 1.7969e-01, -1.4301e-01, -2.4433e-01,  2.6367e-01,  2.5167e-02,\n",
            "         -2.1137e-01, -7.4701e-02,  1.5997e-01,  6.5696e-02, -1.4813e-03,\n",
            "         -2.0838e-01,  2.8921e-01,  8.4904e-02,  2.4934e-01, -5.4332e-02,\n",
            "          3.0071e-01],\n",
            "        [-4.7580e-02, -7.6481e-02,  3.4109e-02, -1.5673e-01,  1.0375e-01,\n",
            "          1.9831e-01, -4.1480e-02,  4.8797e-02, -2.8900e-02, -1.9595e-02,\n",
            "          4.0486e-02,  1.1943e-02,  6.6604e-02, -4.2723e-02, -1.4520e-01,\n",
            "          3.2637e-01],\n",
            "        [ 1.2829e-01,  9.3031e-02,  3.6918e-01, -3.4444e-01,  1.6909e-01,\n",
            "          4.9133e-02,  3.2781e-01,  3.7070e-01,  6.0510e-02, -3.5023e-02,\n",
            "         -5.3891e-02,  3.5990e-01,  1.2973e-01, -5.1246e-02,  1.8039e-03,\n",
            "         -1.6265e-01],\n",
            "        [-1.2253e-01, -6.6055e-03, -7.9068e-02, -2.1709e-02, -1.2566e-01,\n",
            "         -7.2884e-02,  7.3626e-01,  2.7695e-01,  1.9757e-01,  3.0867e-01,\n",
            "          3.0060e-01,  4.6456e-01, -2.4232e-01,  1.9947e-01,  1.1047e-01,\n",
            "         -1.1790e-01],\n",
            "        [ 1.6138e-01,  2.5353e-01,  1.5377e-01, -2.8039e-01,  2.8337e-02,\n",
            "          1.2423e-01, -1.1123e-02,  2.1677e-01, -1.3816e-01, -5.0837e-02,\n",
            "          2.4725e-03,  6.4861e-02, -1.2000e-01, -2.3955e-01, -2.2835e-01,\n",
            "         -1.0388e-01],\n",
            "        [ 1.5097e-02, -2.7803e-02,  2.3893e-01, -1.1563e-01, -5.0485e-02,\n",
            "         -2.0249e-02,  2.9245e-01,  6.3526e-02, -9.7638e-02,  1.8421e-03,\n",
            "          1.2326e-01, -2.7872e-02,  4.3301e-03,  1.0163e-03, -1.5171e-04,\n",
            "         -1.7258e-01],\n",
            "        [ 1.4679e-02, -1.2914e-01, -3.0964e-01,  2.7260e-01,  1.2236e-01,\n",
            "         -3.5776e-02, -4.1735e-01, -1.1538e-01, -1.2335e-01, -1.8252e-01,\n",
            "         -1.0157e-01, -2.1411e-01, -1.8586e-01,  4.8755e-02,  1.2369e-01,\n",
            "         -6.1582e-02],\n",
            "        [ 2.5373e-02, -2.1935e-01,  9.7885e-02,  8.9743e-02,  1.0752e-01,\n",
            "         -1.7486e-01, -1.6255e-01, -1.3223e-01, -6.7109e-02, -1.6156e-01,\n",
            "         -2.9660e-01, -2.2071e-01, -2.9908e-01, -2.4921e-01, -1.0130e-01,\n",
            "          1.1813e-02],\n",
            "        [ 1.9539e-01, -2.2595e-02,  2.7461e-01, -2.0555e-02,  4.2501e-02,\n",
            "         -1.5530e-02,  4.0770e-01,  2.2047e-01,  8.4262e-02,  2.6203e-01,\n",
            "          3.1747e-01,  6.8576e-01,  5.4033e-03, -1.3110e-02,  1.6135e-02,\n",
            "         -2.0092e-01],\n",
            "        [-1.4154e-01, -5.8579e-04,  1.3027e-01,  9.2789e-02,  1.3463e-01,\n",
            "          7.6270e-02, -1.9917e-01,  2.7212e-01, -4.7681e-02, -1.0469e-01,\n",
            "         -1.3409e-01,  1.6722e-01,  6.5315e-02,  1.7841e-02,  2.2486e-02,\n",
            "          6.2742e-03],\n",
            "        [-2.0700e-01, -1.4263e-01,  8.4379e-02, -1.6187e-01,  2.4448e-01,\n",
            "         -2.0527e-01,  3.2996e-01,  4.1570e-01, -9.0828e-02,  2.5860e-01,\n",
            "          1.0366e-01,  3.2441e-01,  1.3427e-01, -1.2278e-01, -2.1734e-01,\n",
            "          8.9342e-02],\n",
            "        [ 3.6721e-02,  3.9912e-02,  6.6431e-02, -4.9642e-01,  1.0567e-01,\n",
            "          2.2308e-01,  3.3282e-01, -4.7398e-01,  2.9180e-01,  1.8957e-01,\n",
            "         -9.9124e-02,  3.6361e-01,  3.2616e-01, -4.9789e-02, -1.4960e-01,\n",
            "          1.3086e-01],\n",
            "        [-9.7579e-02,  9.7209e-02,  1.3024e-01, -3.5350e-01, -7.0200e-02,\n",
            "         -1.7615e-01, -2.3083e-01, -4.1906e-02, -1.7637e-01,  2.1140e-02,\n",
            "          1.2557e-01, -2.4515e-01, -3.1865e-01,  1.3735e-01,  6.2464e-02,\n",
            "          1.7466e-01],\n",
            "        [-1.3518e-01,  1.8079e-01, -2.9341e-01,  5.8846e-02, -4.0695e-02,\n",
            "          8.6645e-02, -1.9028e-01,  1.0380e-01, -3.3052e-02, -2.5059e-01,\n",
            "         -2.5654e-01, -2.0229e-01, -8.6180e-02, -6.7891e-02, -5.1838e-02,\n",
            "         -2.9030e-01],\n",
            "        [-9.8133e-02, -2.0684e-02,  5.3039e-02, -1.0932e-01,  4.9404e-03,\n",
            "         -1.8339e-01,  1.9471e-01,  3.4571e-01,  7.5404e-02,  1.5377e-01,\n",
            "          1.8933e-01, -3.6479e-01, -1.3679e-01,  3.8795e-02, -9.4737e-02,\n",
            "         -1.0179e-01]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.2691, -0.3710, -0.1495,  0.2137,  0.2169, -0.1911,  0.5582,  0.0711,\n",
            "         0.0751,  0.3362, -0.3139,  0.4170, -0.0125, -0.0197,  0.0014,  0.4918],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.1809, -0.0669, -0.4371,  0.0653,  0.1017, -0.2506,  0.5259, -0.0835,\n",
            "         -0.1449,  0.0995, -0.1663,  0.1267,  0.0470, -0.1495, -0.2830,  0.2597]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.2566], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
              "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random_pi_b.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "It3Bo2HN6YX6",
        "outputId": "950ca4ac-9d5c-451d-c0c5-3557fdc20bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c1e4d5a6-3632-4c3f-9e25-f04dd68935c9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c1e4d5a6-3632-4c3f-9e25-f04dd68935c9\")) {                    Plotly.newPlot(                        \"c1e4d5a6-3632-4c3f-9e25-f04dd68935c9\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[1.7967267603800279,1.467908576865813,1.3258707471462574,1.1899142791159545,1.1217831616368792,1.053652044157804,0.9855209266787288,0.9183278508621946,0.947467219377844,0.9851509505088547],[1.5809156896321797,1.2179458629662732,1.136065500613331,1.028887396544338,0.9637965548217926,0.8987057130992473,0.833614871376702,0.7858303912803377,0.8201291561756285,0.8595501468819213],[1.4540686034856911,1.0964223157926922,0.9349160078432159,0.8482153289191623,0.7932571831775987,0.7281663414550537,0.6630754997325081,0.642210152110949,0.6814511220732009,0.7239687462723023],[1.3182147856477346,1.0453921270249023,0.821743244501745,0.6469974793981104,0.5788784884756291,0.5248625782694767,0.4700937355935156,0.4896438907555515,0.5345590321286656,0.5794774589719762],[1.1813961025595003,0.9874242153659007,0.7853168423586621,0.570303610003962,0.4084429535902846,0.31816869988011776,0.29077986345077855,0.31446738971643473,0.3572265446450995,0.4132908418071449],[1.0688035060904368,0.929456303706899,0.7372086862823217,0.536661571510497,0.3528746150830465,0.2109542345566836,0.1705939664624107,0.14974939817970323,0.19025128240884573,0.24253076153280412],[1.0163943233216586,0.8714883920478975,0.6863830035210761,0.49138976902850434,0.3263701364062183,0.1851142533348975,0.14801641168813673,0.10501986627121473,0.07609069081110909,0.08715082855163148],[0.9639851405528805,0.813520480388896,0.644368319254772,0.4571537173201099,0.3077850538526008,0.17497913784262317,0.13830142094044998,0.11326083370805004,0.06986953796906675,0.021305209698340627],[0.9062601924509346,0.7555525687298943,0.6023536349884678,0.4332924219673123,0.2935360587140924,0.18117039520899209,0.12710518045546745,0.1011384266635379,0.07609783943113807,0.039956678791043254],[0.8440181481104385,0.6950484383657092,0.5801067803258085,0.4379438315716432,0.3159948502152683,0.20492389786663362,0.1268879630987494,0.0890160196190257,0.0639754323866259,0.03893484515422596]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[0.021305209698340627,1.7967267603800279],\"ticktext\":[0.021305209698340627,1.7967267603800279]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c1e4d5a6-3632-4c3f-9e25-f04dd68935c9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random_pi_b.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36231419-9312-4c1d-aa15-04a63506aa73",
        "id": "sRHfeo2M61vf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-4.8178e-10, dtype=torch.float64),\n",
              " tensor(1.6278e-19, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993fb05b-9179-4330-d630-78ba0de54a48",
        "id": "wfsewrsW61vp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1871443974984857"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_200_random_pi_b.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf46b4f-3bc6-41f8-f1c2-5520cd56718c",
        "id": "A9HyHxak61vq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.4028, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0122, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test random 400 pi_b"
      ],
      "metadata": {
        "id": "fFDrIbdhCntF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(400, env_50, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 1, 0.05)\n",
        "pi_e = experiment_actions(1000, env_50, P_pi_e)\n",
        "model_400_random_pi_b = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64)\n",
        "test_400_random_pi_b = SCOPE_straight(model_400_random_pi_b, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "GrHBDiSzCntP"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_400_random_pi_b.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4879b8f-2271-4ead-a6b4-1d43a0c58ef9",
        "id": "zIT3vLftCntQ"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.7103, dtype=torch.float64), tensor(0.0590, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_400_random_pi_b.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "403b9d5e-643e-4eab-b7e5-270edce11e42",
        "id": "nHuUijSWCntP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"11827d77-a379-4557-98e4-5853b9196352\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"11827d77-a379-4557-98e4-5853b9196352\")) {                    Plotly.newPlot(                        \"11827d77-a379-4557-98e4-5853b9196352\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,0,23,184,436,830,920,663,520,320,0,11,60,362,411,0,850,441,377,332,0,84,203,361,125,0,678,341,405,655,0,150,202,88,23,0,320,173,170,162,0,121,51,20,7,0,89,77,69,43,20,76,9,6,0,0,19,17,9,14,1,11,1,3,0,0,18,17,8,11,1,1,0,0,0,0,19,15,10,21,0,0,0,0,0,0,15,16,11,12,0,0,0,0,0,0,13,8,7,9],\"zmax\":920,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('11827d77-a379-4557-98e4-5853b9196352');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_400_random_pi_b.train_var_scope(500, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e5b9b4-9248-41c7-dfb7-8671d569d6e3",
        "id": "2SaDrZikCntQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2298, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2863851925761847\n",
            "Total Loss: 0.5162089638144078\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5702, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2473059801237863\n",
            "Total Loss: 0.8175554880907143\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5554, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2326082591574042\n",
            "Total Loss: 0.7880451023448775\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5384, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21986669348691343\n",
            "Total Loss: 0.7582925337594453\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5203, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20852381842836246\n",
            "Total Loss: 0.728832287868817\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.19826675288001688\n",
            "Total Loss: 0.7000363003809686\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4834, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.18897165089833828\n",
            "Total Loss: 0.6723549508317651\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4651, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.18055707200899376\n",
            "Total Loss: 0.6456580612285572\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4470, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.172912477742959\n",
            "Total Loss: 0.6199570756052522\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4293, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.1659557436250448\n",
            "Total Loss: 0.5952608125412007\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4118, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.15959548311099986\n",
            "Total Loss: 0.5713970194820261\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3947, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.1537579790146848\n",
            "Total Loss: 0.5485058420300066\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3785, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.14840443604510045\n",
            "Total Loss: 0.5268567739326011\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3625, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.14348833599280547\n",
            "Total Loss: 0.5060349171699419\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3472, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.13895918143484104\n",
            "Total Loss: 0.48613854212261365\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3323, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.13478196591529057\n",
            "Total Loss: 0.467122800321549\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3180, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.1309268905269555\n",
            "Total Loss: 0.44893933800596186\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3043, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.12735200770945743\n",
            "Total Loss: 0.4316061594195954\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2911, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.12403121282357239\n",
            "Total Loss: 0.4151066725447233\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2785, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.1209448456195101\n",
            "Total Loss: 0.3994010276619956\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2664, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.1180631581066447\n",
            "Total Loss: 0.3844554182469116\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2549, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.11537090630803047\n",
            "Total Loss: 0.37025061344569205\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2439, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.11284169480833979\n",
            "Total Loss: 0.35670228404949506\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2331, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.11043526522024422\n",
            "Total Loss: 0.3435595699773738\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2230, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.10814175225102787\n",
            "Total Loss: 0.3311123826201646\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2133, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.1059537333932855\n",
            "Total Loss: 0.31922928899606723\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2040, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.10386661592352302\n",
            "Total Loss: 0.3079035782151253\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1953, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.10187557411230364\n",
            "Total Loss: 0.29713167798483725\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1870, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09997924860198962\n",
            "Total Loss: 0.2869340966810977\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1792, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09817163936327368\n",
            "Total Loss: 0.2773241402174001\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1718, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09645014374147605\n",
            "Total Loss: 0.2682052348710992\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1648, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09481718353025245\n",
            "Total Loss: 0.25964553921256694\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1583, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09326467886582544\n",
            "Total Loss: 0.25152615449274535\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1520, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09178609853516995\n",
            "Total Loss: 0.24382416403809826\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1461, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.09037666413476403\n",
            "Total Loss: 0.2365178165127424\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1405, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08903552593916654\n",
            "Total Loss: 0.2295664935358125\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1352, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08775721072010959\n",
            "Total Loss: 0.22298839008537572\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1302, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08652874656286054\n",
            "Total Loss: 0.21674102285155183\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1255, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0853480508898544\n",
            "Total Loss: 0.21081088176642782\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1210, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08421519431331585\n",
            "Total Loss: 0.20518317454940221\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1167, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08312780448058639\n",
            "Total Loss: 0.19984912785894587\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1127, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08208055872546424\n",
            "Total Loss: 0.19478991971158105\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1089, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.08107028716545518\n",
            "Total Loss: 0.18998263977150298\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1053, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.080090547759785\n",
            "Total Loss: 0.18541184371060163\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0791399507629104\n",
            "Total Loss: 0.18106328302642946\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0987, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07821595475855339\n",
            "Total Loss: 0.1769236263903342\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0957, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07731178571945062\n",
            "Total Loss: 0.1729751572604095\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0928, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07642446995174586\n",
            "Total Loss: 0.16919397053789206\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0900, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07555561159738797\n",
            "Total Loss: 0.16558068951425942\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0874, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07470584834317465\n",
            "Total Loss: 0.1621069287314654\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0849, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07387586081773648\n",
            "Total Loss: 0.15878110341571183\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0825, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07306191069071502\n",
            "Total Loss: 0.15560899498432212\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0803, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07226486847737125\n",
            "Total Loss: 0.15256901270163356\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0782, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.07148802072258266\n",
            "Total Loss: 0.1496397385209185\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0761, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0707269357205651\n",
            "Total Loss: 0.14682552100209711\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0741, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06998026556721548\n",
            "Total Loss: 0.1441198956295167\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0723, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06924920704957288\n",
            "Total Loss: 0.1415177581802436\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0705, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06853085115471098\n",
            "Total Loss: 0.13902656159943427\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0688, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06782726310867607\n",
            "Total Loss: 0.1366391552071028\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0672, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06713849499049344\n",
            "Total Loss: 0.13433653376608662\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0657, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06645620656491633\n",
            "Total Loss: 0.13210677608267948\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0642, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06577686168207701\n",
            "Total Loss: 0.12995154715967458\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0628, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06510804504962012\n",
            "Total Loss: 0.1278646458090396\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0614, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06444432108979715\n",
            "Total Loss: 0.1258392622890399\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0601, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0637901749619609\n",
            "Total Loss: 0.1238824248670695\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0588, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06314147183492513\n",
            "Total Loss: 0.12197779561186442\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0576, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06249759529367658\n",
            "Total Loss: 0.12010817508182835\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0564, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06186130439320088\n",
            "Total Loss: 0.1182825018582988\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0553, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.06122817448727384\n",
            "Total Loss: 0.11649104755779341\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0543, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.060581153599471756\n",
            "Total Loss: 0.11486767775522956\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0533, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05994914109266375\n",
            "Total Loss: 0.11328724655527114\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0524, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05932585185519192\n",
            "Total Loss: 0.11169445158053454\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0514, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05871540913676362\n",
            "Total Loss: 0.11012863488733543\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0505, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.058123131446184556\n",
            "Total Loss: 0.10860626749277749\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0496, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05755371129562669\n",
            "Total Loss: 0.10712076089106196\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0487, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05700377488842894\n",
            "Total Loss: 0.10565426926208238\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0477, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05647144311488744\n",
            "Total Loss: 0.10420329880151763\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0468, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05595499832786017\n",
            "Total Loss: 0.10276857377485152\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0459, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05545321038687514\n",
            "Total Loss: 0.10135239889760742\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0450, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.054958842083758896\n",
            "Total Loss: 0.09998268444682643\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0442, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05446981144215227\n",
            "Total Loss: 0.0986490294573816\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0433, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.053996962383926254\n",
            "Total Loss: 0.09733683865870635\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0425, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05354179941197838\n",
            "Total Loss: 0.09603788560120095\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0417, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05310483651932846\n",
            "Total Loss: 0.09475689406040208\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0408, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05268088712517449\n",
            "Total Loss: 0.0935124525885123\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0400, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05226412037367053\n",
            "Total Loss: 0.0922957830410684\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0392, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.051849733973555016\n",
            "Total Loss: 0.0910940168343146\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0385, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.051434616096853944\n",
            "Total Loss: 0.08990214369082292\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0377, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05102068671390002\n",
            "Total Loss: 0.08872537794511716\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0370, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.050606204757828024\n",
            "Total Loss: 0.08757118122227919\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0362, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.05019149336548305\n",
            "Total Loss: 0.08641552207549899\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0355, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04978479144252726\n",
            "Total Loss: 0.08532445818751327\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0349, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04938738499172196\n",
            "Total Loss: 0.08424760990755979\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0342, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.048993168490777936\n",
            "Total Loss: 0.08316098968076371\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0335, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04860184388712316\n",
            "Total Loss: 0.08212453213152807\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0329, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.048212226997318385\n",
            "Total Loss: 0.08109747170133938\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0323, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04781627247316049\n",
            "Total Loss: 0.08008895191837334\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0317, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.047416440881913074\n",
            "Total Loss: 0.07911662975331128\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0312, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04701422121477403\n",
            "Total Loss: 0.0781793069637758\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0306, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04661533301623461\n",
            "Total Loss: 0.07726476574042299\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0301, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04622126108954041\n",
            "Total Loss: 0.07636753152123993\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0297, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04583561474847478\n",
            "Total Loss: 0.07549574978777773\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0292, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04545756618820542\n",
            "Total Loss: 0.07465727051050285\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0287, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.045086321213661344\n",
            "Total Loss: 0.07382637879054067\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0283, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.044721864890589026\n",
            "Total Loss: 0.07301216116223425\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0278, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04436331086004217\n",
            "Total Loss: 0.07221003984719583\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0274, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.044010885125226086\n",
            "Total Loss: 0.07141735854678588\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0270, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0436638050117803\n",
            "Total Loss: 0.07063088055628043\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0265, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04332099894566407\n",
            "Total Loss: 0.06985055401975412\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0261, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04298279148968409\n",
            "Total Loss: 0.06907933917334455\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0257, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04264972039557376\n",
            "Total Loss: 0.06831729474994916\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0252, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0423200743065059\n",
            "Total Loss: 0.06756778620726046\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0248, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04199304761579122\n",
            "Total Loss: 0.06683170991210652\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0244, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04166940768353809\n",
            "Total Loss: 0.0661165382788514\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0241, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04134871687487795\n",
            "Total Loss: 0.06541474829031751\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0237, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.041030956301435315\n",
            "Total Loss: 0.06472836323328576\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0233, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04071465237314019\n",
            "Total Loss: 0.0640532377906766\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0230, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.040400782448351845\n",
            "Total Loss: 0.06339004778522986\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0226, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.04008957359950369\n",
            "Total Loss: 0.06273878924935052\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0223, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03978120944884346\n",
            "Total Loss: 0.06209851636660791\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0220, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03947562371540651\n",
            "Total Loss: 0.06146950423009423\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0217, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03916941391769405\n",
            "Total Loss: 0.060852392294632615\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0214, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03885823289652614\n",
            "Total Loss: 0.06023372428302032\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0211, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.038547312808513284\n",
            "Total Loss: 0.059620393889821446\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0208, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.038237247546435926\n",
            "Total Loss: 0.05901236746888863\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0205, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.037918326099439945\n",
            "Total Loss: 0.05838921765791244\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0202, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03759409211266411\n",
            "Total Loss: 0.05776964473178225\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0199, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03726653921043609\n",
            "Total Loss: 0.057155240699902875\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0196, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03693775848353213\n",
            "Total Loss: 0.05654828256105143\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0193, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.036609719061350285\n",
            "Total Loss: 0.0559504322409041\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0191, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03628538239664475\n",
            "Total Loss: 0.05536424249240464\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0188, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03596539949892757\n",
            "Total Loss: 0.05478995454605908\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0186, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.035651222136588806\n",
            "Total Loss: 0.05422838604769424\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0183, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03534162583435268\n",
            "Total Loss: 0.05368073599130145\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0181, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0350394341036949\n",
            "Total Loss: 0.053150835342942404\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0179, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.034745576195332785\n",
            "Total Loss: 0.052633455829767375\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0177, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.034461791463839944\n",
            "Total Loss: 0.052119658046063666\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0174, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03418758407922605\n",
            "Total Loss: 0.05160889937299116\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0172, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03392133661682674\n",
            "Total Loss: 0.05110021844858314\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0169, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03366218941534514\n",
            "Total Loss: 0.05059405962751857\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0167, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03340973722530215\n",
            "Total Loss: 0.050091147677094036\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0164, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.033163404966195784\n",
            "Total Loss: 0.049591868010022036\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0162, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03292174844046603\n",
            "Total Loss: 0.049096618352357485\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0159, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03268582933286656\n",
            "Total Loss: 0.04860800506592863\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0157, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03245382026751554\n",
            "Total Loss: 0.0481254814637017\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0154, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03222534530519837\n",
            "Total Loss: 0.047652623155973024\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0152, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03199922925354483\n",
            "Total Loss: 0.04719224015066533\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0150, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03177445588515271\n",
            "Total Loss: 0.04673783759245872\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0147, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03155083270404695\n",
            "Total Loss: 0.046289893334339804\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0145, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0313282299227026\n",
            "Total Loss: 0.045850428493589446\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0143, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03110640546944043\n",
            "Total Loss: 0.04541740577368132\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0141, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03088321333663167\n",
            "Total Loss: 0.044985220422290334\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0139, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.03065862302832907\n",
            "Total Loss: 0.04455626394260144\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0137, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030433208418621616\n",
            "Total Loss: 0.04414986478994152\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0136, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.030210687482944473\n",
            "Total Loss: 0.043770793467949015\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0134, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02999007774468181\n",
            "Total Loss: 0.04339377506609139\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0132, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02976846987065737\n",
            "Total Loss: 0.04300680253761639\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0131, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029548592198163387\n",
            "Total Loss: 0.04262213533720177\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0129, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029331119130934735\n",
            "Total Loss: 0.04223881638041371\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0127, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.029116306424930054\n",
            "Total Loss: 0.04185595824940202\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0126, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028901971605551736\n",
            "Total Loss: 0.04147894866329137\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0124, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02868982056343206\n",
            "Total Loss: 0.04111308192826179\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0123, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0284793797918868\n",
            "Total Loss: 0.04074862037980683\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0121, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028246397194270797\n",
            "Total Loss: 0.0403556631340559\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0120, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.028000332227181783\n",
            "Total Loss: 0.0399540405498715\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0118, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02775190081841106\n",
            "Total Loss: 0.03954394264672406\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0116, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02750338857523835\n",
            "Total Loss: 0.039128519869296205\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0115, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.027256189909828366\n",
            "Total Loss: 0.038720698977747776\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0113, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02701376623002641\n",
            "Total Loss: 0.03832444477958472\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0112, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.026777636137806513\n",
            "Total Loss: 0.0379380704987082\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0110, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.026551932422239057\n",
            "Total Loss: 0.037566801686716375\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0109, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.026338937981374873\n",
            "Total Loss: 0.037211448476734965\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0107, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02613508809887547\n",
            "Total Loss: 0.03685394962487807\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0106, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025935445503062832\n",
            "Total Loss: 0.036501341148016446\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0104, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025741540534538355\n",
            "Total Loss: 0.03615384041812582\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0103, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.025549197392461698\n",
            "Total Loss: 0.035810289592215765\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02536075687147473\n",
            "Total Loss: 0.0354714715332376\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0100, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02517510916804732\n",
            "Total Loss: 0.03513696271030128\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0098, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02499185974168578\n",
            "Total Loss: 0.03480620130261567\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0097, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02481128840848596\n",
            "Total Loss: 0.03448079369920013\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0094, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024638644611883947\n",
            "Total Loss: 0.03406505466183364\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0092, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02447141645364346\n",
            "Total Loss: 0.033630179753628316\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0089, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02431251525562206\n",
            "Total Loss: 0.03321421646050007\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0087, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024160749578489807\n",
            "Total Loss: 0.03282868498561535\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0084, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.024021032865008993\n",
            "Total Loss: 0.032383248832078416\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0080, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023889013179937215\n",
            "Total Loss: 0.03192564071983202\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0077, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02376077123939021\n",
            "Total Loss: 0.03148299415463817\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0074, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023635381021469108\n",
            "Total Loss: 0.031058403778608322\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0071, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023520020419840932\n",
            "Total Loss: 0.030669190377574082\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0069, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023411330604175325\n",
            "Total Loss: 0.030307192316351293\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0067, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02330146727504272\n",
            "Total Loss: 0.029960819957041085\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0064, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023188609559277096\n",
            "Total Loss: 0.029628432556149418\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0062, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.023070867467658917\n",
            "Total Loss: 0.029308330380233973\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0061, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022948385266796337\n",
            "Total Loss: 0.028999960440583148\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0059, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022824392597623876\n",
            "Total Loss: 0.028703984049282438\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0057, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022708502711754477\n",
            "Total Loss: 0.028424450038810913\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0056, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02259092008863513\n",
            "Total Loss: 0.028159131595310423\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0054, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022472672064205054\n",
            "Total Loss: 0.027905947961791217\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0053, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02235281880148933\n",
            "Total Loss: 0.027661092987826125\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0052, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.022232427761918017\n",
            "Total Loss: 0.02742446601716244\n",
            "----------------------------------------\n",
            "Epoch 201\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0051, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02211129011946284\n",
            "Total Loss: 0.027194850942211475\n",
            "----------------------------------------\n",
            "Epoch 202\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0050, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02199012014231942\n",
            "Total Loss: 0.026971854142489326\n",
            "----------------------------------------\n",
            "Epoch 203\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0049, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021869684801606084\n",
            "Total Loss: 0.026755441394524987\n",
            "----------------------------------------\n",
            "Epoch 204\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0048, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021750344698274686\n",
            "Total Loss: 0.026544323915980203\n",
            "----------------------------------------\n",
            "Epoch 205\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0047, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021631162769415263\n",
            "Total Loss: 0.02634035962189059\n",
            "----------------------------------------\n",
            "Epoch 206\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0046, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02151246864251592\n",
            "Total Loss: 0.026141677706511175\n",
            "----------------------------------------\n",
            "Epoch 207\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0046, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021394727163919002\n",
            "Total Loss: 0.025948199215775927\n",
            "----------------------------------------\n",
            "Epoch 208\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0045, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021277752340374826\n",
            "Total Loss: 0.025759329207053486\n",
            "----------------------------------------\n",
            "Epoch 209\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0044, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02116107419681907\n",
            "Total Loss: 0.025573202934883525\n",
            "----------------------------------------\n",
            "Epoch 210\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0043, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.021042982334961\n",
            "Total Loss: 0.025386974657418786\n",
            "----------------------------------------\n",
            "Epoch 211\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0043, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020925696127797457\n",
            "Total Loss: 0.02520493067246923\n",
            "----------------------------------------\n",
            "Epoch 212\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0042, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020809582487823428\n",
            "Total Loss: 0.025024886128787734\n",
            "----------------------------------------\n",
            "Epoch 213\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0042, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020691008267232348\n",
            "Total Loss: 0.024844119436485013\n",
            "----------------------------------------\n",
            "Epoch 214\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0041, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020570985315430286\n",
            "Total Loss: 0.02466397256847261\n",
            "----------------------------------------\n",
            "Epoch 215\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0040, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020451540127642685\n",
            "Total Loss: 0.024487635276464966\n",
            "----------------------------------------\n",
            "Epoch 216\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0040, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.02033307843139012\n",
            "Total Loss: 0.02431583154627709\n",
            "----------------------------------------\n",
            "Epoch 217\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020215729643910905\n",
            "Total Loss: 0.0241483214494183\n",
            "----------------------------------------\n",
            "Epoch 218\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.020100004501516923\n",
            "Total Loss: 0.023985348455425482\n",
            "----------------------------------------\n",
            "Epoch 219\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019985616385614923\n",
            "Total Loss: 0.023826327988190586\n",
            "----------------------------------------\n",
            "Epoch 220\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019872798314217982\n",
            "Total Loss: 0.023670898974847147\n",
            "----------------------------------------\n",
            "Epoch 221\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01976125377123689\n",
            "Total Loss: 0.02351799739467002\n",
            "----------------------------------------\n",
            "Epoch 222\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019650982445226815\n",
            "Total Loss: 0.02336831721092818\n",
            "----------------------------------------\n",
            "Epoch 223\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01954187643996658\n",
            "Total Loss: 0.023221391389795985\n",
            "----------------------------------------\n",
            "Epoch 224\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01943229483376504\n",
            "Total Loss: 0.023081684451973963\n",
            "----------------------------------------\n",
            "Epoch 225\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019323349854674585\n",
            "Total Loss: 0.022948377521483893\n",
            "----------------------------------------\n",
            "Epoch 226\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01921683860571461\n",
            "Total Loss: 0.022816476809898997\n",
            "----------------------------------------\n",
            "Epoch 227\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01911281181758891\n",
            "Total Loss: 0.022686529898613493\n",
            "----------------------------------------\n",
            "Epoch 228\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.019010925119410934\n",
            "Total Loss: 0.022558728312446933\n",
            "----------------------------------------\n",
            "Epoch 229\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01891098721590745\n",
            "Total Loss: 0.022433168240175917\n",
            "----------------------------------------\n",
            "Epoch 230\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018812350427061454\n",
            "Total Loss: 0.022309508133703134\n",
            "----------------------------------------\n",
            "Epoch 231\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018715307538834738\n",
            "Total Loss: 0.022188241219401057\n",
            "----------------------------------------\n",
            "Epoch 232\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01862004928379276\n",
            "Total Loss: 0.02206952554133193\n",
            "----------------------------------------\n",
            "Epoch 233\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018526212022095073\n",
            "Total Loss: 0.021952905474883253\n",
            "----------------------------------------\n",
            "Epoch 234\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018434292176902494\n",
            "Total Loss: 0.021838831728692755\n",
            "----------------------------------------\n",
            "Epoch 235\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01834394251505628\n",
            "Total Loss: 0.021726648994204865\n",
            "----------------------------------------\n",
            "Epoch 236\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018255050329945025\n",
            "Total Loss: 0.021616013593771292\n",
            "----------------------------------------\n",
            "Epoch 237\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018167417288009347\n",
            "Total Loss: 0.021506619579220933\n",
            "----------------------------------------\n",
            "Epoch 238\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.018081318959893528\n",
            "Total Loss: 0.021398521696258735\n",
            "----------------------------------------\n",
            "Epoch 239\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017996743477036968\n",
            "Total Loss: 0.02129156794264606\n",
            "----------------------------------------\n",
            "Epoch 240\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01791372680141476\n",
            "Total Loss: 0.021185768815610483\n",
            "----------------------------------------\n",
            "Epoch 241\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017832281697720335\n",
            "Total Loss: 0.021080920101096513\n",
            "----------------------------------------\n",
            "Epoch 242\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017752171095816012\n",
            "Total Loss: 0.02097698523037168\n",
            "----------------------------------------\n",
            "Epoch 243\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017673721118550922\n",
            "Total Loss: 0.02087443723578258\n",
            "----------------------------------------\n",
            "Epoch 244\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01759704452960472\n",
            "Total Loss: 0.02077341397348414\n",
            "----------------------------------------\n",
            "Epoch 245\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017522119164340117\n",
            "Total Loss: 0.020673926013915687\n",
            "----------------------------------------\n",
            "Epoch 246\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017448777184719502\n",
            "Total Loss: 0.020575797302708067\n",
            "----------------------------------------\n",
            "Epoch 247\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017377057901143328\n",
            "Total Loss: 0.020479112717426515\n",
            "----------------------------------------\n",
            "Epoch 248\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017306190898558726\n",
            "Total Loss: 0.020383177321953036\n",
            "----------------------------------------\n",
            "Epoch 249\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017236274484922223\n",
            "Total Loss: 0.02028823716666425\n",
            "----------------------------------------\n",
            "Epoch 250\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017167401839215585\n",
            "Total Loss: 0.020194429722343178\n",
            "----------------------------------------\n",
            "Epoch 251\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017099485805858395\n",
            "Total Loss: 0.020101724903607874\n",
            "----------------------------------------\n",
            "Epoch 252\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.017032455213715107\n",
            "Total Loss: 0.02001011446275016\n",
            "----------------------------------------\n",
            "Epoch 253\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01696627215797274\n",
            "Total Loss: 0.019919527940584224\n",
            "----------------------------------------\n",
            "Epoch 254\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01690082954113198\n",
            "Total Loss: 0.019829903315190415\n",
            "----------------------------------------\n",
            "Epoch 255\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016836366001168224\n",
            "Total Loss: 0.019741524418353428\n",
            "----------------------------------------\n",
            "Epoch 256\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016772255732756446\n",
            "Total Loss: 0.01965379148519754\n",
            "----------------------------------------\n",
            "Epoch 257\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016708695477231316\n",
            "Total Loss: 0.019566954200786135\n",
            "----------------------------------------\n",
            "Epoch 258\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016645706620239863\n",
            "Total Loss: 0.019481038658190602\n",
            "----------------------------------------\n",
            "Epoch 259\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016583374833206967\n",
            "Total Loss: 0.01939612907004961\n",
            "----------------------------------------\n",
            "Epoch 260\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016521796431098662\n",
            "Total Loss: 0.01931230651872068\n",
            "----------------------------------------\n",
            "Epoch 261\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016461130381680247\n",
            "Total Loss: 0.019229741813045705\n",
            "----------------------------------------\n",
            "Epoch 262\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016401135060559124\n",
            "Total Loss: 0.01914818643424\n",
            "----------------------------------------\n",
            "Epoch 263\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016341760841607687\n",
            "Total Loss: 0.019067560726223273\n",
            "----------------------------------------\n",
            "Epoch 264\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016283014317184298\n",
            "Total Loss: 0.018987841952847955\n",
            "----------------------------------------\n",
            "Epoch 265\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016224826380025575\n",
            "Total Loss: 0.018908992199654764\n",
            "----------------------------------------\n",
            "Epoch 266\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016167720185187688\n",
            "Total Loss: 0.018831534620099465\n",
            "----------------------------------------\n",
            "Epoch 267\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016111286902431726\n",
            "Total Loss: 0.01875505407077086\n",
            "----------------------------------------\n",
            "Epoch 268\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.016055363111735724\n",
            "Total Loss: 0.018679395313124507\n",
            "----------------------------------------\n",
            "Epoch 269\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015999989030502787\n",
            "Total Loss: 0.018604608221498682\n",
            "----------------------------------------\n",
            "Epoch 270\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01594512559027011\n",
            "Total Loss: 0.018530639410681216\n",
            "----------------------------------------\n",
            "Epoch 271\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015890747858740045\n",
            "Total Loss: 0.018457455871004082\n",
            "----------------------------------------\n",
            "Epoch 272\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015836912036625565\n",
            "Total Loss: 0.018385103504420443\n",
            "----------------------------------------\n",
            "Epoch 273\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01578315666840273\n",
            "Total Loss: 0.018312859939753783\n",
            "----------------------------------------\n",
            "Epoch 274\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015725437868793626\n",
            "Total Loss: 0.018236064859166196\n",
            "----------------------------------------\n",
            "Epoch 275\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01566640267131052\n",
            "Total Loss: 0.018158655454407355\n",
            "----------------------------------------\n",
            "Epoch 276\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015607074785664128\n",
            "Total Loss: 0.018081734161530475\n",
            "----------------------------------------\n",
            "Epoch 277\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015547621893602977\n",
            "Total Loss: 0.018005530243443774\n",
            "----------------------------------------\n",
            "Epoch 278\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015487230990531472\n",
            "Total Loss: 0.0179292081445324\n",
            "----------------------------------------\n",
            "Epoch 279\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015427287658222091\n",
            "Total Loss: 0.017853792321986434\n",
            "----------------------------------------\n",
            "Epoch 280\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01536760420391518\n",
            "Total Loss: 0.017778809672208157\n",
            "----------------------------------------\n",
            "Epoch 281\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015308459099116958\n",
            "Total Loss: 0.017704404121113328\n",
            "----------------------------------------\n",
            "Epoch 282\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015250276115236725\n",
            "Total Loss: 0.01763059339209347\n",
            "----------------------------------------\n",
            "Epoch 283\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015193321690135502\n",
            "Total Loss: 0.01755737640572743\n",
            "----------------------------------------\n",
            "Epoch 284\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015136743200361411\n",
            "Total Loss: 0.017483938767576812\n",
            "----------------------------------------\n",
            "Epoch 285\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.015080667611956557\n",
            "Total Loss: 0.017410840560669733\n",
            "----------------------------------------\n",
            "Epoch 286\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01502552949640736\n",
            "Total Loss: 0.01733853166608225\n",
            "----------------------------------------\n",
            "Epoch 287\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014971214468203955\n",
            "Total Loss: 0.01726711692459553\n",
            "----------------------------------------\n",
            "Epoch 288\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014917728885808773\n",
            "Total Loss: 0.0171967384061476\n",
            "----------------------------------------\n",
            "Epoch 289\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014864974895953615\n",
            "Total Loss: 0.017127401148070084\n",
            "----------------------------------------\n",
            "Epoch 290\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014812929405840012\n",
            "Total Loss: 0.017059178342348366\n",
            "----------------------------------------\n",
            "Epoch 291\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014761553891238871\n",
            "Total Loss: 0.01699205750239327\n",
            "----------------------------------------\n",
            "Epoch 292\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014710933029798251\n",
            "Total Loss: 0.01692610732483349\n",
            "----------------------------------------\n",
            "Epoch 293\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014660163468710645\n",
            "Total Loss: 0.01686054990706287\n",
            "----------------------------------------\n",
            "Epoch 294\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014609713579640566\n",
            "Total Loss: 0.016795836263385256\n",
            "----------------------------------------\n",
            "Epoch 295\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014559256523479194\n",
            "Total Loss: 0.016731499736605406\n",
            "----------------------------------------\n",
            "Epoch 296\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01450925147893104\n",
            "Total Loss: 0.01666812288545145\n",
            "----------------------------------------\n",
            "Epoch 297\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014459986479688214\n",
            "Total Loss: 0.016606034968520058\n",
            "----------------------------------------\n",
            "Epoch 298\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014411677021751501\n",
            "Total Loss: 0.016545120339537728\n",
            "----------------------------------------\n",
            "Epoch 299\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0143643255862036\n",
            "Total Loss: 0.016485328336790693\n",
            "----------------------------------------\n",
            "Epoch 300\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014317984582538542\n",
            "Total Loss: 0.01642662337782829\n",
            "----------------------------------------\n",
            "Epoch 301\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014272776528205891\n",
            "Total Loss: 0.016370411264957106\n",
            "----------------------------------------\n",
            "Epoch 302\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014228596171495455\n",
            "Total Loss: 0.016315884701988803\n",
            "----------------------------------------\n",
            "Epoch 303\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014185536854380445\n",
            "Total Loss: 0.016262224200974455\n",
            "----------------------------------------\n",
            "Epoch 304\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014143389364598533\n",
            "Total Loss: 0.016209156617097088\n",
            "----------------------------------------\n",
            "Epoch 305\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014100839192715632\n",
            "Total Loss: 0.016155559837564754\n",
            "----------------------------------------\n",
            "Epoch 306\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01405907939592662\n",
            "Total Loss: 0.016102600612398533\n",
            "----------------------------------------\n",
            "Epoch 307\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.014017389033422386\n",
            "Total Loss: 0.016049658403010855\n",
            "----------------------------------------\n",
            "Epoch 308\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013976508974222356\n",
            "Total Loss: 0.01599749638750267\n",
            "----------------------------------------\n",
            "Epoch 309\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013936488507242755\n",
            "Total Loss: 0.01594607210913511\n",
            "----------------------------------------\n",
            "Epoch 310\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01389706182312656\n",
            "Total Loss: 0.015895113603407503\n",
            "----------------------------------------\n",
            "Epoch 311\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01385820297259963\n",
            "Total Loss: 0.01584461875686296\n",
            "----------------------------------------\n",
            "Epoch 312\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013820042583574622\n",
            "Total Loss: 0.01579418224299863\n",
            "----------------------------------------\n",
            "Epoch 313\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013781868580235716\n",
            "Total Loss: 0.015739082978295422\n",
            "----------------------------------------\n",
            "Epoch 314\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013743714714847224\n",
            "Total Loss: 0.015684889494911775\n",
            "----------------------------------------\n",
            "Epoch 315\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013705960600942384\n",
            "Total Loss: 0.01563218925322161\n",
            "----------------------------------------\n",
            "Epoch 316\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01366906866846554\n",
            "Total Loss: 0.015580359476912343\n",
            "----------------------------------------\n",
            "Epoch 317\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013632987925214973\n",
            "Total Loss: 0.01552928409351132\n",
            "----------------------------------------\n",
            "Epoch 318\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013597654759257014\n",
            "Total Loss: 0.015479156169220137\n",
            "----------------------------------------\n",
            "Epoch 319\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013563324461823368\n",
            "Total Loss: 0.015430011751761962\n",
            "----------------------------------------\n",
            "Epoch 320\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013530031176526399\n",
            "Total Loss: 0.01538179754202133\n",
            "----------------------------------------\n",
            "Epoch 321\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013497412321587552\n",
            "Total Loss: 0.015334562081527446\n",
            "----------------------------------------\n",
            "Epoch 322\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01346595744666856\n",
            "Total Loss: 0.015288130532821385\n",
            "----------------------------------------\n",
            "Epoch 323\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01343580864564062\n",
            "Total Loss: 0.015241448276479485\n",
            "----------------------------------------\n",
            "Epoch 324\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013406743333551397\n",
            "Total Loss: 0.015194673748547691\n",
            "----------------------------------------\n",
            "Epoch 325\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013378758765062566\n",
            "Total Loss: 0.015148105787285725\n",
            "----------------------------------------\n",
            "Epoch 326\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0133518117388026\n",
            "Total Loss: 0.015102099853344174\n",
            "----------------------------------------\n",
            "Epoch 327\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013325508580353187\n",
            "Total Loss: 0.015056131538669814\n",
            "----------------------------------------\n",
            "Epoch 328\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01329968445278198\n",
            "Total Loss: 0.015011093640480384\n",
            "----------------------------------------\n",
            "Epoch 329\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013273810293042112\n",
            "Total Loss: 0.014966616393705692\n",
            "----------------------------------------\n",
            "Epoch 330\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013247751619926942\n",
            "Total Loss: 0.014922395814098259\n",
            "----------------------------------------\n",
            "Epoch 331\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013221268192515628\n",
            "Total Loss: 0.014878617462769373\n",
            "----------------------------------------\n",
            "Epoch 332\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013194779167202672\n",
            "Total Loss: 0.014835877322943761\n",
            "----------------------------------------\n",
            "Epoch 333\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013167868183299267\n",
            "Total Loss: 0.014793619511045984\n",
            "----------------------------------------\n",
            "Epoch 334\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013140468506047482\n",
            "Total Loss: 0.014751970568572075\n",
            "----------------------------------------\n",
            "Epoch 335\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013112603282342393\n",
            "Total Loss: 0.014710532515684238\n",
            "----------------------------------------\n",
            "Epoch 336\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013084253671228781\n",
            "Total Loss: 0.014669345544573591\n",
            "----------------------------------------\n",
            "Epoch 337\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013055522334569108\n",
            "Total Loss: 0.014628493703008791\n",
            "----------------------------------------\n",
            "Epoch 338\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.013026503334947783\n",
            "Total Loss: 0.014587882639689612\n",
            "----------------------------------------\n",
            "Epoch 339\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012997175958339083\n",
            "Total Loss: 0.014547378390913312\n",
            "----------------------------------------\n",
            "Epoch 340\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012967607906276718\n",
            "Total Loss: 0.014507032976902735\n",
            "----------------------------------------\n",
            "Epoch 341\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012937754531966269\n",
            "Total Loss: 0.014466788413198872\n",
            "----------------------------------------\n",
            "Epoch 342\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012907709649465098\n",
            "Total Loss: 0.014426706490029749\n",
            "----------------------------------------\n",
            "Epoch 343\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012877498648194588\n",
            "Total Loss: 0.014386827992462384\n",
            "----------------------------------------\n",
            "Epoch 344\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01284697222611054\n",
            "Total Loss: 0.01434716006240137\n",
            "----------------------------------------\n",
            "Epoch 345\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012816296864018951\n",
            "Total Loss: 0.01430781992263811\n",
            "----------------------------------------\n",
            "Epoch 346\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012785455044130001\n",
            "Total Loss: 0.014268769255434797\n",
            "----------------------------------------\n",
            "Epoch 347\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01275466937448788\n",
            "Total Loss: 0.014230427835662392\n",
            "----------------------------------------\n",
            "Epoch 348\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01272386057427073\n",
            "Total Loss: 0.014192282285628627\n",
            "----------------------------------------\n",
            "Epoch 349\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012693075072098577\n",
            "Total Loss: 0.014154317742226082\n",
            "----------------------------------------\n",
            "Epoch 350\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012662313956739056\n",
            "Total Loss: 0.014116758081170171\n",
            "----------------------------------------\n",
            "Epoch 351\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012631611975250211\n",
            "Total Loss: 0.014079435005175242\n",
            "----------------------------------------\n",
            "Epoch 352\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012600996539004447\n",
            "Total Loss: 0.0140422829304435\n",
            "----------------------------------------\n",
            "Epoch 353\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012570510978598298\n",
            "Total Loss: 0.014005361567561428\n",
            "----------------------------------------\n",
            "Epoch 354\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012540485268450606\n",
            "Total Loss: 0.013968855096648714\n",
            "----------------------------------------\n",
            "Epoch 355\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012510684582464305\n",
            "Total Loss: 0.013932627564666256\n",
            "----------------------------------------\n",
            "Epoch 356\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012481272613844751\n",
            "Total Loss: 0.013896590634273719\n",
            "----------------------------------------\n",
            "Epoch 357\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012452395732561817\n",
            "Total Loss: 0.013860597202473299\n",
            "----------------------------------------\n",
            "Epoch 358\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012423903230786544\n",
            "Total Loss: 0.01382440852589769\n",
            "----------------------------------------\n",
            "Epoch 359\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012395723483219545\n",
            "Total Loss: 0.013788363875771065\n",
            "----------------------------------------\n",
            "Epoch 360\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012367738424458104\n",
            "Total Loss: 0.013752669655476195\n",
            "----------------------------------------\n",
            "Epoch 361\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01233999094775701\n",
            "Total Loss: 0.01371722188191847\n",
            "----------------------------------------\n",
            "Epoch 362\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012312437957550833\n",
            "Total Loss: 0.013682006135360596\n",
            "----------------------------------------\n",
            "Epoch 363\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01228530539697904\n",
            "Total Loss: 0.013646954179935845\n",
            "----------------------------------------\n",
            "Epoch 364\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012258405203217098\n",
            "Total Loss: 0.013612337742825891\n",
            "----------------------------------------\n",
            "Epoch 365\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012231501427255298\n",
            "Total Loss: 0.013577918342146741\n",
            "----------------------------------------\n",
            "Epoch 366\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012204582195591538\n",
            "Total Loss: 0.013543676467779176\n",
            "----------------------------------------\n",
            "Epoch 367\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01217757447942272\n",
            "Total Loss: 0.013509572277656642\n",
            "----------------------------------------\n",
            "Epoch 368\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012150531492183718\n",
            "Total Loss: 0.013475625029118653\n",
            "----------------------------------------\n",
            "Epoch 369\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012123460987987581\n",
            "Total Loss: 0.013441829698589609\n",
            "----------------------------------------\n",
            "Epoch 370\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012096275273668994\n",
            "Total Loss: 0.013408126470013807\n",
            "----------------------------------------\n",
            "Epoch 371\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012068974675151242\n",
            "Total Loss: 0.013374532686840596\n",
            "----------------------------------------\n",
            "Epoch 372\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012041606225228594\n",
            "Total Loss: 0.013341144832047784\n",
            "----------------------------------------\n",
            "Epoch 373\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.012014199889085026\n",
            "Total Loss: 0.01330799977379832\n",
            "----------------------------------------\n",
            "Epoch 374\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011986743504416577\n",
            "Total Loss: 0.013275016698342045\n",
            "----------------------------------------\n",
            "Epoch 375\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011959285422162872\n",
            "Total Loss: 0.013242194373619058\n",
            "----------------------------------------\n",
            "Epoch 376\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011931816283271566\n",
            "Total Loss: 0.01320952869878826\n",
            "----------------------------------------\n",
            "Epoch 377\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011904662252475747\n",
            "Total Loss: 0.013177008843366863\n",
            "----------------------------------------\n",
            "Epoch 378\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011877940898688719\n",
            "Total Loss: 0.013144846448690132\n",
            "----------------------------------------\n",
            "Epoch 379\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011851199334472236\n",
            "Total Loss: 0.013112962907155424\n",
            "----------------------------------------\n",
            "Epoch 380\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011823986492561273\n",
            "Total Loss: 0.013081691957077493\n",
            "----------------------------------------\n",
            "Epoch 381\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011796846657892419\n",
            "Total Loss: 0.013051013084845017\n",
            "----------------------------------------\n",
            "Epoch 382\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011769856452405976\n",
            "Total Loss: 0.013020551621091161\n",
            "----------------------------------------\n",
            "Epoch 383\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011743056221764083\n",
            "Total Loss: 0.01299026641784676\n",
            "----------------------------------------\n",
            "Epoch 384\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011716410965408361\n",
            "Total Loss: 0.012959242942834692\n",
            "----------------------------------------\n",
            "Epoch 385\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01169017333765334\n",
            "Total Loss: 0.01292775124462878\n",
            "----------------------------------------\n",
            "Epoch 386\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011664309772773747\n",
            "Total Loss: 0.01289599546616143\n",
            "----------------------------------------\n",
            "Epoch 387\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011639092577163177\n",
            "Total Loss: 0.012864081806451372\n",
            "----------------------------------------\n",
            "Epoch 388\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011614369392768706\n",
            "Total Loss: 0.012832276317667781\n",
            "----------------------------------------\n",
            "Epoch 389\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01158991205748415\n",
            "Total Loss: 0.012800142380512147\n",
            "----------------------------------------\n",
            "Epoch 390\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011565818311689466\n",
            "Total Loss: 0.012768154805955313\n",
            "----------------------------------------\n",
            "Epoch 391\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011542059868699247\n",
            "Total Loss: 0.012736422189531844\n",
            "----------------------------------------\n",
            "Epoch 392\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011518406227765541\n",
            "Total Loss: 0.012704843621976434\n",
            "----------------------------------------\n",
            "Epoch 393\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011494931251481315\n",
            "Total Loss: 0.012673536653881538\n",
            "----------------------------------------\n",
            "Epoch 394\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011471535714226605\n",
            "Total Loss: 0.012642521812917524\n",
            "----------------------------------------\n",
            "Epoch 395\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011448162140666738\n",
            "Total Loss: 0.012611759181809139\n",
            "----------------------------------------\n",
            "Epoch 396\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011424913239434389\n",
            "Total Loss: 0.012581235507063533\n",
            "----------------------------------------\n",
            "Epoch 397\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011401448684713817\n",
            "Total Loss: 0.012550998149158056\n",
            "----------------------------------------\n",
            "Epoch 398\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011377693508145844\n",
            "Total Loss: 0.012521060666298076\n",
            "----------------------------------------\n",
            "Epoch 399\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01135385778084957\n",
            "Total Loss: 0.012491427897485731\n",
            "----------------------------------------\n",
            "Epoch 400\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011329827773329547\n",
            "Total Loss: 0.012461984518914717\n",
            "----------------------------------------\n",
            "Epoch 401\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01130534698166462\n",
            "Total Loss: 0.012432806133330388\n",
            "----------------------------------------\n",
            "Epoch 402\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01128043880969346\n",
            "Total Loss: 0.012403783475608011\n",
            "----------------------------------------\n",
            "Epoch 403\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011255169821848385\n",
            "Total Loss: 0.012374819853758863\n",
            "----------------------------------------\n",
            "Epoch 404\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01122957482882288\n",
            "Total Loss: 0.012346019234484624\n",
            "----------------------------------------\n",
            "Epoch 405\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011204017483577753\n",
            "Total Loss: 0.012317440376267136\n",
            "----------------------------------------\n",
            "Epoch 406\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011178481463351415\n",
            "Total Loss: 0.012289024469376162\n",
            "----------------------------------------\n",
            "Epoch 407\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011153033079289878\n",
            "Total Loss: 0.012260810171296065\n",
            "----------------------------------------\n",
            "Epoch 408\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011127739488928225\n",
            "Total Loss: 0.012232817268468447\n",
            "----------------------------------------\n",
            "Epoch 409\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01110259771021934\n",
            "Total Loss: 0.012205046847279126\n",
            "----------------------------------------\n",
            "Epoch 410\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011077460353234352\n",
            "Total Loss: 0.012177492898809034\n",
            "----------------------------------------\n",
            "Epoch 411\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011052287598395796\n",
            "Total Loss: 0.012150059325957713\n",
            "----------------------------------------\n",
            "Epoch 412\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.011027141155823364\n",
            "Total Loss: 0.012122855596200873\n",
            "----------------------------------------\n",
            "Epoch 413\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01100232204123735\n",
            "Total Loss: 0.012095848713055003\n",
            "----------------------------------------\n",
            "Epoch 414\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010977850517116005\n",
            "Total Loss: 0.012069013487277989\n",
            "----------------------------------------\n",
            "Epoch 415\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010953674665775556\n",
            "Total Loss: 0.012042298587869864\n",
            "----------------------------------------\n",
            "Epoch 416\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010929653859034367\n",
            "Total Loss: 0.012015569620312022\n",
            "----------------------------------------\n",
            "Epoch 417\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010905884712443739\n",
            "Total Loss: 0.01198894367822571\n",
            "----------------------------------------\n",
            "Epoch 418\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01088234677124341\n",
            "Total Loss: 0.01196245778965117\n",
            "----------------------------------------\n",
            "Epoch 419\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01085899738691578\n",
            "Total Loss: 0.011936171091037322\n",
            "----------------------------------------\n",
            "Epoch 420\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010835588471733404\n",
            "Total Loss: 0.011909955088310603\n",
            "----------------------------------------\n",
            "Epoch 421\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010812308810601606\n",
            "Total Loss: 0.011883840467110383\n",
            "----------------------------------------\n",
            "Epoch 422\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010789135640304455\n",
            "Total Loss: 0.01185790274639682\n",
            "----------------------------------------\n",
            "Epoch 423\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010766345329514738\n",
            "Total Loss: 0.011832113226223337\n",
            "----------------------------------------\n",
            "Epoch 424\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010743915165472602\n",
            "Total Loss: 0.011806435408104169\n",
            "----------------------------------------\n",
            "Epoch 425\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010721821895844915\n",
            "Total Loss: 0.011780869826605428\n",
            "----------------------------------------\n",
            "Epoch 426\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010700050639059936\n",
            "Total Loss: 0.01175543383790537\n",
            "----------------------------------------\n",
            "Epoch 427\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01067855135604503\n",
            "Total Loss: 0.011730138668482918\n",
            "----------------------------------------\n",
            "Epoch 428\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0106572906370163\n",
            "Total Loss: 0.011704986650641354\n",
            "----------------------------------------\n",
            "Epoch 429\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010636204786404962\n",
            "Total Loss: 0.011679958821229946\n",
            "----------------------------------------\n",
            "Epoch 430\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010615237163769265\n",
            "Total Loss: 0.011655141096556153\n",
            "----------------------------------------\n",
            "Epoch 431\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01059419630851281\n",
            "Total Loss: 0.011630404749818612\n",
            "----------------------------------------\n",
            "Epoch 432\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010573073001661746\n",
            "Total Loss: 0.011605739532697026\n",
            "----------------------------------------\n",
            "Epoch 433\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010551732110608792\n",
            "Total Loss: 0.0115812420992525\n",
            "----------------------------------------\n",
            "Epoch 434\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0105303943459808\n",
            "Total Loss: 0.01155686720595112\n",
            "----------------------------------------\n",
            "Epoch 435\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010509108231177051\n",
            "Total Loss: 0.01153261360301475\n",
            "----------------------------------------\n",
            "Epoch 436\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010487896407295801\n",
            "Total Loss: 0.011508483183078892\n",
            "----------------------------------------\n",
            "Epoch 437\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010466741284131751\n",
            "Total Loss: 0.011484461500892218\n",
            "----------------------------------------\n",
            "Epoch 438\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010445653245279287\n",
            "Total Loss: 0.011460585653632132\n",
            "----------------------------------------\n",
            "Epoch 439\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010424731814641099\n",
            "Total Loss: 0.011436818226847677\n",
            "----------------------------------------\n",
            "Epoch 440\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010403958120132416\n",
            "Total Loss: 0.011413184630641537\n",
            "----------------------------------------\n",
            "Epoch 441\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010383312389055785\n",
            "Total Loss: 0.011389690408155316\n",
            "----------------------------------------\n",
            "Epoch 442\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01036273848351008\n",
            "Total Loss: 0.011366359198235582\n",
            "----------------------------------------\n",
            "Epoch 443\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010342027083731007\n",
            "Total Loss: 0.011343104754540884\n",
            "----------------------------------------\n",
            "Epoch 444\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010321154937410978\n",
            "Total Loss: 0.011319858081495512\n",
            "----------------------------------------\n",
            "Epoch 445\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01030021953321073\n",
            "Total Loss: 0.011296753156532469\n",
            "----------------------------------------\n",
            "Epoch 446\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010279596563393122\n",
            "Total Loss: 0.01127390600513741\n",
            "----------------------------------------\n",
            "Epoch 447\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010259178708811871\n",
            "Total Loss: 0.011251178356535399\n",
            "----------------------------------------\n",
            "Epoch 448\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010238948153859024\n",
            "Total Loss: 0.0112285506994394\n",
            "----------------------------------------\n",
            "Epoch 449\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010218909319295833\n",
            "Total Loss: 0.01120602087287221\n",
            "----------------------------------------\n",
            "Epoch 450\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010199062083159582\n",
            "Total Loss: 0.011183625165472444\n",
            "----------------------------------------\n",
            "Epoch 451\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010179397216587298\n",
            "Total Loss: 0.011161477543956765\n",
            "----------------------------------------\n",
            "Epoch 452\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010159719556116938\n",
            "Total Loss: 0.01113944707101371\n",
            "----------------------------------------\n",
            "Epoch 453\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010139993320354968\n",
            "Total Loss: 0.011117540239197738\n",
            "----------------------------------------\n",
            "Epoch 454\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010120006572856994\n",
            "Total Loss: 0.011095747545877784\n",
            "----------------------------------------\n",
            "Epoch 455\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010099896294682734\n",
            "Total Loss: 0.011074076146409857\n",
            "----------------------------------------\n",
            "Epoch 456\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.01008001521113971\n",
            "Total Loss: 0.011052568534088208\n",
            "----------------------------------------\n",
            "Epoch 457\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010060389050999657\n",
            "Total Loss: 0.011031183627185047\n",
            "----------------------------------------\n",
            "Epoch 458\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010041020943245837\n",
            "Total Loss: 0.011009926332953288\n",
            "----------------------------------------\n",
            "Epoch 459\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010022064097141197\n",
            "Total Loss: 0.010988993157613513\n",
            "----------------------------------------\n",
            "Epoch 460\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.010003350770303277\n",
            "Total Loss: 0.010968259470368337\n",
            "----------------------------------------\n",
            "Epoch 461\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009984756320472516\n",
            "Total Loss: 0.01094759985376113\n",
            "----------------------------------------\n",
            "Epoch 462\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009966497954186509\n",
            "Total Loss: 0.010927076252446625\n",
            "----------------------------------------\n",
            "Epoch 463\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009948605467880931\n",
            "Total Loss: 0.010906715255458868\n",
            "----------------------------------------\n",
            "Epoch 464\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009930861047459332\n",
            "Total Loss: 0.010886525408780153\n",
            "----------------------------------------\n",
            "Epoch 465\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009913248494871803\n",
            "Total Loss: 0.010866546193217063\n",
            "----------------------------------------\n",
            "Epoch 466\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009895904326736753\n",
            "Total Loss: 0.010846713652939582\n",
            "----------------------------------------\n",
            "Epoch 467\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009878784969667383\n",
            "Total Loss: 0.010827016695189883\n",
            "----------------------------------------\n",
            "Epoch 468\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009861680593892384\n",
            "Total Loss: 0.010807443899384653\n",
            "----------------------------------------\n",
            "Epoch 469\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009844601984300073\n",
            "Total Loss: 0.010787991476690186\n",
            "----------------------------------------\n",
            "Epoch 470\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009827742969691317\n",
            "Total Loss: 0.010768676117236736\n",
            "----------------------------------------\n",
            "Epoch 471\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009810942592068167\n",
            "Total Loss: 0.010749521476914697\n",
            "----------------------------------------\n",
            "Epoch 472\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009794447669184648\n",
            "Total Loss: 0.010730554910122005\n",
            "----------------------------------------\n",
            "Epoch 473\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009778075265874169\n",
            "Total Loss: 0.010711719086623862\n",
            "----------------------------------------\n",
            "Epoch 474\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009761879679208461\n",
            "Total Loss: 0.01069256092389754\n",
            "----------------------------------------\n",
            "Epoch 475\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009745821293895845\n",
            "Total Loss: 0.01067308523085006\n",
            "----------------------------------------\n",
            "Epoch 476\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009730000672963616\n",
            "Total Loss: 0.010653700572217676\n",
            "----------------------------------------\n",
            "Epoch 477\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0097143420210323\n",
            "Total Loss: 0.01063453087466818\n",
            "----------------------------------------\n",
            "Epoch 478\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009698759319642819\n",
            "Total Loss: 0.010615669185357593\n",
            "----------------------------------------\n",
            "Epoch 479\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009683043365104263\n",
            "Total Loss: 0.010597088225397547\n",
            "----------------------------------------\n",
            "Epoch 480\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009667064231471628\n",
            "Total Loss: 0.010578653667338804\n",
            "----------------------------------------\n",
            "Epoch 481\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0096507700749507\n",
            "Total Loss: 0.010560259066505468\n",
            "----------------------------------------\n",
            "Epoch 482\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009634180277891572\n",
            "Total Loss: 0.010541858499338502\n",
            "----------------------------------------\n",
            "Epoch 483\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00961735488760195\n",
            "Total Loss: 0.010523458883764858\n",
            "----------------------------------------\n",
            "Epoch 484\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00960050371994839\n",
            "Total Loss: 0.010505186878169133\n",
            "----------------------------------------\n",
            "Epoch 485\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009583789153161732\n",
            "Total Loss: 0.010487008069389313\n",
            "----------------------------------------\n",
            "Epoch 486\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009567250206023049\n",
            "Total Loss: 0.010468903122116355\n",
            "----------------------------------------\n",
            "Epoch 487\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00955098757012509\n",
            "Total Loss: 0.010450911039419898\n",
            "----------------------------------------\n",
            "Epoch 488\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009534629895196841\n",
            "Total Loss: 0.01043270247160051\n",
            "----------------------------------------\n",
            "Epoch 489\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009518495301074294\n",
            "Total Loss: 0.010414355696120958\n",
            "----------------------------------------\n",
            "Epoch 490\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009502632210104084\n",
            "Total Loss: 0.010395913604832142\n",
            "----------------------------------------\n",
            "Epoch 491\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009487057552706202\n",
            "Total Loss: 0.010377588656766945\n",
            "----------------------------------------\n",
            "Epoch 492\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009471593583673148\n",
            "Total Loss: 0.010359335605952553\n",
            "----------------------------------------\n",
            "Epoch 493\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009456216454963402\n",
            "Total Loss: 0.01034115458944453\n",
            "----------------------------------------\n",
            "Epoch 494\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009440955120373693\n",
            "Total Loss: 0.010323079641238086\n",
            "----------------------------------------\n",
            "Epoch 495\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009425734958546575\n",
            "Total Loss: 0.010305081462771253\n",
            "----------------------------------------\n",
            "Epoch 496\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009410560487990202\n",
            "Total Loss: 0.010287194298345797\n",
            "----------------------------------------\n",
            "Epoch 497\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009395525198409048\n",
            "Total Loss: 0.010269394998405361\n",
            "----------------------------------------\n",
            "Epoch 498\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009380567875073924\n",
            "Total Loss: 0.01025166704965922\n",
            "----------------------------------------\n",
            "Epoch 499\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009365703207995454\n",
            "Total Loss: 0.010234046163818783\n",
            "----------------------------------------\n",
            "Epoch 500\n",
            "IS variance:  tensor(4.4092e-11, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.009350940396313259\n",
            "Total Loss: 0.01021654645715603\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.0186, -0.1972],\n",
            "        [-0.3706, -0.3068],\n",
            "        [ 0.2751,  0.1081],\n",
            "        [-0.5364, -0.5308],\n",
            "        [ 0.2213,  0.3552],\n",
            "        [ 0.4289, -0.5128],\n",
            "        [-0.3092, -0.5198],\n",
            "        [-0.6256,  0.5239],\n",
            "        [-0.3738, -0.4623],\n",
            "        [-0.3052, -0.0055],\n",
            "        [ 0.2603,  0.0117],\n",
            "        [-0.7439,  0.4636],\n",
            "        [ 0.1646,  0.0771],\n",
            "        [ 0.0946, -0.1665],\n",
            "        [-0.0081,  0.1858],\n",
            "        [-0.2221, -0.4742]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.0533, -0.3120, -0.7187, -0.5761,  0.2779,  0.3973, -0.2647, -0.4171,\n",
            "        -0.5832, -0.2371, -0.2243, -0.5920, -0.5850,  0.5120,  0.3300, -0.4199],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[-0.1177,  0.0552, -0.2745, -0.1992,  0.1423,  0.0961, -0.1609, -0.0457,\n",
            "         -0.1216,  0.1443, -0.1822,  0.1093,  0.0511, -0.0117, -0.1981, -0.0437],\n",
            "        [ 0.1405,  0.0375, -0.1717, -0.0944, -0.1968, -0.1396,  0.0098, -0.0300,\n",
            "         -0.0414, -0.0584, -0.0884, -0.0543,  0.1135, -0.1216, -0.0309, -0.1156],\n",
            "        [-0.1156, -0.2106, -0.1480, -0.1215, -0.0610,  0.0407,  0.1165,  0.0117,\n",
            "          0.0276,  0.0368, -0.2178, -0.0089,  0.1860,  0.2664,  0.0288,  0.0711],\n",
            "        [ 0.0458,  0.2251,  0.2779, -0.0038, -0.2295,  0.0824, -0.0106,  0.0183,\n",
            "         -0.0628, -0.0223, -0.3327,  0.0642, -0.0504,  0.0935, -0.2593, -0.1720],\n",
            "        [-0.0031, -0.2483,  0.1123,  0.1643, -0.0140, -0.0593,  0.0911, -0.1093,\n",
            "          0.1842, -0.2327,  0.1611,  0.1282, -0.1679,  0.0528,  0.1971, -0.2459],\n",
            "        [-0.2132,  0.2441,  0.4509, -0.1061,  0.0307, -0.3210,  0.2160,  0.1631,\n",
            "          0.0179, -0.1739,  0.1290, -0.0996, -0.1300, -0.2949,  0.2493, -0.0475],\n",
            "        [ 0.2275, -0.1066, -0.5028,  0.1472,  0.1261, -0.1080, -0.1157, -0.0163,\n",
            "          0.0511,  0.2034, -0.0849,  0.0721, -0.1762, -0.1261,  0.1521,  0.2126],\n",
            "        [ 0.0831,  0.1702, -0.1650,  0.2022, -0.0474, -0.1346,  0.1440,  0.0190,\n",
            "         -0.0738,  0.0414, -0.0146, -0.1236, -0.4238,  0.1959, -0.0404, -0.2164],\n",
            "        [ 0.2111, -0.0143, -0.1892,  0.0009, -0.2418,  0.1937, -0.0071, -0.1693,\n",
            "         -0.1809,  0.2044, -0.1419, -0.2157,  0.1146,  0.1251, -0.1787, -0.0017],\n",
            "        [ 0.1093, -0.1100, -0.1713, -0.1925,  0.0480,  0.1909,  0.0990, -0.0854,\n",
            "          0.0691,  0.2447, -0.0030,  0.1170, -0.0011,  0.0178,  0.1488,  0.0439],\n",
            "        [-0.1150, -0.0344, -0.2067,  0.0541, -0.1238,  0.2437,  0.0160,  0.0390,\n",
            "         -0.0168,  0.2171, -0.1809,  0.1055, -0.1627,  0.1798,  0.0395,  0.1476],\n",
            "        [ 0.0539, -0.2007, -0.0718, -0.2019,  0.1162,  0.0344,  0.2311,  0.0403,\n",
            "          0.0942, -0.1031,  0.1650, -0.0881, -0.1250, -0.1125,  0.0571, -0.2097],\n",
            "        [ 0.0199,  0.1871,  0.1457,  0.2301, -0.0352,  0.1512,  0.0172, -0.2027,\n",
            "         -0.2080, -0.2113,  0.0072,  0.0773, -0.1488, -0.0041,  0.0402,  0.1663],\n",
            "        [ 0.0864,  0.1208,  0.0769, -0.1497, -0.0844, -0.3141,  0.1436, -0.0325,\n",
            "          0.1342,  0.1814,  0.0277, -0.1441, -0.2540, -0.0291,  0.0316,  0.0246],\n",
            "        [ 0.0765,  0.0677, -0.1492, -0.0259,  0.1809,  0.0167,  0.2338, -0.0056,\n",
            "          0.0088, -0.0649, -0.1796, -0.1616, -0.0874,  0.1306, -0.0031,  0.0133],\n",
            "        [-0.2097,  0.1283,  0.2576, -0.0258, -0.0257,  0.0645,  0.0710,  0.1343,\n",
            "         -0.1164, -0.1588, -0.0194, -0.1565,  0.2038, -0.1204, -0.2051, -0.0748]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.0563, -0.1365,  0.4128, -0.1557, -0.2748, -0.1153,  0.0699,  0.4607,\n",
            "        -0.0208,  0.0074,  0.1967,  0.1786, -0.0784, -0.0171, -0.0900,  0.1305],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.1444, -0.2177,  0.1513, -0.3088, -0.2583, -0.0259,  0.0883,  0.3571,\n",
            "         -0.2419,  0.0796,  0.1273,  0.0099,  0.1786, -0.1827,  0.0104, -0.1571]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.2638], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
              "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_400_random_pi_b.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7fe7ebf3-e6db-480b-c29e-ce0af762563e",
        "id": "KHFBY772CntQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f226683d-5e5c-4015-8dcd-75c6386ecb0e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f226683d-5e5c-4015-8dcd-75c6386ecb0e\")) {                    Plotly.newPlot(                        \"f226683d-5e5c-4015-8dcd-75c6386ecb0e\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[0.5910281195652765,0.599944144285047,0.6013064210361019,0.5897657549049931,0.5360130654784725,0.4585975197533225,0.4383518386132964,0.42414605123639765,0.4132574754565863,0.402368899676775],[0.5673928269701577,0.5766625209428555,0.5694647762396494,0.5292376775694243,0.45458847955645065,0.3877638766574545,0.37618849311104685,0.3652999173312355,0.35441134155142423,0.34199716963240345],[0.5401064339373612,0.5491106998762553,0.5279206696226268,0.45315791936725625,0.3756607431251288,0.3285945288230362,0.3211070539161256,0.3097391439189039,0.2983712339216822,0.28700332392446043],[0.5114085085743607,0.5194862940789842,0.47969578678444985,0.4042953231808297,0.31630134818377675,0.28318154172656357,0.27200808760263717,0.26126634464002574,0.2505340265773957,0.23980170851476568],[0.5026906739231902,0.49735751027161945,0.42958678370511916,0.35299454254540236,0.2598939640081788,0.23143978388134448,0.2202663297574181,0.2099866975368489,0.20498089792841823,0.19997509831998758],[0.49571373276349195,0.46789607863520066,0.38490624481142877,0.3023745946704749,0.22989118391852087,0.19220222682241828,0.17631757772706677,0.1711284189707795,0.16612261936234887,0.16111681975391817],[0.48874387924549245,0.42952717940147356,0.3410282688257626,0.27157922538468576,0.21639250510209646,0.18246370649773583,0.15109720385415643,0.1322701404047102,0.12726434079627952,0.12225854118784885],[0.47666887098828253,0.3989803874635753,0.32726129770871554,0.2639738220305926,0.2051932797994397,0.17450452426900348,0.1424071560721632,0.11004316506465711,0.08840606223021014,0.08340026262177946],[0.4684860715641469,0.3913241265376082,0.31973107697679903,0.25474197895552736,0.19642660475192245,0.16511281714854953,0.13481077147056084,0.10235060564659051,0.07012050775057105,0.04454198405571019],[0.46153839104589073,0.38276988981808513,0.311365787698716,0.24551013588046203,0.18998586706038895,0.1557211100280957,0.125419064350107,0.09482862891751437,0.06229405522101794,0.030306353519190893]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[0.030306353519190893,0.6013064210361019],\"ticktext\":[0.030306353519190893,0.6013064210361019]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f226683d-5e5c-4015-8dcd-75c6386ecb0e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5825b5e-ac9d-45a0-cc7d-531201d224cc",
        "id": "NGrNE464EYE7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15634452293280188"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_400_random_pi_b.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1274a4f7-03e3-48e5-929a-1e6ad27b28c2",
        "id": "xe06MjZgEYE9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1504, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 600 pi_b top 2"
      ],
      "metadata": {
        "id": "ULn4Hoyu85kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(600, env_50, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 1, 0.05)\n",
        "pi_e = experiment_actions(1000, env_50, P_pi_e)\n",
        "model_600_random_pi_b = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64)\n",
        "test_600_random_pi_b = SCOPE_straight(model_600_random_pi_b, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "FIfZx1bS8-7G"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_600_random_pi_b.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986bd42a-2ae3-46d3-c6e8-8e7017817eeb",
        "id": "IwDciHMj8-7H"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(24.2014, dtype=torch.float64), tensor(134.2654, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_600_random_pi_b.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f12e95aa-75e3-44ea-8f9b-6a31de00b563",
        "id": "o0IMnlZV8-7I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"e1e46cec-4570-4dec-aa7f-987a12b3db14\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e1e46cec-4570-4dec-aa7f-987a12b3db14\")) {                    Plotly.newPlot(                        \"e1e46cec-4570-4dec-aa7f-987a12b3db14\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,3,34,290,636,1312,1441,875,730,505,0,11,81,560,559,0,1345,678,619,546,0,104,238,458,158,0,980,523,602,1011,0,181,211,110,50,0,535,268,246,245,0,176,74,41,16,0,167,113,103,69,27,115,33,13,4,0,33,21,40,16,7,22,7,1,0,0,34,18,44,19,1,1,2,0,0,0,32,19,32,10,0,0,1,0,0,0,39,18,34,12,0,0,1,0,0,0,23,13,23,9],\"zmax\":1441,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e1e46cec-4570-4dec-aa7f-987a12b3db14');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_600_random_pi_b.train_var_scope(300, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7d380d-2e99-42dc-e32b-c2a97d5a707c",
        "id": "AMPHu0qv8-7I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.008124469012146312\n",
            "SCOPE mean: 0.11593467505750471, SCOPE var: 0.001134831577478478\n",
            "Total Loss: 0.011470035771407587\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007877572700959606\n",
            "SCOPE mean: 0.11402489523061764, SCOPE var: 0.0011566205224887104\n",
            "Total Loss: 0.008660720224668825\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007835659068336106\n",
            "SCOPE mean: 0.1149561133499077, SCOPE var: 0.0011602695166762504\n",
            "Total Loss: 0.00864455892775779\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00781327248599228\n",
            "SCOPE mean: 0.11650521160052017, SCOPE var: 0.0011712368107998686\n",
            "Total Loss: 0.008583540773734768\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0078068080769548446\n",
            "SCOPE mean: 0.11759279905746084, SCOPE var: 0.0011825409781654355\n",
            "Total Loss: 0.008531025569341417\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007791067258056491\n",
            "SCOPE mean: 0.11760819029879795, SCOPE var: 0.0011810892179655761\n",
            "Total Loss: 0.008478166583153212\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007759666876520431\n",
            "SCOPE mean: 0.1167941805196363, SCOPE var: 0.001169221054552784\n",
            "Total Loss: 0.008426775877720725\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007720853759773746\n",
            "SCOPE mean: 0.11554456402355345, SCOPE var: 0.0011515404422226723\n",
            "Total Loss: 0.008380442428119106\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007685676797457618\n",
            "SCOPE mean: 0.11439950873663551, SCOPE var: 0.0011334282661550077\n",
            "Total Loss: 0.008343843102122234\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007654656082694976\n",
            "SCOPE mean: 0.1137352611986255, SCOPE var: 0.00111973395523087\n",
            "Total Loss: 0.008311895460945466\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007621772533953928\n",
            "SCOPE mean: 0.11376316733497333, SCOPE var: 0.001114400997632692\n",
            "Total Loss: 0.008276496085522077\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007581544521994147\n",
            "SCOPE mean: 0.11433983495483414, SCOPE var: 0.0011107866166693988\n",
            "Total Loss: 0.008231125590912528\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007537190824361304\n",
            "SCOPE mean: 0.11547220007788513, SCOPE var: 0.0011167260258329187\n",
            "Total Loss: 0.008180392271472785\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007494550625655385\n",
            "SCOPE mean: 0.11685044743846941, SCOPE var: 0.0011270066856737169\n",
            "Total Loss: 0.008131781030619432\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007457007972904669\n",
            "SCOPE mean: 0.11819452338256144, SCOPE var: 0.0011382730053276177\n",
            "Total Loss: 0.008089482373256043\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007423345814965873\n",
            "SCOPE mean: 0.11929874128056028, SCOPE var: 0.0011475993952037465\n",
            "Total Loss: 0.008052844225362146\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007389553345662418\n",
            "SCOPE mean: 0.11999527452162417, SCOPE var: 0.0011528171193096646\n",
            "Total Loss: 0.00801760376170688\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0073527111711526585\n",
            "SCOPE mean: 0.12022960586836123, SCOPE var: 0.0011529734112841316\n",
            "Total Loss: 0.007980088531107531\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0073132171732442856\n",
            "SCOPE mean: 0.12013572682193172, SCOPE var: 0.0011522281230335095\n",
            "Total Loss: 0.007940092218099467\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007272788535289423\n",
            "SCOPE mean: 0.11971310044037518, SCOPE var: 0.0011456486635592659\n",
            "Total Loss: 0.007898433817619746\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007233268373935128\n",
            "SCOPE mean: 0.1190784335849409, SCOPE var: 0.001133240491825727\n",
            "Total Loss: 0.007855915433411388\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007195866924616426\n",
            "SCOPE mean: 0.11843689294418434, SCOPE var: 0.0011212088588071858\n",
            "Total Loss: 0.007814314104298982\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00716068281290107\n",
            "SCOPE mean: 0.1179003082598098, SCOPE var: 0.0011128875782402002\n",
            "Total Loss: 0.007775104745287888\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007126647100976957\n",
            "SCOPE mean: 0.1175421835182991, SCOPE var: 0.0011068913592844538\n",
            "Total Loss: 0.0077379666104174195\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007093528261151065\n",
            "SCOPE mean: 0.11737548995665002, SCOPE var: 0.001102934167132901\n",
            "Total Loss: 0.007702663307373159\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007060442863258999\n",
            "SCOPE mean: 0.11734296209504398, SCOPE var: 0.0011004694101011106\n",
            "Total Loss: 0.007668234012605242\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.007026790493413335\n",
            "SCOPE mean: 0.11737915926240866, SCOPE var: 0.0010987513187300326\n",
            "Total Loss: 0.007633278888137968\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0069923056197031515\n",
            "SCOPE mean: 0.11746023631676149, SCOPE var: 0.0010974484966570928\n",
            "Total Loss: 0.0075969656737360225\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006957022245713827\n",
            "SCOPE mean: 0.11756057727494176, SCOPE var: 0.0010960262800616433\n",
            "Total Loss: 0.0075594979967627445\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006921467087829798\n",
            "SCOPE mean: 0.1177071074494605, SCOPE var: 0.0010958884387025612\n",
            "Total Loss: 0.007522860945005903\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006885945064335639\n",
            "SCOPE mean: 0.11780225954566895, SCOPE var: 0.0010900588827676774\n",
            "Total Loss: 0.00748677505307038\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006850876898648046\n",
            "SCOPE mean: 0.11784293953080155, SCOPE var: 0.001079024401370808\n",
            "Total Loss: 0.007450637631037206\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006817180058170639\n",
            "SCOPE mean: 0.11796755127944741, SCOPE var: 0.0010709437478957893\n",
            "Total Loss: 0.007415124958170253\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006784225063736083\n",
            "SCOPE mean: 0.11806498653276469, SCOPE var: 0.0010616220566334784\n",
            "Total Loss: 0.007379222350348678\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006752092630096508\n",
            "SCOPE mean: 0.11814128179716145, SCOPE var: 0.0010515667102701915\n",
            "Total Loss: 0.0073430841143848164\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006720443425216252\n",
            "SCOPE mean: 0.11815113652737477, SCOPE var: 0.0010392782769615026\n",
            "Total Loss: 0.007306773639209183\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0066888641459991165\n",
            "SCOPE mean: 0.11813437136744633, SCOPE var: 0.0010278612974133619\n",
            "Total Loss: 0.007270884039307527\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006657121741985006\n",
            "SCOPE mean: 0.11800545843234872, SCOPE var: 0.0010128471126973145\n",
            "Total Loss: 0.007235161345526396\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006625471558798853\n",
            "SCOPE mean: 0.11784659145120695, SCOPE var: 0.0009990929248651459\n",
            "Total Loss: 0.007200024771954463\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006594078645330061\n",
            "SCOPE mean: 0.11765192088272532, SCOPE var: 0.0009839204373403557\n",
            "Total Loss: 0.007164981783759343\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0065630250528090186\n",
            "SCOPE mean: 0.11752805487877777, SCOPE var: 0.0009700395374678437\n",
            "Total Loss: 0.007130594007102763\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006532114067347815\n",
            "SCOPE mean: 0.11754287071319763, SCOPE var: 0.0009575263696485406\n",
            "Total Loss: 0.007096408319901915\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006501130284686605\n",
            "SCOPE mean: 0.11769449021380597, SCOPE var: 0.0009463479141394305\n",
            "Total Loss: 0.0070620142745490356\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006470429153181109\n",
            "SCOPE mean: 0.1179642762312132, SCOPE var: 0.0009377844368905581\n",
            "Total Loss: 0.007027971647733829\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006439902699605019\n",
            "SCOPE mean: 0.11820656592762283, SCOPE var: 0.0009276486590209608\n",
            "Total Loss: 0.006994349419513239\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006409254722152267\n",
            "SCOPE mean: 0.11833911867670413, SCOPE var: 0.0009147414694190063\n",
            "Total Loss: 0.006961031603890956\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0063782640488646925\n",
            "SCOPE mean: 0.11842616480045508, SCOPE var: 0.000903160883105373\n",
            "Total Loss: 0.0069281089039631176\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006346950099993336\n",
            "SCOPE mean: 0.11843948481944329, SCOPE var: 0.0008924151411338704\n",
            "Total Loss: 0.006895527745847311\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006315374619927228\n",
            "SCOPE mean: 0.1183247081882183, SCOPE var: 0.0008793253677183904\n",
            "Total Loss: 0.0068630555225697686\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00628403300318783\n",
            "SCOPE mean: 0.1181369563143415, SCOPE var: 0.0008652974195199626\n",
            "Total Loss: 0.006830697758237885\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006253205014918133\n",
            "SCOPE mean: 0.11798111241601687, SCOPE var: 0.000853264063037228\n",
            "Total Loss: 0.006798464041677157\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006222913155560218\n",
            "SCOPE mean: 0.1178808053997533, SCOPE var: 0.000843084366809599\n",
            "Total Loss: 0.006766407541029687\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006193190979088112\n",
            "SCOPE mean: 0.11778074195334805, SCOPE var: 0.0008324661268876624\n",
            "Total Loss: 0.00673469174626667\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0061641509843707\n",
            "SCOPE mean: 0.11771972210107183, SCOPE var: 0.0008230494247904672\n",
            "Total Loss: 0.006703415741889726\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006135437953787612\n",
            "SCOPE mean: 0.11766698162237503, SCOPE var: 0.0008144531733225757\n",
            "Total Loss: 0.006672057030950092\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0061069856489754685\n",
            "SCOPE mean: 0.11760986268607808, SCOPE var: 0.0008063132229409559\n",
            "Total Loss: 0.006640633455537462\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006078985942014901\n",
            "SCOPE mean: 0.11757658096850887, SCOPE var: 0.0007999360572139904\n",
            "Total Loss: 0.006609531518132072\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006051489750820026\n",
            "SCOPE mean: 0.11750630114323407, SCOPE var: 0.0007928281621551388\n",
            "Total Loss: 0.006578974667442706\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.006024529890504702\n",
            "SCOPE mean: 0.11737228553544876, SCOPE var: 0.0007833661698282518\n",
            "Total Loss: 0.006549336168469618\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005998587954234358\n",
            "SCOPE mean: 0.11728108389809866, SCOPE var: 0.0007746475673013932\n",
            "Total Loss: 0.00651995920002502\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005973219104179911\n",
            "SCOPE mean: 0.11720213084464404, SCOPE var: 0.0007661053729434551\n",
            "Total Loss: 0.006490582509110978\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005947918453490078\n",
            "SCOPE mean: 0.11711760167203884, SCOPE var: 0.0007572648422777829\n",
            "Total Loss: 0.006461623569174509\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005922314593444466\n",
            "SCOPE mean: 0.11700752248946118, SCOPE var: 0.0007482955418753533\n",
            "Total Loss: 0.006432839721660982\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005896515013242896\n",
            "SCOPE mean: 0.11696539818827391, SCOPE var: 0.0007429911086308592\n",
            "Total Loss: 0.00640433956868709\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005870669742541217\n",
            "SCOPE mean: 0.11694752030070517, SCOPE var: 0.000736903422345624\n",
            "Total Loss: 0.006376131521106245\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005844733223433926\n",
            "SCOPE mean: 0.11699071999017265, SCOPE var: 0.0007302039461845512\n",
            "Total Loss: 0.00634810732712148\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005818555501493068\n",
            "SCOPE mean: 0.11713491195770996, SCOPE var: 0.00072349974288614\n",
            "Total Loss: 0.0063199606622803\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005792220429154021\n",
            "SCOPE mean: 0.11729983989970551, SCOPE var: 0.0007165312579590559\n",
            "Total Loss: 0.006291910345882747\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005765961494391749\n",
            "SCOPE mean: 0.11744931780640257, SCOPE var: 0.0007104827627479824\n",
            "Total Loss: 0.006264648875053524\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005739474231104673\n",
            "SCOPE mean: 0.11755609407696548, SCOPE var: 0.0007056401738699013\n",
            "Total Loss: 0.006237599008542067\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005712939449119328\n",
            "SCOPE mean: 0.1175939668849908, SCOPE var: 0.0007002863851812452\n",
            "Total Loss: 0.006210602828391853\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005686909655115232\n",
            "SCOPE mean: 0.11762420949548136, SCOPE var: 0.0006946605270084144\n",
            "Total Loss: 0.006183698432032449\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005661825182927477\n",
            "SCOPE mean: 0.1177591677882564, SCOPE var: 0.0006919542289565936\n",
            "Total Loss: 0.006157076673065268\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00563732223630283\n",
            "SCOPE mean: 0.1178495742177968, SCOPE var: 0.0006877852587651455\n",
            "Total Loss: 0.006131225341026128\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005613039991446236\n",
            "SCOPE mean: 0.11786130179346935, SCOPE var: 0.0006820586233897503\n",
            "Total Loss: 0.006105463054180216\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005588932148472809\n",
            "SCOPE mean: 0.11784422700292718, SCOPE var: 0.0006753420301599263\n",
            "Total Loss: 0.006079750889738376\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005564978093793295\n",
            "SCOPE mean: 0.11779529531985163, SCOPE var: 0.0006677951852029201\n",
            "Total Loss: 0.006054103299777058\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0055414709215904625\n",
            "SCOPE mean: 0.11775691980367933, SCOPE var: 0.0006616073001496731\n",
            "Total Loss: 0.006028798239587334\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005518176980022403\n",
            "SCOPE mean: 0.11773065422067357, SCOPE var: 0.0006567357372639042\n",
            "Total Loss: 0.006003701575452427\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005494633690954871\n",
            "SCOPE mean: 0.11764766219461499, SCOPE var: 0.000651652911309579\n",
            "Total Loss: 0.005978491024157037\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005471217720679688\n",
            "SCOPE mean: 0.11759734561223541, SCOPE var: 0.0006470330087146963\n",
            "Total Loss: 0.005953798799922174\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005448465293885177\n",
            "SCOPE mean: 0.11761090275134131, SCOPE var: 0.0006442255732617993\n",
            "Total Loss: 0.005929390962018193\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0054262671622692694\n",
            "SCOPE mean: 0.11758969472097906, SCOPE var: 0.0006411104896934771\n",
            "Total Loss: 0.005905162336394564\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005404384661410146\n",
            "SCOPE mean: 0.11754354227433057, SCOPE var: 0.0006372370205707191\n",
            "Total Loss: 0.00588107019793174\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0053826558355503365\n",
            "SCOPE mean: 0.1174610181435301, SCOPE var: 0.0006326597048723261\n",
            "Total Loss: 0.005857020863613339\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0053611375654392815\n",
            "SCOPE mean: 0.11741492629107547, SCOPE var: 0.0006279873310670319\n",
            "Total Loss: 0.005832998413003339\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005339730012338335\n",
            "SCOPE mean: 0.11741938206903828, SCOPE var: 0.0006234371902320946\n",
            "Total Loss: 0.0058095353719247525\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005318298302914109\n",
            "SCOPE mean: 0.11745197217844454, SCOPE var: 0.0006184539714316873\n",
            "Total Loss: 0.0057860533084111284\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005296620782539172\n",
            "SCOPE mean: 0.1174147217436981, SCOPE var: 0.0006148129090234413\n",
            "Total Loss: 0.005762827376234604\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0052742591219628225\n",
            "SCOPE mean: 0.11734407271211802, SCOPE var: 0.0006123988975865666\n",
            "Total Loss: 0.005739562287567698\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005251763759997743\n",
            "SCOPE mean: 0.11729843978007325, SCOPE var: 0.0006103777490615159\n",
            "Total Loss: 0.005716970761179307\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0052297974981224985\n",
            "SCOPE mean: 0.11722399955460187, SCOPE var: 0.0006091583911357338\n",
            "Total Loss: 0.005694533847842679\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005208614225789815\n",
            "SCOPE mean: 0.11706272509948326, SCOPE var: 0.0006069490151539968\n",
            "Total Loss: 0.005672301901846312\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005187857228732268\n",
            "SCOPE mean: 0.11691698173517809, SCOPE var: 0.000604004027941059\n",
            "Total Loss: 0.005650039182849106\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005167407532368506\n",
            "SCOPE mean: 0.11679606486901956, SCOPE var: 0.000600616346361684\n",
            "Total Loss: 0.005627761640763658\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005147318291179086\n",
            "SCOPE mean: 0.11670851835626138, SCOPE var: 0.000596969710668337\n",
            "Total Loss: 0.005605978074511784\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0051270753280632285\n",
            "SCOPE mean: 0.11666439478986286, SCOPE var: 0.0005936563336318418\n",
            "Total Loss: 0.005584057768975843\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005106760844227341\n",
            "SCOPE mean: 0.11665162870227036, SCOPE var: 0.0005906029868303916\n",
            "Total Loss: 0.00556239006495725\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005086790309584867\n",
            "SCOPE mean: 0.11654999465907018, SCOPE var: 0.0005884141144285539\n",
            "Total Loss: 0.0055410125924736375\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005066929507167406\n",
            "SCOPE mean: 0.11631013391813895, SCOPE var: 0.0005854585574526801\n",
            "Total Loss: 0.005519879124669753\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005047259118184452\n",
            "SCOPE mean: 0.11610715037377708, SCOPE var: 0.0005823498238698268\n",
            "Total Loss: 0.005498736077703509\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005028041140731617\n",
            "SCOPE mean: 0.11599203863075447, SCOPE var: 0.0005805072770567645\n",
            "Total Loss: 0.005477805069942089\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.005009590608222738\n",
            "SCOPE mean: 0.11585366804144091, SCOPE var: 0.0005786009456855745\n",
            "Total Loss: 0.0054571472921293605\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004991233736097004\n",
            "SCOPE mean: 0.11576474911657875, SCOPE var: 0.000576788421633121\n",
            "Total Loss: 0.005436634339152083\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00497287968264311\n",
            "SCOPE mean: 0.11572906033936976, SCOPE var: 0.000575183592309597\n",
            "Total Loss: 0.005416353001106934\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004954483595630425\n",
            "SCOPE mean: 0.11561419654314273, SCOPE var: 0.0005729441603093174\n",
            "Total Loss: 0.005396313289194448\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004936073375808467\n",
            "SCOPE mean: 0.11550805290490367, SCOPE var: 0.0005704919161063198\n",
            "Total Loss: 0.0053763558478100455\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004917582598851635\n",
            "SCOPE mean: 0.11542543754024294, SCOPE var: 0.0005681411256996989\n",
            "Total Loss: 0.005356458611569122\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004899448445091492\n",
            "SCOPE mean: 0.11526612699943892, SCOPE var: 0.0005649855806737791\n",
            "Total Loss: 0.005336694407991461\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004881377467265123\n",
            "SCOPE mean: 0.115135029941044, SCOPE var: 0.0005619803810525352\n",
            "Total Loss: 0.005317221115456316\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004863249222537274\n",
            "SCOPE mean: 0.11506045189614636, SCOPE var: 0.0005596482151157236\n",
            "Total Loss: 0.00529798453779029\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004845144898319855\n",
            "SCOPE mean: 0.11496255576850951, SCOPE var: 0.0005578424177935708\n",
            "Total Loss: 0.00527875677240716\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0048270353620691\n",
            "SCOPE mean: 0.11491220811887916, SCOPE var: 0.0005566567759736361\n",
            "Total Loss: 0.005259639095492707\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0048092993602734885\n",
            "SCOPE mean: 0.11488086487697481, SCOPE var: 0.0005552703451110726\n",
            "Total Loss: 0.005240779812816947\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004791774712560099\n",
            "SCOPE mean: 0.11476795183909466, SCOPE var: 0.0005533850326289295\n",
            "Total Loss: 0.005222075023239758\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00477437776804576\n",
            "SCOPE mean: 0.11472685871361725, SCOPE var: 0.000552716202317162\n",
            "Total Loss: 0.005203320880832339\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0047568499406355335\n",
            "SCOPE mean: 0.11477938921365785, SCOPE var: 0.0005517865400810537\n",
            "Total Loss: 0.005184856313479823\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004739868319173199\n",
            "SCOPE mean: 0.1146534581465829, SCOPE var: 0.0005497618968275461\n",
            "Total Loss: 0.005166384707779376\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004723003638948818\n",
            "SCOPE mean: 0.11455822479019555, SCOPE var: 0.0005476141371314355\n",
            "Total Loss: 0.005148054533923598\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004706465909491783\n",
            "SCOPE mean: 0.1143572270267789, SCOPE var: 0.0005449888067075102\n",
            "Total Loss: 0.00512996363099853\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004689723364134145\n",
            "SCOPE mean: 0.11427951324701907, SCOPE var: 0.0005425761641876878\n",
            "Total Loss: 0.0051118532605820485\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004672798560755508\n",
            "SCOPE mean: 0.11430030023811853, SCOPE var: 0.000540350979429928\n",
            "Total Loss: 0.005093903950414632\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0046560196571993695\n",
            "SCOPE mean: 0.11430648911216963, SCOPE var: 0.0005383098592841849\n",
            "Total Loss: 0.005076136713619704\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004639638341341646\n",
            "SCOPE mean: 0.11421605987360185, SCOPE var: 0.0005361797829752511\n",
            "Total Loss: 0.005058393876350188\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0046238215411002\n",
            "SCOPE mean: 0.11411280901059949, SCOPE var: 0.0005357800671804721\n",
            "Total Loss: 0.005041361330418319\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0046077449828452206\n",
            "SCOPE mean: 0.11426085524437385, SCOPE var: 0.0005373026268539971\n",
            "Total Loss: 0.005024319923199431\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004591405429020812\n",
            "SCOPE mean: 0.11456348161082679, SCOPE var: 0.0005388426063765015\n",
            "Total Loss: 0.005007330787813275\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00457533579687199\n",
            "SCOPE mean: 0.11483681157935434, SCOPE var: 0.0005397887034754203\n",
            "Total Loss: 0.004990508344134504\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004559854133871549\n",
            "SCOPE mean: 0.11500177685206066, SCOPE var: 0.0005399102066005421\n",
            "Total Loss: 0.004974039125960715\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0045452469760783485\n",
            "SCOPE mean: 0.11486998494198174, SCOPE var: 0.0005382626186663621\n",
            "Total Loss: 0.004957473844213375\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004531281278071946\n",
            "SCOPE mean: 0.11459196116189478, SCOPE var: 0.0005366605479812886\n",
            "Total Loss: 0.004940921876629787\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004517572708879487\n",
            "SCOPE mean: 0.11426274649318975, SCOPE var: 0.0005351572992263696\n",
            "Total Loss: 0.004924763269584307\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004503728936853492\n",
            "SCOPE mean: 0.11407914221727547, SCOPE var: 0.0005353203285873023\n",
            "Total Loss: 0.0049087361068478235\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0044893515335105\n",
            "SCOPE mean: 0.11411599948053663, SCOPE var: 0.0005362011798435957\n",
            "Total Loss: 0.004892715888976083\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004474524210613126\n",
            "SCOPE mean: 0.1143351863901216, SCOPE var: 0.0005372544931422806\n",
            "Total Loss: 0.004876553049878597\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004459900306984995\n",
            "SCOPE mean: 0.1145971496309282, SCOPE var: 0.0005386008352155381\n",
            "Total Loss: 0.0048607174209413994\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004445810443078524\n",
            "SCOPE mean: 0.11470567468170684, SCOPE var: 0.0005394180910799295\n",
            "Total Loss: 0.004845276579673417\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00443226685321305\n",
            "SCOPE mean: 0.11455024376311915, SCOPE var: 0.0005383758266063512\n",
            "Total Loss: 0.004829745623052554\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004418964179737087\n",
            "SCOPE mean: 0.114264783996499, SCOPE var: 0.0005360115992182285\n",
            "Total Loss: 0.004814403919139053\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00440549407864613\n",
            "SCOPE mean: 0.11409656320739486, SCOPE var: 0.000534747070932552\n",
            "Total Loss: 0.00479922909559806\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004391378925374992\n",
            "SCOPE mean: 0.11406717314049027, SCOPE var: 0.0005339974852452407\n",
            "Total Loss: 0.004783894664773147\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004377136819004509\n",
            "SCOPE mean: 0.11407606209845457, SCOPE var: 0.0005333909275325373\n",
            "Total Loss: 0.004768778653690667\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004362851113860735\n",
            "SCOPE mean: 0.11420994708571447, SCOPE var: 0.000535199205336065\n",
            "Total Loss: 0.004753798178595815\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004348872135735007\n",
            "SCOPE mean: 0.11425754838200687, SCOPE var: 0.0005362895355786135\n",
            "Total Loss: 0.004738704916979987\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004335329355709742\n",
            "SCOPE mean: 0.11425522656859716, SCOPE var: 0.0005373791344255869\n",
            "Total Loss: 0.004723852672211794\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004321762661368106\n",
            "SCOPE mean: 0.11434225785912686, SCOPE var: 0.0005385629646839549\n",
            "Total Loss: 0.004709319996496567\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004307810210008955\n",
            "SCOPE mean: 0.11456398829472342, SCOPE var: 0.0005395619924171254\n",
            "Total Loss: 0.004694708707750118\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004293808222630705\n",
            "SCOPE mean: 0.11470250732063973, SCOPE var: 0.0005397928639138621\n",
            "Total Loss: 0.004680042402136803\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0042799669327969955\n",
            "SCOPE mean: 0.11472509788618344, SCOPE var: 0.0005392420712529883\n",
            "Total Loss: 0.004665643700163309\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004266391329720137\n",
            "SCOPE mean: 0.1146762923467244, SCOPE var: 0.0005376787770752169\n",
            "Total Loss: 0.0046515375626974605\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0042536787845080725\n",
            "SCOPE mean: 0.1143561193009531, SCOPE var: 0.0005349081686047006\n",
            "Total Loss: 0.004637283038811866\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004241285256204668\n",
            "SCOPE mean: 0.1141062603697784, SCOPE var: 0.0005343117169929344\n",
            "Total Loss: 0.00462344251870049\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004228701113301399\n",
            "SCOPE mean: 0.11404822647566926, SCOPE var: 0.0005346232266220371\n",
            "Total Loss: 0.00460975370807969\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004215818633049136\n",
            "SCOPE mean: 0.11416509275773751, SCOPE var: 0.0005349689741651756\n",
            "Total Loss: 0.004596128245698061\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004203102275838071\n",
            "SCOPE mean: 0.11423227314647436, SCOPE var: 0.00053435133943447\n",
            "Total Loss: 0.004582525253927198\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0041905378037818625\n",
            "SCOPE mean: 0.11423945562239142, SCOPE var: 0.0005335317534003118\n",
            "Total Loss: 0.00456895790877166\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004178323127229974\n",
            "SCOPE mean: 0.11407066508843762, SCOPE var: 0.0005322488829931697\n",
            "Total Loss: 0.004555643499598994\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004166240400167817\n",
            "SCOPE mean: 0.11408270508816319, SCOPE var: 0.000533163283266773\n",
            "Total Loss: 0.004542561843589761\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004154219447216335\n",
            "SCOPE mean: 0.11418012153699623, SCOPE var: 0.0005337406457637048\n",
            "Total Loss: 0.00452942150799725\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004142521283262926\n",
            "SCOPE mean: 0.11418506030103098, SCOPE var: 0.000533469447248295\n",
            "Total Loss: 0.004516358221357987\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00413090949849351\n",
            "SCOPE mean: 0.11415344841054344, SCOPE var: 0.0005323132216908827\n",
            "Total Loss: 0.004503195258444914\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004119161483929765\n",
            "SCOPE mean: 0.11418786803582225, SCOPE var: 0.0005305365190892346\n",
            "Total Loss: 0.004489921727482736\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00410738171323775\n",
            "SCOPE mean: 0.11430694948191417, SCOPE var: 0.0005293753577842946\n",
            "Total Loss: 0.004477026640245058\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004095898910909269\n",
            "SCOPE mean: 0.11429293209500699, SCOPE var: 0.0005283561844966849\n",
            "Total Loss: 0.004464300548217525\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0040848442132416125\n",
            "SCOPE mean: 0.11414316296635611, SCOPE var: 0.0005274817804714939\n",
            "Total Loss: 0.004451501871686902\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004073774860578234\n",
            "SCOPE mean: 0.11404846746567392, SCOPE var: 0.0005276766264843095\n",
            "Total Loss: 0.00443872930637273\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004062100959822956\n",
            "SCOPE mean: 0.11419758626699013, SCOPE var: 0.0005278714276381378\n",
            "Total Loss: 0.004426107921753636\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004049869580112263\n",
            "SCOPE mean: 0.11455334369094433, SCOPE var: 0.000528065346805396\n",
            "Total Loss: 0.004413324087249806\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004037382016552487\n",
            "SCOPE mean: 0.11495302023162951, SCOPE var: 0.0005278730537628179\n",
            "Total Loss: 0.004400567481811704\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004025353248404517\n",
            "SCOPE mean: 0.1152138344484724, SCOPE var: 0.0005271500722648332\n",
            "Total Loss: 0.004387906838360933\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004013854481208254\n",
            "SCOPE mean: 0.11534441988891886, SCOPE var: 0.0005266995894741784\n",
            "Total Loss: 0.0043753529684159706\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004002459465236953\n",
            "SCOPE mean: 0.11544750673798451, SCOPE var: 0.0005260474079932982\n",
            "Total Loss: 0.004362943253364402\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003991151621251736\n",
            "SCOPE mean: 0.11554551537072935, SCOPE var: 0.0005250320489034368\n",
            "Total Loss: 0.004350460375188757\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0039800935865910115\n",
            "SCOPE mean: 0.11561265739463857, SCOPE var: 0.0005241141331425015\n",
            "Total Loss: 0.004338048172244537\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003969283770974254\n",
            "SCOPE mean: 0.11570109076345259, SCOPE var: 0.0005246825758662626\n",
            "Total Loss: 0.0043260665860284505\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003958506952865037\n",
            "SCOPE mean: 0.11579284048648428, SCOPE var: 0.000524871299460843\n",
            "Total Loss: 0.004314013015851562\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0039477660402100516\n",
            "SCOPE mean: 0.11584998955222145, SCOPE var: 0.0005243369054029468\n",
            "Total Loss: 0.0043018156790485765\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003937110118645815\n",
            "SCOPE mean: 0.11585524490272624, SCOPE var: 0.0005228768488605359\n",
            "Total Loss: 0.004289953313584786\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00392670673270185\n",
            "SCOPE mean: 0.11570733946418468, SCOPE var: 0.0005207623526717818\n",
            "Total Loss: 0.0042779807163565425\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00391643852952401\n",
            "SCOPE mean: 0.11554555442862381, SCOPE var: 0.0005199063967021036\n",
            "Total Loss: 0.004266340505983248\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003905521937962358\n",
            "SCOPE mean: 0.11562640490081333, SCOPE var: 0.0005200596370348819\n",
            "Total Loss: 0.0042547144423891875\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003894153321806543\n",
            "SCOPE mean: 0.11586962408243258, SCOPE var: 0.0005203588856452026\n",
            "Total Loss: 0.004243022077581513\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003882656531925042\n",
            "SCOPE mean: 0.11612585486591347, SCOPE var: 0.0005200138216314106\n",
            "Total Loss: 0.00423143150342771\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038718378755254763\n",
            "SCOPE mean: 0.11615728828933752, SCOPE var: 0.0005205249692945378\n",
            "Total Loss: 0.00421997909364118\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038616825193879085\n",
            "SCOPE mean: 0.11591142924361618, SCOPE var: 0.000520313796892095\n",
            "Total Loss: 0.004208493358895321\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038517630654690137\n",
            "SCOPE mean: 0.1156232655596763, SCOPE var: 0.0005191364602020358\n",
            "Total Loss: 0.004197279396108159\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038415763589836555\n",
            "SCOPE mean: 0.11542903389832397, SCOPE var: 0.0005171322924300443\n",
            "Total Loss: 0.004186023120059118\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003831081811401098\n",
            "SCOPE mean: 0.11538526261952889, SCOPE var: 0.0005149972934051101\n",
            "Total Loss: 0.004174810419512182\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003820715561364861\n",
            "SCOPE mean: 0.11541978770751517, SCOPE var: 0.0005145363408950866\n",
            "Total Loss: 0.004163842806127943\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003810601513275325\n",
            "SCOPE mean: 0.11538896039747626, SCOPE var: 0.0005139337305277606\n",
            "Total Loss: 0.004152821335547692\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038008050783618704\n",
            "SCOPE mean: 0.11528694748256874, SCOPE var: 0.0005134140975613258\n",
            "Total Loss: 0.004141945293947652\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003791374821782334\n",
            "SCOPE mean: 0.11511239953813993, SCOPE var: 0.0005126306402438782\n",
            "Total Loss: 0.004131167261738804\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037820756963251215\n",
            "SCOPE mean: 0.11493451343462961, SCOPE var: 0.0005115809044319075\n",
            "Total Loss: 0.004120456952663988\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037730655506846974\n",
            "SCOPE mean: 0.114710926992542, SCOPE var: 0.000510778212201331\n",
            "Total Loss: 0.0041100701668686165\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037637056478372166\n",
            "SCOPE mean: 0.1146623927814968, SCOPE var: 0.0005104300793476568\n",
            "Total Loss: 0.004099603577616541\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037540132924717804\n",
            "SCOPE mean: 0.11476399341048996, SCOPE var: 0.0005100270857131985\n",
            "Total Loss: 0.004089126612335756\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003744092370893932\n",
            "SCOPE mean: 0.11505487648010376, SCOPE var: 0.0005109436872345721\n",
            "Total Loss: 0.004078946166101238\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037347000647350013\n",
            "SCOPE mean: 0.11508754666835855, SCOPE var: 0.0005104035535603874\n",
            "Total Loss: 0.004068668812632026\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037257488472697874\n",
            "SCOPE mean: 0.11497708463598297, SCOPE var: 0.0005094097375254585\n",
            "Total Loss: 0.004058578103881437\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037169449799253304\n",
            "SCOPE mean: 0.11480695832859124, SCOPE var: 0.0005090498786486861\n",
            "Total Loss: 0.004048492874746688\n",
            "----------------------------------------\n",
            "Epoch 201\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037089645790254733\n",
            "SCOPE mean: 0.11455486134870832, SCOPE var: 0.0005076344687968665\n",
            "Total Loss: 0.004038632156350226\n",
            "----------------------------------------\n",
            "Epoch 202\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003700626660657391\n",
            "SCOPE mean: 0.11444632525185908, SCOPE var: 0.0005075518364620552\n",
            "Total Loss: 0.004029010175536322\n",
            "----------------------------------------\n",
            "Epoch 203\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003692075598394077\n",
            "SCOPE mean: 0.11441194847772113, SCOPE var: 0.0005071988909747817\n",
            "Total Loss: 0.004019359270121135\n",
            "----------------------------------------\n",
            "Epoch 204\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036833024136455153\n",
            "SCOPE mean: 0.11442125191858744, SCOPE var: 0.0005062901697527227\n",
            "Total Loss: 0.0040096404543144264\n",
            "----------------------------------------\n",
            "Epoch 205\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036744964956590513\n",
            "SCOPE mean: 0.1144625090676489, SCOPE var: 0.0005057179891508552\n",
            "Total Loss: 0.0039999791293551515\n",
            "----------------------------------------\n",
            "Epoch 206\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003665617095156695\n",
            "SCOPE mean: 0.11449968465077895, SCOPE var: 0.000505971282974418\n",
            "Total Loss: 0.003990289229778813\n",
            "----------------------------------------\n",
            "Epoch 207\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003656738568175196\n",
            "SCOPE mean: 0.11440563303465526, SCOPE var: 0.0005057840443063353\n",
            "Total Loss: 0.003980600096311573\n",
            "----------------------------------------\n",
            "Epoch 208\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036478613474858804\n",
            "SCOPE mean: 0.11427691462834445, SCOPE var: 0.000505478620014104\n",
            "Total Loss: 0.003970818509728017\n",
            "----------------------------------------\n",
            "Epoch 209\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036389953861819405\n",
            "SCOPE mean: 0.11423360075223864, SCOPE var: 0.0005064218851998642\n",
            "Total Loss: 0.0039612096025168604\n",
            "----------------------------------------\n",
            "Epoch 210\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036304330722178616\n",
            "SCOPE mean: 0.11406758723278049, SCOPE var: 0.0005061964172476528\n",
            "Total Loss: 0.0039515454054529185\n",
            "----------------------------------------\n",
            "Epoch 211\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036221380725260694\n",
            "SCOPE mean: 0.11379634559569399, SCOPE var: 0.0005049416984031687\n",
            "Total Loss: 0.003941904932416128\n",
            "----------------------------------------\n",
            "Epoch 212\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036139943732051297\n",
            "SCOPE mean: 0.11346391217443602, SCOPE var: 0.0005029212521752795\n",
            "Total Loss: 0.00393229431477194\n",
            "----------------------------------------\n",
            "Epoch 213\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003605929919104693\n",
            "SCOPE mean: 0.11318961208501646, SCOPE var: 0.0005024442157131281\n",
            "Total Loss: 0.003922907405808385\n",
            "----------------------------------------\n",
            "Epoch 214\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035983819512767964\n",
            "SCOPE mean: 0.11294660631241829, SCOPE var: 0.0005019687596739552\n",
            "Total Loss: 0.003913555746613191\n",
            "----------------------------------------\n",
            "Epoch 215\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035905278566260284\n",
            "SCOPE mean: 0.11275635187227344, SCOPE var: 0.0005015840460887167\n",
            "Total Loss: 0.0039042294246550346\n",
            "----------------------------------------\n",
            "Epoch 216\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035823421727627317\n",
            "SCOPE mean: 0.112680217000547, SCOPE var: 0.0005012903776623307\n",
            "Total Loss: 0.003894912379495146\n",
            "----------------------------------------\n",
            "Epoch 217\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035744115353653783\n",
            "SCOPE mean: 0.11259753334053803, SCOPE var: 0.0005014974517086493\n",
            "Total Loss: 0.003885809237985958\n",
            "----------------------------------------\n",
            "Epoch 218\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003566807712521205\n",
            "SCOPE mean: 0.1123714109098958, SCOPE var: 0.0005005005005945858\n",
            "Total Loss: 0.003876703549647433\n",
            "----------------------------------------\n",
            "Epoch 219\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003559349369635343\n",
            "SCOPE mean: 0.1120894999007132, SCOPE var: 0.0004994617588934743\n",
            "Total Loss: 0.003867522203621912\n",
            "----------------------------------------\n",
            "Epoch 220\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035519194477429336\n",
            "SCOPE mean: 0.11179755569443532, SCOPE var: 0.0004985197805916795\n",
            "Total Loss: 0.003858509498347675\n",
            "----------------------------------------\n",
            "Epoch 221\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035444201193574982\n",
            "SCOPE mean: 0.1115994387947162, SCOPE var: 0.0004989293427370487\n",
            "Total Loss: 0.003849632190576137\n",
            "----------------------------------------\n",
            "Epoch 222\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003536628020297409\n",
            "SCOPE mean: 0.11146452120343547, SCOPE var: 0.0004984222401684932\n",
            "Total Loss: 0.003840770874434393\n",
            "----------------------------------------\n",
            "Epoch 223\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035287752796261686\n",
            "SCOPE mean: 0.11132906439078219, SCOPE var: 0.0004967779438875516\n",
            "Total Loss: 0.003831964086302159\n",
            "----------------------------------------\n",
            "Epoch 224\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0035208086781307506\n",
            "SCOPE mean: 0.11131763497670649, SCOPE var: 0.0004965060956262731\n",
            "Total Loss: 0.0038232557977384667\n",
            "----------------------------------------\n",
            "Epoch 225\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003512735775321308\n",
            "SCOPE mean: 0.11131057297336713, SCOPE var: 0.0004960379750898464\n",
            "Total Loss: 0.0038145050295923764\n",
            "----------------------------------------\n",
            "Epoch 226\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003504601671321963\n",
            "SCOPE mean: 0.11136313153818819, SCOPE var: 0.0004966188806611834\n",
            "Total Loss: 0.0038058263723023244\n",
            "----------------------------------------\n",
            "Epoch 227\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003496296902952608\n",
            "SCOPE mean: 0.1113755753183726, SCOPE var: 0.0004960035255674248\n",
            "Total Loss: 0.003798041123969554\n",
            "----------------------------------------\n",
            "Epoch 228\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003489207393489814\n",
            "SCOPE mean: 0.11132952918406902, SCOPE var: 0.0004945601186649349\n",
            "Total Loss: 0.0037890122375952083\n",
            "----------------------------------------\n",
            "Epoch 229\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034822380273940834\n",
            "SCOPE mean: 0.11131078695919726, SCOPE var: 0.0004937765117496031\n",
            "Total Loss: 0.003781192947866886\n",
            "----------------------------------------\n",
            "Epoch 230\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034751259060845758\n",
            "SCOPE mean: 0.11125518152099585, SCOPE var: 0.0004928576954111133\n",
            "Total Loss: 0.0037735046104241258\n",
            "----------------------------------------\n",
            "Epoch 231\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034680476781597308\n",
            "SCOPE mean: 0.11118769389633017, SCOPE var: 0.0004919256066373882\n",
            "Total Loss: 0.0037658668889975285\n",
            "----------------------------------------\n",
            "Epoch 232\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034608103790738382\n",
            "SCOPE mean: 0.11124678827813195, SCOPE var: 0.0004924876867569955\n",
            "Total Loss: 0.003758228938981532\n",
            "----------------------------------------\n",
            "Epoch 233\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003453415274103776\n",
            "SCOPE mean: 0.11129201409006655, SCOPE var: 0.000491927085453433\n",
            "Total Loss: 0.003750493565762601\n",
            "----------------------------------------\n",
            "Epoch 234\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034461088819796714\n",
            "SCOPE mean: 0.11130911503917598, SCOPE var: 0.0004908696393482137\n",
            "Total Loss: 0.003742718287731717\n",
            "----------------------------------------\n",
            "Epoch 235\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034389346138359346\n",
            "SCOPE mean: 0.11133613724910356, SCOPE var: 0.0004904060033257918\n",
            "Total Loss: 0.003734968229468264\n",
            "----------------------------------------\n",
            "Epoch 236\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034318045390589855\n",
            "SCOPE mean: 0.11134442558527344, SCOPE var: 0.0004897027501427268\n",
            "Total Loss: 0.0037272949227774204\n",
            "----------------------------------------\n",
            "Epoch 237\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003424848193437104\n",
            "SCOPE mean: 0.11132938674246844, SCOPE var: 0.0004891335097340508\n",
            "Total Loss: 0.0037196315349971804\n",
            "----------------------------------------\n",
            "Epoch 238\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003417927278027008\n",
            "SCOPE mean: 0.11130741679667137, SCOPE var: 0.0004885444823698622\n",
            "Total Loss: 0.003712206680856404\n",
            "----------------------------------------\n",
            "Epoch 239\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003410635189112273\n",
            "SCOPE mean: 0.11135771559678379, SCOPE var: 0.0004892114541839431\n",
            "Total Loss: 0.003705951572397328\n",
            "----------------------------------------\n",
            "Epoch 240\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0034036041143461595\n",
            "SCOPE mean: 0.11138059856379501, SCOPE var: 0.0004884570628285333\n",
            "Total Loss: 0.0036990783453821514\n",
            "----------------------------------------\n",
            "Epoch 241\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033973217528789103\n",
            "SCOPE mean: 0.11140834729387591, SCOPE var: 0.00048668910660541195\n",
            "Total Loss: 0.0036919325102866976\n",
            "----------------------------------------\n",
            "Epoch 242\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033926299947161557\n",
            "SCOPE mean: 0.11111321032074174, SCOPE var: 0.0004851908008565171\n",
            "Total Loss: 0.0036843436147440908\n",
            "----------------------------------------\n",
            "Epoch 243\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033876237821783978\n",
            "SCOPE mean: 0.11089753222679824, SCOPE var: 0.0004836434533764017\n",
            "Total Loss: 0.003677241550807619\n",
            "----------------------------------------\n",
            "Epoch 244\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033816071773666878\n",
            "SCOPE mean: 0.11084820152127843, SCOPE var: 0.0004828200902376542\n",
            "Total Loss: 0.0036708193930577023\n",
            "----------------------------------------\n",
            "Epoch 245\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033751186155838546\n",
            "SCOPE mean: 0.1109046532729768, SCOPE var: 0.00048234601712448524\n",
            "Total Loss: 0.003664381485270579\n",
            "----------------------------------------\n",
            "Epoch 246\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003368452294529122\n",
            "SCOPE mean: 0.11098387496555873, SCOPE var: 0.00048198216782525445\n",
            "Total Loss: 0.0036578971274542553\n",
            "----------------------------------------\n",
            "Epoch 247\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033618939418424026\n",
            "SCOPE mean: 0.11101211864190591, SCOPE var: 0.0004815494074357658\n",
            "Total Loss: 0.003651421619332001\n",
            "----------------------------------------\n",
            "Epoch 248\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003356622053989295\n",
            "SCOPE mean: 0.11057397465194256, SCOPE var: 0.00048016580301992563\n",
            "Total Loss: 0.0036448283077642404\n",
            "----------------------------------------\n",
            "Epoch 249\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033511654999372363\n",
            "SCOPE mean: 0.11022324896459886, SCOPE var: 0.0004781326343636653\n",
            "Total Loss: 0.0036382803806929245\n",
            "----------------------------------------\n",
            "Epoch 250\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033452513908142434\n",
            "SCOPE mean: 0.11013243766135593, SCOPE var: 0.0004774221680654246\n",
            "Total Loss: 0.0036316940977920995\n",
            "----------------------------------------\n",
            "Epoch 251\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00333893116522179\n",
            "SCOPE mean: 0.11022115258938682, SCOPE var: 0.00047702121577609984\n",
            "Total Loss: 0.0036251091192570214\n",
            "----------------------------------------\n",
            "Epoch 252\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033335703457317302\n",
            "SCOPE mean: 0.1100800795492888, SCOPE var: 0.00047680495272259493\n",
            "Total Loss: 0.0036188782420919956\n",
            "----------------------------------------\n",
            "Epoch 253\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003327814708305514\n",
            "SCOPE mean: 0.11005091366760104, SCOPE var: 0.0004756194566598206\n",
            "Total Loss: 0.0036127457719491813\n",
            "----------------------------------------\n",
            "Epoch 254\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003322178246438999\n",
            "SCOPE mean: 0.10979009006329483, SCOPE var: 0.0004743048632708876\n",
            "Total Loss: 0.003607869527029904\n",
            "----------------------------------------\n",
            "Epoch 255\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033160756587071747\n",
            "SCOPE mean: 0.10984176317359552, SCOPE var: 0.0004735742832689878\n",
            "Total Loss: 0.003601991228521159\n",
            "----------------------------------------\n",
            "Epoch 256\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033096358824706677\n",
            "SCOPE mean: 0.11013575777293663, SCOPE var: 0.00047316297946679853\n",
            "Total Loss: 0.003596152470577346\n",
            "----------------------------------------\n",
            "Epoch 257\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003304616234030783\n",
            "SCOPE mean: 0.11000617201974801, SCOPE var: 0.0004718307467992579\n",
            "Total Loss: 0.0035899152408118643\n",
            "----------------------------------------\n",
            "Epoch 258\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0033010184980499417\n",
            "SCOPE mean: 0.10957808428418547, SCOPE var: 0.0004698534006413414\n",
            "Total Loss: 0.0035835383837307232\n",
            "----------------------------------------\n",
            "Epoch 259\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032961731564724764\n",
            "SCOPE mean: 0.10946898779225869, SCOPE var: 0.00046917959833701726\n",
            "Total Loss: 0.003578236009450715\n",
            "----------------------------------------\n",
            "Epoch 260\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032902371891839156\n",
            "SCOPE mean: 0.10965419723879032, SCOPE var: 0.00046910226260110356\n",
            "Total Loss: 0.003572912866634918\n",
            "----------------------------------------\n",
            "Epoch 261\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032837421174130503\n",
            "SCOPE mean: 0.10990659473474411, SCOPE var: 0.0004674741662148682\n",
            "Total Loss: 0.003567486475065864\n",
            "----------------------------------------\n",
            "Epoch 262\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003278514819829715\n",
            "SCOPE mean: 0.10986331139174915, SCOPE var: 0.0004665930882718081\n",
            "Total Loss: 0.003562268055764728\n",
            "----------------------------------------\n",
            "Epoch 263\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003274268744462606\n",
            "SCOPE mean: 0.10948361297223515, SCOPE var: 0.0004648252288720512\n",
            "Total Loss: 0.0035568985863617844\n",
            "----------------------------------------\n",
            "Epoch 264\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003269446736010101\n",
            "SCOPE mean: 0.10927379844866963, SCOPE var: 0.0004634886832586353\n",
            "Total Loss: 0.003551451693970697\n",
            "----------------------------------------\n",
            "Epoch 265\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032638437096935155\n",
            "SCOPE mean: 0.10928885261797014, SCOPE var: 0.00046271229056186845\n",
            "Total Loss: 0.0035457114889596232\n",
            "----------------------------------------\n",
            "Epoch 266\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032577622221575835\n",
            "SCOPE mean: 0.10949396681901574, SCOPE var: 0.0004625301419378711\n",
            "Total Loss: 0.003540207326189779\n",
            "----------------------------------------\n",
            "Epoch 267\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032527105241967422\n",
            "SCOPE mean: 0.10940153967409832, SCOPE var: 0.0004615613679753888\n",
            "Total Loss: 0.003534632122998022\n",
            "----------------------------------------\n",
            "Epoch 268\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032486258190690277\n",
            "SCOPE mean: 0.10909319566071406, SCOPE var: 0.00046037825046824933\n",
            "Total Loss: 0.0035289704510119\n",
            "----------------------------------------\n",
            "Epoch 269\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032437282890224062\n",
            "SCOPE mean: 0.10885860966089572, SCOPE var: 0.00046062884145950226\n",
            "Total Loss: 0.003524673730081479\n",
            "----------------------------------------\n",
            "Epoch 270\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003236913080560749\n",
            "SCOPE mean: 0.10917110163976519, SCOPE var: 0.00046084083365614017\n",
            "Total Loss: 0.003519382582904368\n",
            "----------------------------------------\n",
            "Epoch 271\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032298318884218854\n",
            "SCOPE mean: 0.10982136902635943, SCOPE var: 0.00046050371201688357\n",
            "Total Loss: 0.003514445634034755\n",
            "----------------------------------------\n",
            "Epoch 272\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032247971186454864\n",
            "SCOPE mean: 0.10993815993240302, SCOPE var: 0.00045826352914294836\n",
            "Total Loss: 0.003509413859195904\n",
            "----------------------------------------\n",
            "Epoch 273\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032211387734305754\n",
            "SCOPE mean: 0.1095834159889871, SCOPE var: 0.00045625492492352044\n",
            "Total Loss: 0.0035040319968011164\n",
            "----------------------------------------\n",
            "Epoch 274\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003217405449340008\n",
            "SCOPE mean: 0.10934190693334403, SCOPE var: 0.0004547205920060871\n",
            "Total Loss: 0.0034991903407587954\n",
            "----------------------------------------\n",
            "Epoch 275\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0032129808853396586\n",
            "SCOPE mean: 0.10934999465394162, SCOPE var: 0.00045356230863756485\n",
            "Total Loss: 0.0034943184101685182\n",
            "----------------------------------------\n",
            "Epoch 276\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003207733649325763\n",
            "SCOPE mean: 0.1095865232790687, SCOPE var: 0.0004531458849672159\n",
            "Total Loss: 0.0034894752607520065\n",
            "----------------------------------------\n",
            "Epoch 277\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003202892134805878\n",
            "SCOPE mean: 0.10954794422343724, SCOPE var: 0.000451173664400251\n",
            "Total Loss: 0.0034846917432758764\n",
            "----------------------------------------\n",
            "Epoch 278\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031971508561392968\n",
            "SCOPE mean: 0.10931107196263123, SCOPE var: 0.0004490144461088967\n",
            "Total Loss: 0.0034797180310631656\n",
            "----------------------------------------\n",
            "Epoch 279\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031921688694955637\n",
            "SCOPE mean: 0.10891814104157452, SCOPE var: 0.00044603399660223355\n",
            "Total Loss: 0.003475538311355147\n",
            "----------------------------------------\n",
            "Epoch 280\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031859466213652082\n",
            "SCOPE mean: 0.10897085068246974, SCOPE var: 0.0004438545700450543\n",
            "Total Loss: 0.0034706798284818603\n",
            "----------------------------------------\n",
            "Epoch 281\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031793083459018285\n",
            "SCOPE mean: 0.10932880901223666, SCOPE var: 0.0004420201947170238\n",
            "Total Loss: 0.0034659809365336866\n",
            "----------------------------------------\n",
            "Epoch 282\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003174609008123857\n",
            "SCOPE mean: 0.10918475642272894, SCOPE var: 0.0004387761159112748\n",
            "Total Loss: 0.00346142243936004\n",
            "----------------------------------------\n",
            "Epoch 283\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031716142120065205\n",
            "SCOPE mean: 0.10865965830500815, SCOPE var: 0.00043592719585610235\n",
            "Total Loss: 0.0034564926024816716\n",
            "----------------------------------------\n",
            "Epoch 284\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031698806997014577\n",
            "SCOPE mean: 0.10790213910020777, SCOPE var: 0.00043178431101337103\n",
            "Total Loss: 0.0034523528288781177\n",
            "----------------------------------------\n",
            "Epoch 285\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031660775511842204\n",
            "SCOPE mean: 0.10772824316812257, SCOPE var: 0.0004277397789931165\n",
            "Total Loss: 0.003447663013726553\n",
            "----------------------------------------\n",
            "Epoch 286\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031604861998375993\n",
            "SCOPE mean: 0.10804506053282945, SCOPE var: 0.00042437113531258296\n",
            "Total Loss: 0.0034428142898696354\n",
            "----------------------------------------\n",
            "Epoch 287\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003155721846271564\n",
            "SCOPE mean: 0.10822407668360001, SCOPE var: 0.0004219552084739115\n",
            "Total Loss: 0.003438697989478186\n",
            "----------------------------------------\n",
            "Epoch 288\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031519590373538897\n",
            "SCOPE mean: 0.10801860635093222, SCOPE var: 0.0004189498578038439\n",
            "Total Loss: 0.003434266374189119\n",
            "----------------------------------------\n",
            "Epoch 289\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003149113888533641\n",
            "SCOPE mean: 0.10748550285692286, SCOPE var: 0.0004151207273602356\n",
            "Total Loss: 0.0034296439811756486\n",
            "----------------------------------------\n",
            "Epoch 290\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031465303891793136\n",
            "SCOPE mean: 0.10690687432495993, SCOPE var: 0.0004114358459345603\n",
            "Total Loss: 0.003425635745630474\n",
            "----------------------------------------\n",
            "Epoch 291\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003141697027275614\n",
            "SCOPE mean: 0.10695206785634484, SCOPE var: 0.00040927544432987944\n",
            "Total Loss: 0.0034209276103321583\n",
            "----------------------------------------\n",
            "Epoch 292\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031359850150105974\n",
            "SCOPE mean: 0.10731822915341914, SCOPE var: 0.0004082307236400817\n",
            "Total Loss: 0.0034170715153243173\n",
            "----------------------------------------\n",
            "Epoch 293\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031304832984235683\n",
            "SCOPE mean: 0.10735250151616733, SCOPE var: 0.0004069746505934338\n",
            "Total Loss: 0.0034124188472649335\n",
            "----------------------------------------\n",
            "Epoch 294\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031263640638852183\n",
            "SCOPE mean: 0.10700663805016301, SCOPE var: 0.0004043013204994281\n",
            "Total Loss: 0.0034079421766915353\n",
            "----------------------------------------\n",
            "Epoch 295\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031234786509992817\n",
            "SCOPE mean: 0.1064446107728887, SCOPE var: 0.0004007220250049498\n",
            "Total Loss: 0.003404069432304795\n",
            "----------------------------------------\n",
            "Epoch 296\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031196524734891256\n",
            "SCOPE mean: 0.10624818434250706, SCOPE var: 0.00039817040866139465\n",
            "Total Loss: 0.0034001133996158584\n",
            "----------------------------------------\n",
            "Epoch 297\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031141967650172727\n",
            "SCOPE mean: 0.10661256538512091, SCOPE var: 0.0003973077739936152\n",
            "Total Loss: 0.00339566173664373\n",
            "----------------------------------------\n",
            "Epoch 298\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0031089150758450623\n",
            "SCOPE mean: 0.10711522123448049, SCOPE var: 0.0003967202905846069\n",
            "Total Loss: 0.003391969502592861\n",
            "----------------------------------------\n",
            "Epoch 299\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003105520132431105\n",
            "SCOPE mean: 0.10717766164191846, SCOPE var: 0.0003960984054795584\n",
            "Total Loss: 0.003387937704849216\n",
            "----------------------------------------\n",
            "Epoch 300\n",
            "IS variance:  tensor(4.8629e-09, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003103802907982472\n",
            "SCOPE mean: 0.10673434170837459, SCOPE var: 0.0003937199902593215\n",
            "Total Loss: 0.0033835209443496936\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.0877, -0.1790],\n",
            "        [-0.0263, -0.0670],\n",
            "        [ 0.6181,  0.2938],\n",
            "        [-0.6721,  0.1251],\n",
            "        [ 0.8347, -0.1682],\n",
            "        [-0.0077, -0.1497],\n",
            "        [-0.6120,  0.0066],\n",
            "        [-0.0453,  0.2553],\n",
            "        [ 0.7401,  0.1473],\n",
            "        [ 0.1039,  0.2418],\n",
            "        [-0.5825, -0.5235],\n",
            "        [-0.5225, -0.4385],\n",
            "        [-0.7701, -0.0069],\n",
            "        [ 0.3131, -0.3647],\n",
            "        [ 0.7306, -0.1804],\n",
            "        [ 0.2702,  0.0505]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.8951,  0.3808, -0.5925,  0.4947, -0.1630,  0.7495, -0.6856,  0.3197,\n",
            "        -0.3123, -0.6056, -0.1913, -0.3826,  0.3940,  0.6258,  0.1614, -0.7394],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[-1.6561e-01, -4.9582e-01, -2.1159e-01, -1.5096e-01,  1.5757e-01,\n",
            "         -2.2262e-01,  1.4583e-01,  1.5295e-01, -3.8207e-01, -5.4891e-02,\n",
            "          2.4297e-01, -1.3673e-01, -4.4588e-02, -2.1587e-01, -5.0986e-02,\n",
            "         -3.2025e-02],\n",
            "        [ 4.7171e-01, -3.2169e-01, -5.2778e-04, -2.1516e-01, -3.5009e-01,\n",
            "         -4.0657e-01, -1.4454e-01,  3.4643e-02, -1.7623e-01,  6.6261e-02,\n",
            "         -2.2206e-01, -2.4215e-01,  2.8475e-01, -1.4135e-01, -1.5923e-01,\n",
            "         -1.6243e-01],\n",
            "        [ 4.6153e-01,  7.3011e-01, -1.8013e-02, -6.0121e-02, -4.4696e-02,\n",
            "          2.9997e-01,  2.2910e-01,  4.1706e-02, -4.5291e-02, -4.7610e-02,\n",
            "          7.8982e-02, -9.0411e-02,  1.7150e-01,  1.6771e-01, -1.8214e-02,\n",
            "         -2.9280e-01],\n",
            "        [-8.6335e-02,  1.1694e-01, -6.2918e-02, -2.7560e-01, -2.8005e-01,\n",
            "          2.3557e-01, -1.4361e-01, -4.0142e-02, -2.2938e-01, -2.1906e-01,\n",
            "         -7.3527e-02, -1.6425e-01, -1.0430e-01,  3.5750e-02,  1.9500e-01,\n",
            "         -1.7599e-01],\n",
            "        [ 7.3969e-02,  3.0852e-01, -2.6388e-02, -1.2095e-01, -7.5499e-03,\n",
            "          1.8529e-02, -1.6649e-01,  8.0963e-04, -4.0672e-02, -4.6442e-02,\n",
            "          1.2140e-01, -1.0947e-02,  2.5067e-01,  7.9171e-02,  1.5540e-02,\n",
            "         -1.8668e-01],\n",
            "        [ 6.5403e-01,  5.7493e-01,  4.8159e-03,  1.8599e-01, -2.5408e-02,\n",
            "          4.3112e-01, -1.1532e-02,  9.0374e-03,  1.1662e-02, -2.2571e-02,\n",
            "          2.2387e-01,  1.0505e-01,  1.1700e-01, -1.7120e-01, -3.1277e-02,\n",
            "         -1.6742e-01],\n",
            "        [ 1.4644e-01, -3.7668e-03,  2.1390e-02,  6.6828e-02, -1.1666e-01,\n",
            "         -1.4534e-02, -2.3978e-01, -2.5528e-01,  4.9150e-02, -5.4667e-02,\n",
            "          2.0770e-01, -6.2762e-02,  2.4595e-01, -6.1009e-01, -1.4962e-01,\n",
            "         -8.6776e-02],\n",
            "        [ 6.4539e-02,  7.3758e-02,  8.3243e-03,  1.3023e-01, -5.3837e-02,\n",
            "          9.2122e-02,  1.0823e-01, -7.1207e-02, -1.4976e-02, -6.7318e-02,\n",
            "         -1.1448e-01, -2.7665e-02, -7.5748e-02,  1.7401e-01, -4.0851e-02,\n",
            "         -2.2969e-01],\n",
            "        [-2.9038e-01, -3.8443e-01, -4.1356e-01,  1.3129e-01, -3.8546e-02,\n",
            "         -7.7533e-01,  3.3424e-02,  1.3640e-01,  8.5833e-02, -1.1520e-01,\n",
            "         -2.0898e-01,  1.0012e-01, -1.6629e-01, -4.5634e-01,  2.6882e-01,\n",
            "          8.0638e-02],\n",
            "        [ 3.6873e-01,  2.2938e-01,  1.9040e-02, -5.4091e-02, -3.6043e-01,\n",
            "          5.2373e-01,  1.5495e-01,  4.4765e-02, -4.5602e-02,  1.5383e-01,\n",
            "         -1.1588e-01,  2.0708e-01,  3.1877e-01, -7.7033e-01, -3.1158e-01,\n",
            "         -5.0315e-01],\n",
            "        [-2.3802e-01, -2.2495e-01,  1.4086e-01, -4.5303e-01, -1.4097e-01,\n",
            "         -1.6973e-01,  1.4947e-02, -6.1115e-02, -2.5137e-01, -4.7358e-02,\n",
            "         -1.8084e-01,  1.7568e-01, -3.6769e-01, -1.8258e-01, -1.7821e-03,\n",
            "         -8.7789e-04],\n",
            "        [ 5.6525e-01,  1.8706e-01, -4.0367e-02,  1.8563e-02, -4.4220e-02,\n",
            "          6.4572e-01, -1.9286e-01,  1.3876e-02, -6.4903e-03,  5.2257e-02,\n",
            "         -2.1854e-02,  6.2092e-03,  3.1173e-01, -1.9014e-01, -6.3095e-02,\n",
            "         -3.9550e-01],\n",
            "        [-6.9680e-01, -4.3778e-01,  4.2209e-02,  2.2431e-02,  5.4967e-02,\n",
            "         -6.9264e-01, -2.0478e-01, -2.3165e-02,  7.2714e-02, -4.4005e-02,\n",
            "         -8.5319e-02,  1.7251e-01, -1.0983e-01, -3.2041e-01,  3.3659e-02,\n",
            "          1.1821e-01],\n",
            "        [-3.5867e-01, -3.7601e-01,  7.1954e-02,  5.3616e-03,  4.5305e-02,\n",
            "         -5.3541e-01, -1.5954e-01,  8.3633e-02, -4.2136e-01, -5.0920e-02,\n",
            "         -1.9452e-01,  1.6456e-01,  4.5148e-02, -5.3399e-02,  1.5514e-01,\n",
            "          1.5522e-01],\n",
            "        [ 2.4572e-01,  4.6240e-01, -3.4734e-02,  2.0392e-01,  2.5266e-02,\n",
            "          1.5208e-01,  1.3956e-01,  4.3363e-02, -1.9352e-02, -1.7897e-01,\n",
            "         -2.4190e-01,  2.4075e-01, -9.1337e-02,  1.6869e-01,  3.0908e-02,\n",
            "         -3.2930e-01],\n",
            "        [-3.7748e-02, -5.2424e-01, -1.7245e-01,  1.3329e-01,  1.1630e-01,\n",
            "         -1.1846e-01, -6.0918e-02, -5.6433e-02, -2.6627e-01, -2.2710e-01,\n",
            "          1.7257e-03,  8.1709e-02,  7.1921e-02, -1.7144e-01, -2.2043e-01,\n",
            "         -3.3642e-01]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.0482, -0.1387,  0.3048, -0.0314,  0.2645,  0.0603,  0.0863,  0.4119,\n",
            "        -0.2643,  0.1872, -0.2422,  0.2689, -0.1611, -0.0052,  0.3076,  0.0811],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[-0.1535,  0.2736,  0.0669, -0.1567,  0.2036,  0.0881,  0.0843,  0.0951,\n",
            "         -0.0909,  0.0482, -0.2397,  0.0859, -0.2133, -0.0811,  0.0866, -0.2398]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.2796], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
              "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_600_random_pi_b.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "3884afa8-d588-4260-fe9b-35481ea89c4a",
        "id": "qxCN1FxM8-7I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c3c24299-f964-4c27-8af7-44053bae0079\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c3c24299-f964-4c27-8af7-44053bae0079\")) {                    Plotly.newPlot(                        \"c3c24299-f964-4c27-8af7-44053bae0079\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[0.8858908164275113,0.7763566550925447,0.7314032283740581,0.67157376332682,0.5805711024235962,0.5037727628680565,0.44525573699577936,0.3980793399780433,0.35090017352644726,0.3286526089832684],[0.8208378017229481,0.7006961628719688,0.6520203298222498,0.5842765666688415,0.4916122427548182,0.4238015848473583,0.37591037646115616,0.3287339794434201,0.3155734207542048,0.30988336250862664],[0.756616207996441,0.6322209075075444,0.5698894828516128,0.490902368615006,0.4063467923806143,0.35374141294426903,0.31167876832317853,0.3044583694487271,0.2799374670040755,0.25423227212437366],[0.6686546969638634,0.5598308739215838,0.48276219293771844,0.3958607805949417,0.33157244942738184,0.28816042006362175,0.26245522518392,0.2373190204469896,0.2151206058929018,0.19292219133881391],[0.5827235905114474,0.4848589805042022,0.4007458107438606,0.29386062735470386,0.2449729833637645,0.22176358585564884,0.19988860311961476,0.1820188108795932,0.1641490186395717,0.1462792263995502],[0.4978584830675801,0.42294359324651276,0.33716113307565954,0.2326297635740599,0.1935979100559494,0.17686184860453166,0.16012578715311393,0.1433897257016963,0.12665366425027852,0.10991760279886081],[0.47782837774799947,0.4099518603277185,0.32066724812299796,0.228573545594032,0.19043944165115534,0.15253359228418503,0.13579753083276735,0.11906146938134968,0.10232540792993197,0.0855893464785143],[0.46873623374058315,0.4015523604842476,0.310220519392063,0.22916657985240255,0.19103247590952588,0.15289837196664918,0.11476426802377254,0.09473321306100307,0.0779971516095854,0.061261090158167664],[0.45964408973316684,0.3988043100595555,0.3128721191044219,0.22975961411077309,0.19162551016789645,0.15349140622501978,0.11535730228214308,0.07722319833926647,0.053668895289238766,0.03693283383782106],[0.45351897957081166,0.3985256474782122,0.3185832475290697,0.23035264836914363,0.19221854442626696,0.15408444048339026,0.11595033654051362,0.07781623259763701,0.03968212865476034,0.012604577517474513]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[0.012604577517474513,0.8858908164275113],\"ticktext\":[0.012604577517474513,0.8858908164275113]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c3c24299-f964-4c27-8af7-44053bae0079');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5adf0e-2ddc-4156-e8ea-2a7847c9637d",
        "id": "fmUGc3Qa8-7J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15891747325670808"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_600_random_pi_b.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2136b70a-a896-4e6a-dad2-b3cb1ccaa447",
        "id": "clQP5lGU8-7J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1061, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model with l2 reg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M7ibGqVUyQ3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CustomizableFeatureNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_prob=0.2, l2_lambda=0.01, dtype=torch.float32):\n",
        "        super(CustomizableFeatureNet, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # Create the hidden layers based on the provided sizes\n",
        "        for in_dim, out_dim in zip([input_dim] + hidden_dims, hidden_dims):\n",
        "            layer = nn.Linear(in_dim, out_dim).to(dtype)\n",
        "            self.hidden_layers.append(layer)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim).to(dtype)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.l2_lambda = l2_lambda\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = F.relu(layer(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def l2_regularization(self):\n",
        "        l2_reg = torch.tensor(0., device=self.output_layer.weight.device)\n",
        "        for layer in self.hidden_layers:\n",
        "            l2_reg += torch.norm(layer.weight)\n",
        "        l2_reg += torch.norm(self.output_layer.weight)\n",
        "        return self.l2_lambda * l2_reg\n"
      ],
      "metadata": {
        "id": "38YlzOUWyTgH"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 800 pi_b top 2"
      ],
      "metadata": {
        "id": "ozpJbcWKSDp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(800, env_50, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 1, 0.05)\n",
        "pi_e = experiment_actions(1000, env_50, P_pi_e)\n",
        "model_800_random_pi_b = CustomizableFeatureNet(input_dim=2, hidden_dims=[10, 10], output_dim=1, dtype = torch.float64, l2_lambda=0.001)\n",
        "test_800_random_pi_b = SCOPE_straight(model_800_random_pi_b, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "leaujDCkSDqA"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b161c2e9-42f8-436c-fcd3-f8e19afc5cb0",
        "id": "8KclL3PQSDqA"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5781, dtype=torch.float64), tensor(0.0772, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "925d6c95-a236-477d-b18f-c24aff9fd864",
        "id": "EXPS_zUeSDqA"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"91c45e49-6185-443c-953e-c9958bf92584\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"91c45e49-6185-443c-953e-c9958bf92584\")) {                    Plotly.newPlot(                        \"91c45e49-6185-443c-953e-c9958bf92584\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,9,26,215,1296,1353,1394,1442,1003,1159,0,24,167,1043,1199,0,436,806,566,1371,0,128,943,1107,302,0,114,216,327,1512,0,875,1040,371,112,0,24,76,117,378,0,1108,379,143,44,0,7,13,32,68,597,961,165,51,14,0,1,6,5,12,130,204,55,19,8,0,1,7,4,16,25,43,12,10,3,0,1,5,6,11,2,13,5,5,1,0,2,4,5,13,0,1,1,1,0,0,1,3,4,10],\"zmax\":1512,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('91c45e49-6185-443c-953e-c9958bf92584');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.train_var_scope(50, 0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773939a1-fb4b-4e96-f261-88064287563e",
        "id": "d9aHaKyqSDqB"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0044, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004218989771201134\n",
            "SCOPE mean: 0.22285763285074944, SCOPE var: 0.016886603818618454\n",
            "Total Loss: 0.008665766088156722\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004043985329936416\n",
            "SCOPE mean: 0.21677965178133518, SCOPE var: 0.01643719341992325\n",
            "Total Loss: 0.004883264246003726\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004032811828319944\n",
            "SCOPE mean: 0.2141315282737515, SCOPE var: 0.016257919950321238\n",
            "Total Loss: 0.00487736908343905\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0040190097982142955\n",
            "SCOPE mean: 0.21347469095999014, SCOPE var: 0.01622654360761761\n",
            "Total Loss: 0.0048701303764725895\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.004007672329430603\n",
            "SCOPE mean: 0.21365403371635572, SCOPE var: 0.016258323140711194\n",
            "Total Loss: 0.004861727701200513\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003998420692612137\n",
            "SCOPE mean: 0.2143371941730964, SCOPE var: 0.0163246239113178\n",
            "Total Loss: 0.004852084073530522\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0039905149291199265\n",
            "SCOPE mean: 0.21538282470245057, SCOPE var: 0.01641750234169797\n",
            "Total Loss: 0.004841258337113035\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.00398379913957755\n",
            "SCOPE mean: 0.21668231306337257, SCOPE var: 0.016531344326348292\n",
            "Total Loss: 0.004829905305720856\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003976850269027913\n",
            "SCOPE mean: 0.21819346745026505, SCOPE var: 0.01666473950460398\n",
            "Total Loss: 0.0048187242718685\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003968780023108742\n",
            "SCOPE mean: 0.219874191700736, SCOPE var: 0.016815586920519054\n",
            "Total Loss: 0.004807301628925931\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003959963254802076\n",
            "SCOPE mean: 0.22160975192604415, SCOPE var: 0.016976224744427008\n",
            "Total Loss: 0.0047955670674282224\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003950640329466868\n",
            "SCOPE mean: 0.2233086897135071, SCOPE var: 0.01713956433593889\n",
            "Total Loss: 0.004783918307200096\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003941387545853619\n",
            "SCOPE mean: 0.224851870710785, SCOPE var: 0.01729596721980582\n",
            "Total Loss: 0.004772256641442606\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0039322569270233585\n",
            "SCOPE mean: 0.22619122795805296, SCOPE var: 0.017440146162482517\n",
            "Total Loss: 0.004760579324187314\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003923320178412601\n",
            "SCOPE mean: 0.22729823086940204, SCOPE var: 0.01756832831775872\n",
            "Total Loss: 0.004748928781912699\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003914511813786572\n",
            "SCOPE mean: 0.22817365259286754, SCOPE var: 0.01767795716756328\n",
            "Total Loss: 0.004737346982908531\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003905496760106183\n",
            "SCOPE mean: 0.2289201934525495, SCOPE var: 0.01777142023864361\n",
            "Total Loss: 0.004726039599191369\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003896084836885273\n",
            "SCOPE mean: 0.22963100522277854, SCOPE var: 0.017850415453862567\n",
            "Total Loss: 0.004714872645496301\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003885921892439823\n",
            "SCOPE mean: 0.2300550145810472, SCOPE var: 0.017896696588604006\n",
            "Total Loss: 0.004703548394087714\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038754532916501017\n",
            "SCOPE mean: 0.23030545985269377, SCOPE var: 0.01791521198204723\n",
            "Total Loss: 0.00469221547680657\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003865147162408521\n",
            "SCOPE mean: 0.2303999744296794, SCOPE var: 0.01790683248478544\n",
            "Total Loss: 0.004681103227671362\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038551531138523803\n",
            "SCOPE mean: 0.23044886728360278, SCOPE var: 0.01788951217166084\n",
            "Total Loss: 0.004670258403642456\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003845913347855735\n",
            "SCOPE mean: 0.23037471125891876, SCOPE var: 0.017860459211872067\n",
            "Total Loss: 0.004659360321660601\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038373940659905616\n",
            "SCOPE mean: 0.23018466440751512, SCOPE var: 0.01782151299312816\n",
            "Total Loss: 0.004648360772119251\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038295054490168575\n",
            "SCOPE mean: 0.22990980874625225, SCOPE var: 0.01777558901187161\n",
            "Total Loss: 0.004637308608896641\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038222010763517625\n",
            "SCOPE mean: 0.22959084035540664, SCOPE var: 0.01772616799861862\n",
            "Total Loss: 0.00462634812229896\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038151270239045764\n",
            "SCOPE mean: 0.2291349946508115, SCOPE var: 0.01766565848558661\n",
            "Total Loss: 0.004615593735790009\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0038080157231529057\n",
            "SCOPE mean: 0.22861673363097598, SCOPE var: 0.017600454981004685\n",
            "Total Loss: 0.004604914414169203\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003800431627432412\n",
            "SCOPE mean: 0.2282582060528699, SCOPE var: 0.017548352654507365\n",
            "Total Loss: 0.004594189425436751\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037925411298710934\n",
            "SCOPE mean: 0.22807957530824724, SCOPE var: 0.01750836323679968\n",
            "Total Loss: 0.004583472994349075\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037842815534426173\n",
            "SCOPE mean: 0.22793201382932435, SCOPE var: 0.017471856448754618\n",
            "Total Loss: 0.004572837652370811\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037760497077957964\n",
            "SCOPE mean: 0.22793998099953186, SCOPE var: 0.01744998845410008\n",
            "Total Loss: 0.004562463423380962\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037684404585538096\n",
            "SCOPE mean: 0.22808583049123854, SCOPE var: 0.017446116996570168\n",
            "Total Loss: 0.00455235438663873\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003761288529232632\n",
            "SCOPE mean: 0.22821921097919717, SCOPE var: 0.017455516075272223\n",
            "Total Loss: 0.004542091418274273\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003753551447878459\n",
            "SCOPE mean: 0.2285034567927583, SCOPE var: 0.017490111721523968\n",
            "Total Loss: 0.004531963090222761\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003744993594231816\n",
            "SCOPE mean: 0.22879609578748236, SCOPE var: 0.01753784913129293\n",
            "Total Loss: 0.004521636602312158\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003736066749635213\n",
            "SCOPE mean: 0.22924128292152696, SCOPE var: 0.0176062512843646\n",
            "Total Loss: 0.004511392187197997\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037268987982296254\n",
            "SCOPE mean: 0.22980453840870424, SCOPE var: 0.0176910662867737\n",
            "Total Loss: 0.004501375661340909\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037185128913487424\n",
            "SCOPE mean: 0.2303362025169938, SCOPE var: 0.017779690460959025\n",
            "Total Loss: 0.004491139739893264\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037108057412312038\n",
            "SCOPE mean: 0.23086242469807103, SCOPE var: 0.017872219171476245\n",
            "Total Loss: 0.004480938097129126\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0037029866554114587\n",
            "SCOPE mean: 0.23141418284358378, SCOPE var: 0.01796838959081834\n",
            "Total Loss: 0.004470937094221566\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036947238455791514\n",
            "SCOPE mean: 0.23181826256354401, SCOPE var: 0.018048260109380865\n",
            "Total Loss: 0.004461100191631557\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003685663975261432\n",
            "SCOPE mean: 0.23217462034614594, SCOPE var: 0.018113434437673298\n",
            "Total Loss: 0.004451201419246511\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036761369430387903\n",
            "SCOPE mean: 0.23259685620538512, SCOPE var: 0.018172890273813767\n",
            "Total Loss: 0.0044412236592985795\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036664236385728984\n",
            "SCOPE mean: 0.23304060467382046, SCOPE var: 0.01822486893209552\n",
            "Total Loss: 0.004431427242798605\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036573836603262886\n",
            "SCOPE mean: 0.23332111927417137, SCOPE var: 0.018260693755445534\n",
            "Total Loss: 0.004421718815886274\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036489940933725484\n",
            "SCOPE mean: 0.2334356407194654, SCOPE var: 0.018281407302041888\n",
            "Total Loss: 0.00441192237360017\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003641016047292704\n",
            "SCOPE mean: 0.233513050991506, SCOPE var: 0.0182935817970373\n",
            "Total Loss: 0.004402356810689165\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.0036323260220775767\n",
            "SCOPE mean: 0.23355418520768673, SCOPE var: 0.018296352023747516\n",
            "Total Loss: 0.004392755037584679\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(0.0772, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.003623236855669545\n",
            "SCOPE mean: 0.2337837208448738, SCOPE var: 0.018311651651489593\n",
            "Total Loss: 0.004383175308128767\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.0612, -0.1346],\n",
            "        [ 0.4817, -0.5663],\n",
            "        [ 0.2014, -0.7416],\n",
            "        [-0.6795, -0.5101],\n",
            "        [ 0.2557, -0.8669],\n",
            "        [ 0.1575, -0.9013],\n",
            "        [-0.3675,  0.0699],\n",
            "        [ 0.1907, -0.8611],\n",
            "        [-0.0591, -0.0361],\n",
            "        [ 0.1867,  0.0654]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.6976, -0.3128, -0.0700, -0.2926, -0.2872, -0.3599, -0.1033, -0.4716,\n",
            "         0.3315,  0.1339], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 0.0478,  0.1619, -0.5250, -0.2989, -0.2221, -0.3277,  0.2590, -0.1465,\n",
            "          0.2154, -0.0704],\n",
            "        [ 0.1312,  0.0170, -0.3354,  0.0075, -0.2703,  0.0066,  0.1464, -0.0836,\n",
            "         -0.2829, -0.2884],\n",
            "        [ 0.1354,  0.0472,  0.4930, -0.0139,  0.2559,  0.7316, -0.0784,  0.5469,\n",
            "         -0.1280, -0.0878],\n",
            "        [-0.0214, -0.0741, -0.4346,  0.2888,  0.1128, -0.3162, -0.2182, -0.7672,\n",
            "         -0.5158,  0.2728],\n",
            "        [-0.0027, -0.0617,  0.2521,  0.1422,  0.4443,  0.1213, -0.2775,  0.7724,\n",
            "         -0.3448, -0.2898],\n",
            "        [ 0.0133,  0.1842, -0.3466,  0.2520, -0.0018, -0.2928, -0.0034,  0.0325,\n",
            "          0.1765, -0.1566],\n",
            "        [-0.2438,  0.0563, -0.2459, -0.2435, -0.0964, -0.3872, -0.2797, -0.1566,\n",
            "         -0.5999,  0.0699],\n",
            "        [ 0.0211,  0.0045, -0.2636,  0.0731, -0.2153, -0.0358, -0.0823, -0.5223,\n",
            "         -0.4867,  0.3853],\n",
            "        [-0.0263, -0.0219, -0.0988, -0.1116,  0.0721, -0.2424,  0.2299, -0.2484,\n",
            "          0.4078,  0.0660],\n",
            "        [ 0.3535, -0.0876, -0.1817, -0.1177,  0.1334, -0.1990, -0.2026,  0.1795,\n",
            "          0.2089,  0.0185]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.1265, -0.2448, -0.2949, -0.2758,  0.1437,  0.2573,  0.0275, -0.4261,\n",
            "         0.1329,  0.0246], dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.1810, -0.1317, -0.3003, -0.4834, -0.2319,  0.1839, -0.1618, -0.4326,\n",
            "          0.2241,  0.5599]], dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.3300], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
              "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f0568bb5-bc68-4331-b6e4-0cdb52339d36",
        "id": "xLcm5WdvSDqB"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"6b197f79-adfe-47c9-b184-efb4b6c00d51\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6b197f79-adfe-47c9-b184-efb4b6c00d51\")) {                    Plotly.newPlot(                        \"6b197f79-adfe-47c9-b184-efb4b6c00d51\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[0.6760104019960433,0.6083830120929246,0.5200582100513629,0.31516317685886536,0.060003685899442716,-0.15983348333614844,-0.34567215022494113,-0.5177900198338858,-0.6899078894428303,-0.8620257590517748],[0.6374998541443884,0.605461887245633,0.5742021694596564,0.5466132024723995,0.5125986629634733,0.4404126686469562,0.42248662470843884,0.3608885454329949,0.2245073742111794,0.024631594535652945],[0.5984178913994906,0.566951339393978,0.5349133724952226,0.5028754055964673,0.46446068814443786,0.41749621108280605,0.3966480736199879,0.37401515693641274,0.3625342828141568,0.36464653856231066],[0.5588129220484035,0.5284407915423229,0.4964028246435676,0.4586416446835142,0.4131553517314275,0.35807799469985013,0.3278194420697342,0.304955707168464,0.28209197226719396,0.25922823736592376],[0.5192079526973163,0.4899302436906679,0.4528843457876549,0.4164880640341343,0.38417499309012815,0.3232431784367781,0.27292365189020856,0.23590172940431403,0.21303799450304398,0.19017425960177387],[0.4796029833462291,0.45600913653324227,0.42854374006898066,0.4096675194480403,0.3628300482068487,0.30189823355349854,0.2409664189001485,0.187769309080567,0.14398401673889394,0.12112028183762394],[0.4655496357474684,0.4405994161038271,0.41436863873782637,0.39922787642664087,0.3414851033235693,0.2805532886702191,0.2197234483011044,0.16462182824576577,0.1086346760388508,0.05806292836650867],[0.4647614611788284,0.4257412088315561,0.4125564550341225,0.38107197309363994,0.3201401584402898,0.2592083437869397,0.20026264109521902,0.14599410735084456,0.0928189483991439,0.037945213044259574],[0.4639732866101884,0.418539783300363,0.4081498570118643,0.3597270282103605,0.2987952135570104,0.23786339890366023,0.18080183388933374,0.12736638645592335,0.07419122750422252,0.021016068552521916],[0.4631851120415484,0.4187984933883074,0.39771021399046486,0.3383820833270811,0.2774502686737309,0.21689433469889646,0.16191382451270298,0.10873866556100217,0.05556350660930143,0.00238834765760082]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[-0.8620257590517748,0.6760104019960433],\"ticktext\":[-0.8620257590517748,0.6760104019960433]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6b197f79-adfe-47c9-b184-efb4b6c00d51');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f52cdfd-fa14-4ccc-a455-56ef2335e86c",
        "id": "4l9AhzcNSDqB"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.720270087636253"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e626c4-9ada-4e50-e1c9-83b56be354af",
        "id": "NbARsqdFSDqB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1058, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 800 pi_b weighted mse"
      ],
      "metadata": {
        "id": "ej6-ESaR1So0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(800, env_50, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 1, 0.05)\n",
        "pi_e = experiment_actions(1000, env_50, P_pi_e)\n",
        "model_800_random_pi_b = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64, l2_lambda=0.002)\n",
        "test_800_random_pi_b = SCOPE_straight(model_800_random_pi_b, 0.99, 10000, pi_b, P_pi_b, P_pi_e, 0.3, dtype = torch.float64)\n"
      ],
      "metadata": {
        "id": "yHebEV-d1So9"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_800_random_pi_b = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 16], output_dim=1, dtype = torch.float64, l2_lambda=0.002)\n"
      ],
      "metadata": {
        "id": "mKtQgu_QN-zH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.IS_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db7eb93-b82e-4844-ec41-21c7bb130b48",
        "id": "8CffL60Y1So9"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2171, dtype=torch.float64), tensor(0.0069, dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.get_state_visitation_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "ef203b5c-cb05-459a-ba73-f8a876b14d11",
        "id": "TfL7Qrkk1So-"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c98d2fed-6d52-42c2-8d37-ec3f28a6f8fd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c98d2fed-6d52-42c2-8d37-ec3f28a6f8fd\")) {                    Plotly.newPlot(                        \"c98d2fed-6d52-42c2-8d37-ec3f28a6f8fd\",                        [{\"colorbar\":{\"title\":{\"text\":\"Visits\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"x\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"y\":[9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0,9,8,7,6,5,4,3,2,1,0],\"z\":[0,0,38,214,1257,1427,1488,1416,970,1182,0,29,189,993,1104,0,485,844,515,1383,0,168,921,1112,328,0,146,242,316,1551,0,853,1021,373,123,0,45,73,121,345,0,1043,383,143,47,0,12,21,28,75,570,961,192,55,17,2,2,4,8,29,136,244,67,16,5,1,1,4,6,24,29,54,17,6,3,2,1,4,7,31,1,13,5,1,1,4,1,6,7,26,0,1,1,0,0,2,1,3,4,18],\"zmax\":1551,\"zmin\":0,\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"State Visitations Heatmap\"},\"xaxis\":{\"title\":{\"text\":\"X-axis\"}},\"yaxis\":{\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"Y-axis\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c98d2fed-6d52-42c2-8d37-ec3f28a6f8fd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.train_var_scope(50, 0.001, 1.05, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c4f509-ef71-474f-88bd-1e7edf1f520c",
        "id": "v2C_ZNpp1So-"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2715759773210386\n",
            "SCOPE mean: 0.2696508115449471, SCOPE var: 0.006249119573549508\n",
            "Total Loss: 0.27522511218355766\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.26279331083988405\n",
            "SCOPE mean: 0.2643576825346261, SCOPE var: 0.006085705937020623\n",
            "Total Loss: 0.2669093205259913\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.26132147548679246\n",
            "SCOPE mean: 0.25856304251984974, SCOPE var: 0.005944873514962677\n",
            "Total Loss: 0.2654164315113085\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2596130212789002\n",
            "SCOPE mean: 0.26101331149016804, SCOPE var: 0.005979567420594783\n",
            "Total Loss: 0.2637220307071859\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2579317290646871\n",
            "SCOPE mean: 0.2652695097286409, SCOPE var: 0.00605964967724669\n",
            "Total Loss: 0.26204912838613326\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2564366091265772\n",
            "SCOPE mean: 0.26842465032807256, SCOPE var: 0.006129274808720348\n",
            "Total Loss: 0.2605428805589829\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.25495761730155775\n",
            "SCOPE mean: 0.26933383489618185, SCOPE var: 0.006155260033565472\n",
            "Total Loss: 0.25903793995769636\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2534058513244786\n",
            "SCOPE mean: 0.2685159607482705, SCOPE var: 0.006140768022021337\n",
            "Total Loss: 0.2574542660125139\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2518832337085601\n",
            "SCOPE mean: 0.2671164946741591, SCOPE var: 0.006103508540650499\n",
            "Total Loss: 0.2558985057430122\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2503994565579994\n",
            "SCOPE mean: 0.26615624538664723, SCOPE var: 0.006065309666973988\n",
            "Total Loss: 0.2543801800187994\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24897604601949117\n",
            "SCOPE mean: 0.2660736887977976, SCOPE var: 0.006043085735809171\n",
            "Total Loss: 0.25292284422282113\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24756137380034324\n",
            "SCOPE mean: 0.26705691735679105, SCOPE var: 0.006043381857499816\n",
            "Total Loss: 0.25147853853586455\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24612532462153824\n",
            "SCOPE mean: 0.26869394429164106, SCOPE var: 0.006059936318653968\n",
            "Total Loss: 0.2500172784131792\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24469443456371703\n",
            "SCOPE mean: 0.27037655740511546, SCOPE var: 0.0060851711081828545\n",
            "Total Loss: 0.24856601490618965\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24327182861809096\n",
            "SCOPE mean: 0.2715915451019262, SCOPE var: 0.006110472592212679\n",
            "Total Loss: 0.2471222573079298\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24184818435524727\n",
            "SCOPE mean: 0.27212758995352, SCOPE var: 0.006129502910924349\n",
            "Total Loss: 0.24567506244515241\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.24043871930685962\n",
            "SCOPE mean: 0.27204588059623436, SCOPE var: 0.0061415634356893995\n",
            "Total Loss: 0.24423930061299917\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2390759084107566\n",
            "SCOPE mean: 0.27197761479621174, SCOPE var: 0.006148834422215803\n",
            "Total Loss: 0.24285082918736364\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2377322343757384\n",
            "SCOPE mean: 0.27211969241855277, SCOPE var: 0.006154042411103607\n",
            "Total Loss: 0.24148649282850654\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2363992844415949\n",
            "SCOPE mean: 0.2726469532977322, SCOPE var: 0.006158942425509145\n",
            "Total Loss: 0.24013629306480475\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.23507877688513673\n",
            "SCOPE mean: 0.2736086278646825, SCOPE var: 0.006165724795636319\n",
            "Total Loss: 0.2387974426812862\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.23374914426870336\n",
            "SCOPE mean: 0.2748421236521624, SCOPE var: 0.006174253943339108\n",
            "Total Loss: 0.2374512710683004\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.23243253627974036\n",
            "SCOPE mean: 0.2761107911708657, SCOPE var: 0.006183598507297739\n",
            "Total Loss: 0.2361179050969962\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.23111641375140196\n",
            "SCOPE mean: 0.27715380198424117, SCOPE var: 0.0061917966877407925\n",
            "Total Loss: 0.23478562639763273\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.229801892377223\n",
            "SCOPE mean: 0.2779872031891552, SCOPE var: 0.006198377313107227\n",
            "Total Loss: 0.23345399444830459\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.22849807580316148\n",
            "SCOPE mean: 0.2786272844320302, SCOPE var: 0.006203647119046486\n",
            "Total Loss: 0.2321324672642535\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2272110534928674\n",
            "SCOPE mean: 0.27898868035053226, SCOPE var: 0.0062065919364898\n",
            "Total Loss: 0.23083034693618354\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.22593581042003663\n",
            "SCOPE mean: 0.27913874018002455, SCOPE var: 0.006206613680688815\n",
            "Total Loss: 0.2295418008760906\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.22469826062295306\n",
            "SCOPE mean: 0.27924653559517226, SCOPE var: 0.006206331113968224\n",
            "Total Loss: 0.2282922056920477\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.22347546375717903\n",
            "SCOPE mean: 0.27964241777050763, SCOPE var: 0.006208479666018642\n",
            "Total Loss: 0.2270580893618555\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.22226754260952983\n",
            "SCOPE mean: 0.28043695682871717, SCOPE var: 0.006214410807054328\n",
            "Total Loss: 0.22583954587986818\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.22108227346880133\n",
            "SCOPE mean: 0.2812755885562299, SCOPE var: 0.0062225451063525035\n",
            "Total Loss: 0.2246441239103597\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21995810001402047\n",
            "SCOPE mean: 0.2816465193411049, SCOPE var: 0.006229239702080295\n",
            "Total Loss: 0.22350834837551095\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21883070567343169\n",
            "SCOPE mean: 0.2821196633674581, SCOPE var: 0.0062357745978599815\n",
            "Total Loss: 0.2223707430329131\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21771992116463337\n",
            "SCOPE mean: 0.2825709035314705, SCOPE var: 0.006242375495307559\n",
            "Total Loss: 0.2212491097410242\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21662727955230204\n",
            "SCOPE mean: 0.28277876765966414, SCOPE var: 0.006249147603185077\n",
            "Total Loss: 0.2201454404375053\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2155499545939051\n",
            "SCOPE mean: 0.28278882107779063, SCOPE var: 0.0062567009099175706\n",
            "Total Loss: 0.21905754893146356\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21449701337383026\n",
            "SCOPE mean: 0.2827727150322942, SCOPE var: 0.006265200357653291\n",
            "Total Loss: 0.2179942260222161\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21344878340941406\n",
            "SCOPE mean: 0.28277060260671644, SCOPE var: 0.00627357437246051\n",
            "Total Loss: 0.21693535207280562\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21241046001934752\n",
            "SCOPE mean: 0.2828170852918703, SCOPE var: 0.006279895267031771\n",
            "Total Loss: 0.2158851208374014\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21138382197628208\n",
            "SCOPE mean: 0.28287988218170995, SCOPE var: 0.0062841260845338115\n",
            "Total Loss: 0.21484665457226448\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.21036358184106776\n",
            "SCOPE mean: 0.28289101388328264, SCOPE var: 0.006285319091910518\n",
            "Total Loss: 0.21381455691350612\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2093483082688184\n",
            "SCOPE mean: 0.2828069608642155, SCOPE var: 0.006283301622202462\n",
            "Total Loss: 0.2127875423246277\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20833369278146094\n",
            "SCOPE mean: 0.2826930982080897, SCOPE var: 0.006283123354188059\n",
            "Total Loss: 0.21176153166928866\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20730763250588302\n",
            "SCOPE mean: 0.28259749413130586, SCOPE var: 0.006288192788513575\n",
            "Total Loss: 0.21072315709512066\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20626650815789901\n",
            "SCOPE mean: 0.2824116985831403, SCOPE var: 0.00629275383908148\n",
            "Total Loss: 0.20966773205458128\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20522023141057114\n",
            "SCOPE mean: 0.28219111002984487, SCOPE var: 0.006298067579264726\n",
            "Total Loss: 0.208604490712524\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.2041766957511775\n",
            "SCOPE mean: 0.28205358328249547, SCOPE var: 0.00630505113457444\n",
            "Total Loss: 0.20754627526498146\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20313793716151268\n",
            "SCOPE mean: 0.28204488202379285, SCOPE var: 0.006313559346752145\n",
            "Total Loss: 0.20649427095753448\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(0.0069, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "MSE loss:  0.20210140360549494\n",
            "SCOPE mean: 0.2824559474145402, SCOPE var: 0.006333582816093562\n",
            "Total Loss: 0.2054468442306137\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[ 3.2347e-01, -2.1520e-01],\n",
            "        [ 4.9993e-01, -5.7918e-01],\n",
            "        [ 1.4809e-01, -2.9132e-01],\n",
            "        [-6.7953e-01, -5.1012e-01],\n",
            "        [ 3.4069e-01, -5.0452e-01],\n",
            "        [ 1.6415e-01, -3.3866e-01],\n",
            "        [-1.8795e-01, -1.4321e-02],\n",
            "        [ 1.2571e-01, -5.1831e-01],\n",
            "        [-1.1126e-01, -1.2516e-01],\n",
            "        [ 7.7277e-02,  3.9222e-04],\n",
            "        [ 2.1652e-01,  9.4888e-02],\n",
            "        [-2.1218e-01, -2.9259e-01],\n",
            "        [-1.7155e-01, -4.9195e-01],\n",
            "        [-2.0662e-01, -5.9314e-01],\n",
            "        [ 7.1473e-02, -1.3163e-01],\n",
            "        [-4.3835e-01,  7.3047e-02]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.4093, -0.8621,  0.2019, -0.4146,  0.3558, -0.1418,  0.4325, -0.4937,\n",
            "         0.6556,  0.1918, -0.7711,  0.0168, -0.4960,  0.1372,  0.6253, -0.1596],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 1.3500e-02, -1.2068e-01,  4.8261e-02,  1.3328e-01,  1.1074e-01,\n",
            "         -6.9963e-02, -1.5433e-02,  2.2130e-01,  3.3609e-01,  1.1816e-01,\n",
            "         -1.1378e-01, -1.7274e-01, -1.2303e-01, -2.1910e-01, -1.2996e-01,\n",
            "          1.0678e-01],\n",
            "        [ 1.0619e-01, -9.3099e-02,  4.5088e-02, -1.9533e-01,  5.4224e-03,\n",
            "          1.4474e-01,  7.9050e-02,  1.7156e-01,  2.9503e-01,  8.1602e-02,\n",
            "         -9.9424e-02, -2.2316e-01,  1.1996e-01,  2.3947e-01, -1.3909e-01,\n",
            "         -1.6049e-01],\n",
            "        [-4.3957e-02,  1.0463e-01, -1.2968e-01,  1.9918e-01,  7.3322e-02,\n",
            "         -1.0445e-01,  1.6782e-01,  1.3510e-01,  2.6950e-01,  1.8804e-02,\n",
            "         -1.1974e-01,  1.6847e-01, -2.1196e-01, -1.9251e-01,  1.5104e-02,\n",
            "         -2.9627e-01],\n",
            "        [-1.0688e-01, -7.4035e-02, -3.8904e-01,  1.8610e-01, -2.9295e-01,\n",
            "         -1.2449e-01, -1.9500e-01, -7.2516e-02, -1.6276e-01,  2.4812e-01,\n",
            "          1.0946e-01, -1.2324e-01, -7.6476e-02,  2.0166e-01, -3.3089e-02,\n",
            "         -6.0043e-02],\n",
            "        [ 3.1173e-02, -1.4571e-01,  6.1853e-02, -2.7436e-02,  1.0350e-01,\n",
            "         -2.6171e-01,  2.8233e-01,  7.0253e-02,  1.6568e-01, -1.3639e-01,\n",
            "         -1.5716e-01, -9.3088e-02,  1.7594e-01, -9.0831e-02,  7.5473e-02,\n",
            "          1.5162e-01],\n",
            "        [ 4.5082e-02,  3.9071e-02,  1.1536e-01, -1.0453e-01, -6.0963e-02,\n",
            "          4.3330e-03,  2.5588e-01,  1.5249e-01,  5.5532e-01, -1.5670e-01,\n",
            "          5.3868e-04, -4.5141e-02,  4.1624e-02, -1.8614e-01,  2.6157e-02,\n",
            "         -4.8085e-02],\n",
            "        [-8.6567e-02,  1.9514e-01, -1.4074e-01, -2.1614e-01,  4.3450e-02,\n",
            "          6.8149e-02, -3.4247e-01, -1.0601e-02, -2.6082e-01, -1.5422e-01,\n",
            "         -3.4895e-01,  2.6017e-02, -1.4483e-01,  2.3247e-02, -1.4049e-02,\n",
            "         -2.9205e-01],\n",
            "        [ 1.1136e-01, -2.2537e-02, -1.7224e-01,  1.5681e-01,  2.4470e-02,\n",
            "         -1.9400e-01,  9.8829e-02,  2.0315e-01, -4.7011e-02, -3.6171e-01,\n",
            "          2.4267e-01,  1.4207e-01, -2.3066e-01,  1.6710e-01, -1.4793e-01,\n",
            "         -2.1089e-01],\n",
            "        [-1.7816e-01,  4.9979e-02, -3.7616e-02,  1.3504e-01,  1.9420e-02,\n",
            "         -9.9860e-02, -5.9019e-02,  2.2394e-02,  1.0047e-01,  3.7133e-02,\n",
            "         -8.3610e-02,  1.0615e-01,  2.0660e-01,  4.3150e-02,  1.3451e-01,\n",
            "          1.8348e-01],\n",
            "        [ 9.6525e-02,  9.4895e-03,  6.1218e-02,  2.1592e-01, -8.4118e-02,\n",
            "          9.7901e-02, -2.6999e-01, -1.5629e-01,  4.1714e-01, -2.8315e-02,\n",
            "         -9.6444e-02,  2.4830e-01,  8.5449e-02, -8.9355e-03,  9.5551e-02,\n",
            "          2.7038e-01],\n",
            "        [-2.0045e-01,  2.1244e-01,  1.0812e-01,  8.8264e-02, -9.4110e-02,\n",
            "          2.1036e-01, -9.5370e-03, -1.6281e-01,  8.7587e-02,  1.0837e-01,\n",
            "          9.8217e-02,  9.4618e-02,  2.1471e-01, -1.4590e-01, -1.9843e-01,\n",
            "          5.0145e-02],\n",
            "        [ 6.5567e-03, -4.0894e-02,  2.5923e-02, -1.6822e-01,  2.3289e-02,\n",
            "          1.2577e-01,  5.5141e-02,  1.8742e-02,  3.2203e-01, -4.7008e-02,\n",
            "         -1.5032e-01,  2.1949e-01, -2.3290e-01,  1.8554e-01,  4.9022e-02,\n",
            "          1.2334e-01],\n",
            "        [-1.0916e-01, -2.3057e-01, -2.3222e-01, -1.6408e-02, -1.8628e-01,\n",
            "          5.4366e-02, -1.8923e-01,  1.1849e-01, -3.4238e-01,  7.2742e-03,\n",
            "         -1.8055e-01,  1.5794e-01,  1.9915e-02,  1.4118e-02, -1.9590e-01,\n",
            "          9.8983e-02],\n",
            "        [ 2.1567e-01, -1.8296e-01, -2.7206e-01, -1.4319e-01, -1.4730e-01,\n",
            "         -1.6347e-01, -1.7073e-01, -1.6750e-01, -3.8717e-01,  1.3459e-01,\n",
            "          1.4879e-01,  1.2026e-01,  1.8559e-01, -2.7583e-02,  1.0810e-01,\n",
            "          6.4337e-02],\n",
            "        [ 3.4076e-02, -2.0640e-01,  9.8545e-02,  6.6389e-02, -1.4250e-01,\n",
            "         -1.0388e-01,  2.1815e-02, -2.9087e-02,  3.5415e-01,  6.9146e-02,\n",
            "         -3.3851e-01,  1.3957e-01,  4.7227e-02, -2.4649e-01,  1.8994e-01,\n",
            "          1.7606e-01],\n",
            "        [-1.4245e-02,  1.2497e-01, -1.3151e-01,  6.9485e-02, -1.0624e-02,\n",
            "          8.8339e-02, -3.1172e-02, -1.1935e-01,  2.2401e-01, -1.3797e-01,\n",
            "         -1.3084e-01,  2.2398e-01, -1.9424e-01,  2.7733e-02, -1.3215e-02,\n",
            "         -2.4475e-02]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.1759,  0.2885,  0.1644, -0.0705,  0.2654,  0.1255,  0.0395, -0.1063,\n",
            "         0.1191,  0.1878,  0.0725,  0.2448, -0.2107, -0.1257,  0.2412,  0.2620],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.1929,  0.1321,  0.2080, -0.1710,  0.1187,  0.2391, -0.0944, -0.2352,\n",
            "          0.2033,  0.1689,  0.0631,  0.1463, -0.1293, -0.1102,  0.1391,  0.2058]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.0921], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomizableFeatureNet(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
              "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.get_heatmap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "bb0d714d-aebb-4215-e194-0f423d6b40d7",
        "id": "i7kYxa001So-"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"d29a3832-d186-4ce0-9234-e5102cd5bde5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d29a3832-d186-4ce0-9234-e5102cd5bde5\")) {                    Plotly.newPlot(                        \"d29a3832-d186-4ce0-9234-e5102cd5bde5\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"z\":[[0.5879974316549313,0.5906878439185339,0.5686551532159744,0.5055466164168474,0.44580740269935115,0.40690092208772033,0.38965616043273554,0.37937823751241007,0.3782866614488333,0.39275884829525254],[0.5590786732306788,0.5330202327358715,0.532272433835479,0.4796854872000891,0.41842303266162467,0.3669819141976805,0.3438416247824012,0.33282451426516524,0.3397079455799382,0.35842770909811694],[0.5369736045386189,0.5020018139945005,0.4807008996808889,0.4244156971068547,0.3689236258951626,0.34825702919255747,0.33765124965135906,0.333968158563448,0.32552215428359665,0.3491380706515025],[0.5148685358465588,0.4791714092419239,0.4494748194854804,0.37754131611501535,0.32740667511868915,0.3184191232077226,0.30595626930815184,0.30255729024846695,0.3133314828729692,0.336491562518149],[0.4927634671544987,0.4570663405498637,0.42232778691525735,0.3480950111717191,0.29597551104452424,0.27785822170286667,0.26893092757702164,0.2680952341633291,0.28816038692555157,0.3159324070962557],[0.4706583984624386,0.43496127185780364,0.39722336377059625,0.32072147467895257,0.2767642405566866,0.263760859440653,0.25482441694328695,0.2455699509676531,0.2536033751761409,0.28703013685296297],[0.44809544706166216,0.4117853558675048,0.3725783567163095,0.293347938186186,0.26239933143144434,0.2586087462661191,0.2585655019274057,0.24955295583426496,0.23429792896327223,0.24649740135120857],[0.4328985757830215,0.38854680585753787,0.3479333496620228,0.2820843023498631,0.261029071825197,0.2534566330915852,0.25341338875287184,0.2533701444141584,0.2442814947252429,0.23945082355593345],[0.4184637997616346,0.37179354463501324,0.35083176693452534,0.2925958882548501,0.2596588122189497,0.24830451991705144,0.248261275578338,0.24821803123962455,0.24817478690091116,0.24630431812274106],[0.4148059886683663,0.38874211960706634,0.36029543672520586,0.30318908096858244,0.25794147000625384,0.24315240674251748,0.2431091624038041,0.2430659180650907,0.24302267372637726,0.24226703861583088]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Values\"},\"ticks\":\"outside\",\"tickvals\":[0.23429792896327223,0.5906878439185339],\"ticktext\":[0.23429792896327223,0.5906878439185339]}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8,9],\"ticktext\":[0,1,2,3,4,5,6,7,8,9],\"title\":{\"text\":\"X\"}},\"yaxis\":{\"tickvals\":[9,8,7,6,5,4,3,2,1,0],\"ticktext\":[9,8,7,6,5,4,3,2,1,0],\"title\":{\"text\":\"Y\"},\"autorange\":\"reversed\"},\"title\":{\"text\":\"Heatmap\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d29a3832-d186-4ce0-9234-e5102cd5bde5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_V_pi_e(pi_e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bfe651-29d9-4e66-c2f8-6a7c1a1e3588",
        "id": "caBnhqlX1So-"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1603494445055814"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_800_random_pi_b.evaluate_scope()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a390fbc7-cf2f-4e45-a47f-707128b7e492",
        "id": "1wey9Vpy1So_"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.9744, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
              " tensor(1.6041, dtype=torch.float64, grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJEYTtlS8-Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2WrUWX46wy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the state_dict of the model\n",
        "model_state_dict = test_200_0p99.model.state_dict()\n",
        "\n",
        "# Print the keys to see the structure of the state_dict\n",
        "print(model_state_dict.keys())\n",
        "\n",
        "# Extract and print the weights of each layer\n",
        "for name, param in model_state_dict.items():\n",
        "    if 'weight' in name:\n",
        "        print(f\"Layer: {name}\")\n",
        "        print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rR01pJ4hQ-z",
        "outputId": "ea85e642-64fe-4767-c74a-4187ab9ae0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'output_layer.weight', 'output_layer.bias'])\n",
            "Layer: hidden_layers.0.weight\n",
            "tensor([[-0.5992, -0.0774],\n",
            "        [ 0.4740, -0.2568],\n",
            "        [ 0.3057,  0.1028],\n",
            "        [ 0.1205, -0.2533],\n",
            "        [ 0.6123,  0.1611],\n",
            "        [-0.2607, -0.6321],\n",
            "        [-0.7033, -0.3404],\n",
            "        [-0.5834, -0.1342],\n",
            "        [ 0.1821, -0.5556],\n",
            "        [-0.4225,  0.3797],\n",
            "        [-0.4621, -0.6912],\n",
            "        [ 0.6232, -0.4203],\n",
            "        [ 0.2262, -0.6769],\n",
            "        [ 0.5225,  0.2460],\n",
            "        [ 0.6153, -0.0401],\n",
            "        [-0.1569, -0.2980]], dtype=torch.float64)\n",
            "Layer: hidden_layers.1.weight\n",
            "tensor([[ 1.0516e-01, -1.5570e-01, -8.9501e-02,  2.2105e-01,  2.2795e-01,\n",
            "          6.3637e-02,  3.5722e-02, -1.2352e-01, -1.2532e-01, -1.9321e-01,\n",
            "          1.7001e-01,  2.7690e-01,  5.0010e-02,  4.4123e-02, -1.7390e-01,\n",
            "         -3.3544e-02],\n",
            "        [ 2.9563e-02, -1.6539e-01,  1.8811e-01,  2.4416e-02, -2.0411e-01,\n",
            "         -2.2469e-01,  3.2124e-02, -1.1786e-01, -2.0997e-01, -2.9288e-03,\n",
            "         -1.8120e-01, -1.8996e-01,  2.2284e-01,  2.0928e-01,  3.6049e-02,\n",
            "          1.2501e-01],\n",
            "        [-1.2149e-01, -2.1002e-01, -9.7943e-02,  1.5191e-02,  2.5198e-01,\n",
            "         -8.9647e-02,  1.0802e-01,  8.8250e-02, -2.0667e-01, -6.7170e-02,\n",
            "         -2.4271e-02, -1.6559e-01, -2.0982e-01, -1.0798e-01,  1.0970e-01,\n",
            "          6.4986e-02],\n",
            "        [ 1.4588e-01,  1.0558e-01,  8.0609e-02,  1.3330e-01,  4.9129e-02,\n",
            "          1.6742e-02,  6.0140e-02,  1.6866e-01, -1.4508e-01, -1.5118e-01,\n",
            "         -2.2014e-01, -2.0411e-01, -1.6783e-02, -1.8986e-01,  6.8790e-02,\n",
            "          2.2454e-02],\n",
            "        [-1.0514e-02,  2.0386e-01,  2.2841e-01, -1.2321e-01, -1.4372e-01,\n",
            "         -2.4803e-01,  1.3377e-02, -4.9486e-02,  1.2915e-03, -1.9275e-01,\n",
            "          2.0011e-01,  2.5863e-01,  2.6719e-01, -2.4577e-01, -2.6740e-02,\n",
            "         -2.8935e-02],\n",
            "        [-2.3204e-02, -8.9060e-02, -1.4838e-01,  8.9374e-03, -7.6390e-02,\n",
            "          1.5483e-01,  1.3361e-01,  1.3709e-01, -2.2401e-01,  1.0198e-01,\n",
            "          1.0681e-01,  7.2959e-02,  1.7711e-01, -4.4961e-03, -2.2231e-01,\n",
            "          9.4986e-02],\n",
            "        [-2.7593e-01,  7.9180e-03, -4.1311e-02, -7.7472e-02, -3.1525e-01,\n",
            "          6.1380e-02, -2.0414e-01, -8.1497e-02,  1.6221e-01,  2.6618e-02,\n",
            "          2.4309e-01,  8.4607e-02,  1.8900e-01,  4.1116e-03,  1.3827e-01,\n",
            "         -2.4482e-01],\n",
            "        [ 2.6721e-02,  1.3537e-01, -8.0593e-02,  2.6549e-01,  6.6624e-02,\n",
            "         -1.6126e-01,  1.8163e-01, -1.7269e-01,  2.2547e-01, -7.9082e-02,\n",
            "         -5.8181e-02, -5.0510e-02, -3.5746e-02,  7.3083e-03, -4.3919e-02,\n",
            "          1.3159e-01],\n",
            "        [-1.3592e-01, -4.7488e-02, -1.4664e-01, -3.0966e-03,  1.7579e-01,\n",
            "         -1.2013e-01,  1.3873e-01,  1.5010e-01,  2.3158e-01, -2.1094e-01,\n",
            "         -1.8497e-01, -1.7136e-01, -6.6302e-02, -1.8402e-01,  1.7502e-01,\n",
            "          1.8137e-01],\n",
            "        [ 1.2734e-01, -2.7244e-01, -6.1841e-02,  2.5599e-01,  1.3099e-01,\n",
            "          1.6996e-01,  1.5586e-01, -1.0916e-01,  1.6139e-01, -6.0406e-02,\n",
            "         -2.4478e-01, -8.9742e-02, -2.4928e-01,  5.3839e-02, -1.9965e-01,\n",
            "         -3.2069e-02],\n",
            "        [-3.1707e-01, -1.4542e-01,  6.7969e-02,  1.0453e-01,  2.5124e-01,\n",
            "         -2.0614e-01,  2.1780e-01, -1.8906e-01,  2.2606e-01, -1.6723e-01,\n",
            "          2.2091e-01,  5.7679e-02, -1.1455e-01,  2.0786e-01,  1.1073e-01,\n",
            "         -9.5604e-02],\n",
            "        [ 2.6024e-01, -1.2099e-01,  2.0291e-01, -7.6303e-02,  3.9195e-02,\n",
            "          1.5864e-01,  1.8815e-01,  7.4287e-03,  9.7794e-02, -1.0302e-01,\n",
            "         -1.3912e-04,  2.1948e-01,  1.1797e-02,  4.6641e-02,  1.7714e-01,\n",
            "         -2.1003e-01],\n",
            "        [ 2.8777e-01, -9.0522e-02, -6.7675e-03, -2.7429e-01,  1.7504e-01,\n",
            "         -2.0078e-01, -9.1050e-02,  3.8632e-02, -2.2507e-01, -2.2281e-01,\n",
            "         -1.4911e-02, -8.6514e-02, -1.7430e-01,  6.1942e-02,  9.1970e-02,\n",
            "          3.2384e-02],\n",
            "        [ 2.6618e-01, -6.2434e-02, -2.8002e-01,  5.8639e-02,  1.6358e-02,\n",
            "         -9.3782e-02,  1.5367e-01, -8.0826e-02,  1.4841e-01,  1.1282e-01,\n",
            "         -1.3212e-01,  1.4293e-02, -9.5888e-02, -7.1260e-02,  7.8707e-02,\n",
            "          5.3369e-02],\n",
            "        [-1.8876e-01,  1.7914e-01,  1.7690e-01, -6.0962e-02, -1.1442e-01,\n",
            "          4.9254e-02, -2.2479e-02,  1.1894e-01, -3.4903e-02,  1.3051e-01,\n",
            "          7.3961e-02, -3.1765e-02, -5.4896e-02, -2.4755e-01, -7.3570e-02,\n",
            "          1.6345e-01],\n",
            "        [-2.4118e-01,  1.2800e-01,  1.3045e-01,  1.0121e-01,  1.7293e-02,\n",
            "          7.1479e-02, -1.6773e-01,  2.0375e-01, -2.9049e-02, -1.8083e-02,\n",
            "         -1.7433e-01,  2.3663e-01,  1.6658e-01,  1.0704e-01, -3.2912e-02,\n",
            "         -1.0926e-01]], dtype=torch.float64)\n",
            "Layer: output_layer.weight\n",
            "tensor([[-0.0364, -0.0046,  0.0472,  0.1042, -0.1400,  0.0453,  0.1236, -0.1070,\n",
            "         -0.1542,  0.0594, -0.0120,  0.0009,  0.0361, -0.1014,  0.0087, -0.0164]],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scope_testing = SCOPE_straight(model, 0.9, 10000, pi_b, P_pi_b, P_pi_e, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "pmjnzbuDy9sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors = scope_testing.prepare()\n",
        "timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = scope_testing.pass_then_boostraps(model, padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors)\n",
        "IS_variance, scope_variance = scope_testing.calc_variance_straight(timestep_bootstraps, weights_bootstraps, rewards_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps)"
      ],
      "metadata": {
        "id": "hoOaF9W7zLoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train var scope"
      ],
      "metadata": {
        "id": "UWo2rOO_SLUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_var_scope(model, num_epochs, learning_rate, test1):\n",
        "\n",
        "    padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors, padded_states_next_tensors, padded_states_current_tensors = test1.prepare()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Enable anomaly detection\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Forward pass\n",
        "        # states_output, states_first_output, states_last_output = test1.pass_states(model, padded_state_tensors, states_first_tensor, states_last_tensor)\n",
        "        # sums_states_weight_diff = test1.states_weight_diff_sums(states_output, padded_weight_diff_tensors)\n",
        "        # gamma_weights_states_last_sub_states_first = test1.last_first_terms_operations(gamma_weights_last_tensor, states_last_output, states_first_output, weight_first_tensor)\n",
        "        # # sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE = test1.bootstrap_shaping_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor)\n",
        "\n",
        "        # samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE = test1.bootstrap_all_terms(sums_states_weight_diff, gamma_weights_states_last_sub_states_first, IS_tensor, padded_psi_tensors)\n",
        "\n",
        "\n",
        "        # Calculate MSE loss between states_output and padded_state_tensors\n",
        "        # mse_loss = F.mse_loss(states_output, padded_state_tensors)\n",
        "\n",
        "        # E_IS_sq, E_IS_all_sq, E_s_wdiff_sq, E_s_wdiff_all_sq, E_IS_SCOPE, E_IS_E_SCOPE, _, variance_loss, E_IS, E_SCOPE = calculate_shaped_variance_play(samples_IS, sample_sums_states_weight_diff, samples_gamma_weight_states_last_sub_states_first, samples_all_shaping, samples_IS_SCOPE)\n",
        "\n",
        "        timestep_bootstraps, rewards_bootstraps, weights_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps = test1.pass_then_boostraps(model, padded_states_next_tensors, padded_states_current_tensors, padded_timestep_tensors, padded_reward_tensors, padded_weight_tensors)\n",
        "        IS_variance, variance_loss = test1.calc_variance_straight(timestep_bootstraps, weights_bootstraps, rewards_bootstraps, phi_states_next_bootstraps, phi_states_current_bootstraps)\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        print(\"IS variance: \", IS_variance)\n",
        "        print(\"SCOPE Var loss: \", variance_loss)\n",
        "        # print(\"MSE loss: \", mse_loss.item())\n",
        "\n",
        "\n",
        "        tot = variance_loss\n",
        "        # tot = variance_loss + mse_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Retain the graph to avoid clearing it before backward pass\n",
        "        tot.backward(retain_graph=True)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += tot.item()\n",
        "\n",
        "        print(f\"Total Loss: {total_loss}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Disable anomaly detection after running the code\n",
        "    torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(f\"Parameter name: {name}\")\n",
        "            print(f\"Weights: {param.data}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "c0xC288M7vT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UOqxfdgBmS81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNLH1JIcmTJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "40GxgnTBmj9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(200, env, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "pi_e = experiment_actions(200, env, P_pi_e)"
      ],
      "metadata": {
        "id": "FmgfU0K6mk6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_200 = CustomizableFeatureNet(input_dim=2, hidden_dims=[16, 32], output_dim=1, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "911brDimm4wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_200 = SCOPE_straight(model_200, 0.9, 10000, pi_b, P_pi_b, P_pi_e, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "U79ry5NxmnCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_200 = train_var_scope(model_200, 1000, 0.0005, test_200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pIcVizym9n_",
        "outputId": "e69cdfd5-0595-4226-80fb-a82cd7b0d07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0008492256656322879\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000832036997470311\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0008178091443168292\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0008067588044517235\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007938512145564716\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007788995781305973\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007657443959582411\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007538666733296384\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007421691758702111\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007303255456230735\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007187112391046453\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000707883908565864\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006971014030871082\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006862588634799347\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006757597277906554\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006657211420834642\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00065551527512402\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006457058049450084\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006359561093684281\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006268297890434793\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006176695790374126\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006088172295279874\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006001613262505393\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005917351211344535\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005834743861129547\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005753437649026478\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005674102836727932\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005595685919933554\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000551992504895417\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005444868933553543\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005371548163724429\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005299083217440924\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005228363198028344\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005158219587202258\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005088778763243133\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005020616816041798\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004952986833330397\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004886668705097123\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00048214910421574147\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004757488869078596\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00046944728948148724\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004632417046995436\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00045707461168116657\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00045102825216827223\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00044507496219341197\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004391699970035594\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004333230599644562\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00042761250144480274\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004219644492074685\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004164604380851949\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004110514097699362\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004057153133658589\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004004735708122344\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00039543175676454563\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003904754894518335\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003855896335018986\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00038081944363259194\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003761076287718471\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00037148031388432314\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00036694533068793947\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003624837490230216\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003581522064171912\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00035390981479894247\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003497487072180543\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003456423814906023\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003415342148518471\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00033748409519833525\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00033357758121261124\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00032966937145839447\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003257789056837663\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00032201754209288264\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003183200983898995\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003146851804160395\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00031110142682717515\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003075700186692655\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003040839840760883\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003006453888635111\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002972676159097015\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00029397181536515573\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000290714618616006\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00028754093491767526\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00028436621835897314\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002812702834925871\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002782205293417825\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00027517542848118186\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002721673910811486\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002691772934722593\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002662458741311763\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00026334525372414023\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00026048058207414274\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000257679029752464\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00025491702413513233\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00025218869607327197\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002495659451975771\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002470028098219896\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00024446738640779373\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00024197302723749767\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00023949914758569896\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00023706675344837175\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00023467001993446638\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00023230188876613452\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00022996372596278763\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002276657162691316\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00022540613135710726\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00022318047154214616\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00022097652743278528\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00021880198498063093\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002166567722388153\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002145398967420797\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00021244651154387546\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002103786889959395\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00020834284407611194\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00020633337019717728\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002043474151014792\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00020238983145584003\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00020046381092816209\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001985589834736423\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001966728843224388\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001948101544284598\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00019297127622431426\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000191153527420611\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00018935645490605007\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00018758075254708326\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001858269587543017\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00018409581733747497\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001823877852810283\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001806990919695813\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017902986656422035\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017738397061152375\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017575113374770711\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017412878630337895\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017252730053461684\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001709447546339227\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016937549791380736\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016780965197323638\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016626873679654024\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016476819395158548\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001632822592991993\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016181057413798233\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016035359810366101\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001589068409013391\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015746872186145762\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001560398248287147\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015461560722389406\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015319526770349903\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015178367998271887\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015038312943554743\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014899510745470528\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001476054614344439\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014621893399159946\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014485502160647672\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014350339452472911\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014216087793788164\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000140830490821699\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013950732187264974\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013820179762622683\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001368967432458594\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013559422535950943\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001343162810900871\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013303815065772028\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013177523769975145\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013052931057876144\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012930307436596485\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012809870006286737\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012692767144281705\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001257186136946716\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012440407716868436\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012311919427811604\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012197322557004407\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012087626076809482\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011971664442531696\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001185639369531777\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011752053728073698\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001164592934238245\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011533870288203292\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011426861724768149\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011326264215393649\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011222679364241443\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011119125356414037\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011020430921033742\n",
            "----------------------------------------\n",
            "Epoch 201\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010922351280646204\n",
            "----------------------------------------\n",
            "Epoch 202\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010822126360227963\n",
            "----------------------------------------\n",
            "Epoch 203\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010723283976101278\n",
            "----------------------------------------\n",
            "Epoch 204\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010627933906459941\n",
            "----------------------------------------\n",
            "Epoch 205\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010532171637350396\n",
            "----------------------------------------\n",
            "Epoch 206\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010435902241609148\n",
            "----------------------------------------\n",
            "Epoch 207\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010341922065436709\n",
            "----------------------------------------\n",
            "Epoch 208\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010248465969284747\n",
            "----------------------------------------\n",
            "Epoch 209\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001015440703268652\n",
            "----------------------------------------\n",
            "Epoch 210\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010061149169569957\n",
            "----------------------------------------\n",
            "Epoch 211\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9697e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.969677409070994e-05\n",
            "----------------------------------------\n",
            "Epoch 212\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8783e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.878335989654519e-05\n",
            "----------------------------------------\n",
            "Epoch 213\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7873e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.787279168785962e-05\n",
            "----------------------------------------\n",
            "Epoch 214\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6976e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.69758197476198e-05\n",
            "----------------------------------------\n",
            "Epoch 215\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6105e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.610492493381914e-05\n",
            "----------------------------------------\n",
            "Epoch 216\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5233e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.523263355361816e-05\n",
            "----------------------------------------\n",
            "Epoch 217\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4360e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.435985163446957e-05\n",
            "----------------------------------------\n",
            "Epoch 218\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3505e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.35053755766774e-05\n",
            "----------------------------------------\n",
            "Epoch 219\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.2670e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.266979831918592e-05\n",
            "----------------------------------------\n",
            "Epoch 220\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1839e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.183938236310378e-05\n",
            "----------------------------------------\n",
            "Epoch 221\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0988e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.098814326320734e-05\n",
            "----------------------------------------\n",
            "Epoch 222\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0143e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.014278646659148e-05\n",
            "----------------------------------------\n",
            "Epoch 223\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9306e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.930625427545809e-05\n",
            "----------------------------------------\n",
            "Epoch 224\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8480e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.847991591928882e-05\n",
            "----------------------------------------\n",
            "Epoch 225\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7656e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.765628041198527e-05\n",
            "----------------------------------------\n",
            "Epoch 226\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6843e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.68429811717542e-05\n",
            "----------------------------------------\n",
            "Epoch 227\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6045e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.604519059921131e-05\n",
            "----------------------------------------\n",
            "Epoch 228\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5246e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.524567753589901e-05\n",
            "----------------------------------------\n",
            "Epoch 229\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4455e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.445540310284939e-05\n",
            "----------------------------------------\n",
            "Epoch 230\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3675e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.367503866475242e-05\n",
            "----------------------------------------\n",
            "Epoch 231\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2888e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.28877858052401e-05\n",
            "----------------------------------------\n",
            "Epoch 232\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2091e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.209147611315046e-05\n",
            "----------------------------------------\n",
            "Epoch 233\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1303e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.130269964015309e-05\n",
            "----------------------------------------\n",
            "Epoch 234\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0524e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.052404453516736e-05\n",
            "----------------------------------------\n",
            "Epoch 235\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9769e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.976872594309536e-05\n",
            "----------------------------------------\n",
            "Epoch 236\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9008e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.900798278279476e-05\n",
            "----------------------------------------\n",
            "Epoch 237\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8226e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.82262197504637e-05\n",
            "----------------------------------------\n",
            "Epoch 238\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7463e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.746266993315425e-05\n",
            "----------------------------------------\n",
            "Epoch 239\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6717e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.67171917254087e-05\n",
            "----------------------------------------\n",
            "Epoch 240\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5976e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.597618400565901e-05\n",
            "----------------------------------------\n",
            "Epoch 241\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5234e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.523359023477829e-05\n",
            "----------------------------------------\n",
            "Epoch 242\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4502e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.45021281678714e-05\n",
            "----------------------------------------\n",
            "Epoch 243\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3773e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.377297094007641e-05\n",
            "----------------------------------------\n",
            "Epoch 244\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3053e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.305314289660489e-05\n",
            "----------------------------------------\n",
            "Epoch 245\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2360e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.236024390150411e-05\n",
            "----------------------------------------\n",
            "Epoch 246\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1663e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.16630498391982e-05\n",
            "----------------------------------------\n",
            "Epoch 247\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0977e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.097702283039156e-05\n",
            "----------------------------------------\n",
            "Epoch 248\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0308e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.030772379447493e-05\n",
            "----------------------------------------\n",
            "Epoch 249\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9643e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.964279675343819e-05\n",
            "----------------------------------------\n",
            "Epoch 250\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8975e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.897455842562479e-05\n",
            "----------------------------------------\n",
            "Epoch 251\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8315e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.8315215636455e-05\n",
            "----------------------------------------\n",
            "Epoch 252\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7702e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.770222175028961e-05\n",
            "----------------------------------------\n",
            "Epoch 253\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7116e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.711640575376605e-05\n",
            "----------------------------------------\n",
            "Epoch 254\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6542e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.654200496153875e-05\n",
            "----------------------------------------\n",
            "Epoch 255\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5981e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.598050820401602e-05\n",
            "----------------------------------------\n",
            "Epoch 256\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5400e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.539972867814303e-05\n",
            "----------------------------------------\n",
            "Epoch 257\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4821e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.482105817145616e-05\n",
            "----------------------------------------\n",
            "Epoch 258\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4261e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.426106372951936e-05\n",
            "----------------------------------------\n",
            "Epoch 259\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3702e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.370162752683761e-05\n",
            "----------------------------------------\n",
            "Epoch 260\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3136e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.313604238992362e-05\n",
            "----------------------------------------\n",
            "Epoch 261\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2593e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.259344745237913e-05\n",
            "----------------------------------------\n",
            "Epoch 262\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2057e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.205717658728037e-05\n",
            "----------------------------------------\n",
            "Epoch 263\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1516e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.151618398033388e-05\n",
            "----------------------------------------\n",
            "Epoch 264\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0991e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.099073526508293e-05\n",
            "----------------------------------------\n",
            "Epoch 265\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0472e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.0471672927703575e-05\n",
            "----------------------------------------\n",
            "Epoch 266\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9963e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.996324473357272e-05\n",
            "----------------------------------------\n",
            "Epoch 267\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9476e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.947636380901482e-05\n",
            "----------------------------------------\n",
            "Epoch 268\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8994e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.899372743628718e-05\n",
            "----------------------------------------\n",
            "Epoch 269\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8505e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.8504877289483376e-05\n",
            "----------------------------------------\n",
            "Epoch 270\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8037e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.803672900925304e-05\n",
            "----------------------------------------\n",
            "Epoch 271\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7575e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.757533646635286e-05\n",
            "----------------------------------------\n",
            "Epoch 272\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7113e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.7113019288540736e-05\n",
            "----------------------------------------\n",
            "Epoch 273\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6658e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.665847843405303e-05\n",
            "----------------------------------------\n",
            "Epoch 274\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6213e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.6212753710204996e-05\n",
            "----------------------------------------\n",
            "Epoch 275\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5769e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.576871097714104e-05\n",
            "----------------------------------------\n",
            "Epoch 276\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5335e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.53349274830648e-05\n",
            "----------------------------------------\n",
            "Epoch 277\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4908e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.490777826260673e-05\n",
            "----------------------------------------\n",
            "Epoch 278\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4483e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.448341001642345e-05\n",
            "----------------------------------------\n",
            "Epoch 279\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4064e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.4064165372956725e-05\n",
            "----------------------------------------\n",
            "Epoch 280\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3653e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.365268471371259e-05\n",
            "----------------------------------------\n",
            "Epoch 281\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3246e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.3245848267945756e-05\n",
            "----------------------------------------\n",
            "Epoch 282\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2842e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.284200317169358e-05\n",
            "----------------------------------------\n",
            "Epoch 283\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2449e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.244853348082652e-05\n",
            "----------------------------------------\n",
            "Epoch 284\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2058e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.205816813909749e-05\n",
            "----------------------------------------\n",
            "Epoch 285\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1672e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.167185029830273e-05\n",
            "----------------------------------------\n",
            "Epoch 286\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1288e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.1287687115397415e-05\n",
            "----------------------------------------\n",
            "Epoch 287\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0911e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.091104628457276e-05\n",
            "----------------------------------------\n",
            "Epoch 288\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0538e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.053780690285663e-05\n",
            "----------------------------------------\n",
            "Epoch 289\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0169e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.016907290713931e-05\n",
            "----------------------------------------\n",
            "Epoch 290\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9800e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.980032913959201e-05\n",
            "----------------------------------------\n",
            "Epoch 291\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9447e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.944722636349524e-05\n",
            "----------------------------------------\n",
            "Epoch 292\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9087e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.908725615747006e-05\n",
            "----------------------------------------\n",
            "Epoch 293\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8734e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.873403874893399e-05\n",
            "----------------------------------------\n",
            "Epoch 294\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8391e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.839098239765689e-05\n",
            "----------------------------------------\n",
            "Epoch 295\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8042e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.804195425956049e-05\n",
            "----------------------------------------\n",
            "Epoch 296\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7699e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.7699016412675674e-05\n",
            "----------------------------------------\n",
            "Epoch 297\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7364e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.736420606014769e-05\n",
            "----------------------------------------\n",
            "Epoch 298\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7034e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.703361545076622e-05\n",
            "----------------------------------------\n",
            "Epoch 299\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6701e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.670104210312564e-05\n",
            "----------------------------------------\n",
            "Epoch 300\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6374e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.637362142046941e-05\n",
            "----------------------------------------\n",
            "Epoch 301\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6054e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.6054495865686785e-05\n",
            "----------------------------------------\n",
            "Epoch 302\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5737e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.5736593336108175e-05\n",
            "----------------------------------------\n",
            "Epoch 303\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5420e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.54204996567932e-05\n",
            "----------------------------------------\n",
            "Epoch 304\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5108e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.510752497015295e-05\n",
            "----------------------------------------\n",
            "Epoch 305\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4799e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.4799312244357624e-05\n",
            "----------------------------------------\n",
            "Epoch 306\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4493e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.449283518855385e-05\n",
            "----------------------------------------\n",
            "Epoch 307\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4189e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.418855865310275e-05\n",
            "----------------------------------------\n",
            "Epoch 308\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3887e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.3886564498877126e-05\n",
            "----------------------------------------\n",
            "Epoch 309\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3589e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.358852927114633e-05\n",
            "----------------------------------------\n",
            "Epoch 310\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3292e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.329234679500792e-05\n",
            "----------------------------------------\n",
            "Epoch 311\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3001e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.300096193925238e-05\n",
            "----------------------------------------\n",
            "Epoch 312\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2713e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.27131341388663e-05\n",
            "----------------------------------------\n",
            "Epoch 313\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2426e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.24258690960057e-05\n",
            "----------------------------------------\n",
            "Epoch 314\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2142e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.214246737842867e-05\n",
            "----------------------------------------\n",
            "Epoch 315\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1860e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.186002219363668e-05\n",
            "----------------------------------------\n",
            "Epoch 316\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1580e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.1580304956656985e-05\n",
            "----------------------------------------\n",
            "Epoch 317\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1300e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.1300166107670276e-05\n",
            "----------------------------------------\n",
            "Epoch 318\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1021e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.1021377296911025e-05\n",
            "----------------------------------------\n",
            "Epoch 319\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0741e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.074067824852833e-05\n",
            "----------------------------------------\n",
            "Epoch 320\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0460e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.046027264010877e-05\n",
            "----------------------------------------\n",
            "Epoch 321\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0181e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.018076411126676e-05\n",
            "----------------------------------------\n",
            "Epoch 322\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9903e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.990324351557248e-05\n",
            "----------------------------------------\n",
            "Epoch 323\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9627e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.9627310512778886e-05\n",
            "----------------------------------------\n",
            "Epoch 324\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9357e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.9356599173565026e-05\n",
            "----------------------------------------\n",
            "Epoch 325\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9084e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.908405524791476e-05\n",
            "----------------------------------------\n",
            "Epoch 326\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8813e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.8812922690681225e-05\n",
            "----------------------------------------\n",
            "Epoch 327\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8543e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.8543174461272455e-05\n",
            "----------------------------------------\n",
            "Epoch 328\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8273e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.8273267673638243e-05\n",
            "----------------------------------------\n",
            "Epoch 329\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8011e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.801086596798809e-05\n",
            "----------------------------------------\n",
            "Epoch 330\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.7747e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.774661066157103e-05\n",
            "----------------------------------------\n",
            "Epoch 331\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.7481e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.74807408492115e-05\n",
            "----------------------------------------\n",
            "Epoch 332\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.7220e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.722009498015186e-05\n",
            "----------------------------------------\n",
            "Epoch 333\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.6964e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.696391533399378e-05\n",
            "----------------------------------------\n",
            "Epoch 334\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.6705e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.670514778304468e-05\n",
            "----------------------------------------\n",
            "Epoch 335\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.6456e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.645607882622268e-05\n",
            "----------------------------------------\n",
            "Epoch 336\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.6209e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.620887551657613e-05\n",
            "----------------------------------------\n",
            "Epoch 337\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5962e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.5962129599482044e-05\n",
            "----------------------------------------\n",
            "Epoch 338\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5717e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.571652022907381e-05\n",
            "----------------------------------------\n",
            "Epoch 339\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5473e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.54727578233288e-05\n",
            "----------------------------------------\n",
            "Epoch 340\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5232e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.5232076211667935e-05\n",
            "----------------------------------------\n",
            "Epoch 341\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4996e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.499647598281811e-05\n",
            "----------------------------------------\n",
            "Epoch 342\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4765e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.476507410630298e-05\n",
            "----------------------------------------\n",
            "Epoch 343\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4527e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.4527102922746365e-05\n",
            "----------------------------------------\n",
            "Epoch 344\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4295e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.429497861910557e-05\n",
            "----------------------------------------\n",
            "Epoch 345\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4066e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.4065595822805726e-05\n",
            "----------------------------------------\n",
            "Epoch 346\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.3839e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.3838730341864375e-05\n",
            "----------------------------------------\n",
            "Epoch 347\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.3617e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.361732272030674e-05\n",
            "----------------------------------------\n",
            "Epoch 348\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.3398e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.339790885123833e-05\n",
            "----------------------------------------\n",
            "Epoch 349\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.3183e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.3183031258184304e-05\n",
            "----------------------------------------\n",
            "Epoch 350\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2965e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.296462021882803e-05\n",
            "----------------------------------------\n",
            "Epoch 351\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2754e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.2753967063140575e-05\n",
            "----------------------------------------\n",
            "Epoch 352\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2547e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.254742624683552e-05\n",
            "----------------------------------------\n",
            "Epoch 353\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2341e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.234098979016392e-05\n",
            "----------------------------------------\n",
            "Epoch 354\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2136e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.213612970216434e-05\n",
            "----------------------------------------\n",
            "Epoch 355\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1933e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.1933096691888275e-05\n",
            "----------------------------------------\n",
            "Epoch 356\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1731e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.1731352755415303e-05\n",
            "----------------------------------------\n",
            "Epoch 357\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1529e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.1528698844340294e-05\n",
            "----------------------------------------\n",
            "Epoch 358\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1324e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.132389925184917e-05\n",
            "----------------------------------------\n",
            "Epoch 359\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1124e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.1123505979830835e-05\n",
            "----------------------------------------\n",
            "Epoch 360\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0924e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.092428357645794e-05\n",
            "----------------------------------------\n",
            "Epoch 361\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0726e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.0726477289141325e-05\n",
            "----------------------------------------\n",
            "Epoch 362\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0533e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.0533185541762077e-05\n",
            "----------------------------------------\n",
            "Epoch 363\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0342e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.0341954897718023e-05\n",
            "----------------------------------------\n",
            "Epoch 364\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0153e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.015280160167472e-05\n",
            "----------------------------------------\n",
            "Epoch 365\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9968e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.99678468118364e-05\n",
            "----------------------------------------\n",
            "Epoch 366\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9786e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9786352860800152e-05\n",
            "----------------------------------------\n",
            "Epoch 367\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9602e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9601761435657524e-05\n",
            "----------------------------------------\n",
            "Epoch 368\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9421e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9420716377132473e-05\n",
            "----------------------------------------\n",
            "Epoch 369\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9239e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.923944645598509e-05\n",
            "----------------------------------------\n",
            "Epoch 370\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9061e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.906081881145796e-05\n",
            "----------------------------------------\n",
            "Epoch 371\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8885e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.8884577157553185e-05\n",
            "----------------------------------------\n",
            "Epoch 372\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8708e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.870755882730994e-05\n",
            "----------------------------------------\n",
            "Epoch 373\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8532e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.8531896822173865e-05\n",
            "----------------------------------------\n",
            "Epoch 374\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8359e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.8358575559953866e-05\n",
            "----------------------------------------\n",
            "Epoch 375\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8187e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.818710442596032e-05\n",
            "----------------------------------------\n",
            "Epoch 376\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8017e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.801693561541207e-05\n",
            "----------------------------------------\n",
            "Epoch 377\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7848e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7848416772059425e-05\n",
            "----------------------------------------\n",
            "Epoch 378\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7681e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7680500128531057e-05\n",
            "----------------------------------------\n",
            "Epoch 379\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7514e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.751419461686585e-05\n",
            "----------------------------------------\n",
            "Epoch 380\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7348e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.734784383544903e-05\n",
            "----------------------------------------\n",
            "Epoch 381\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7186e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7185931422522828e-05\n",
            "----------------------------------------\n",
            "Epoch 382\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7023e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7023125039210853e-05\n",
            "----------------------------------------\n",
            "Epoch 383\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6863e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6863493479831305e-05\n",
            "----------------------------------------\n",
            "Epoch 384\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6703e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.670337194452883e-05\n",
            "----------------------------------------\n",
            "Epoch 385\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6544e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.654419782910289e-05\n",
            "----------------------------------------\n",
            "Epoch 386\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6389e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6388700967940504e-05\n",
            "----------------------------------------\n",
            "Epoch 387\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6237e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6236880869231954e-05\n",
            "----------------------------------------\n",
            "Epoch 388\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6086e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6085819491730384e-05\n",
            "----------------------------------------\n",
            "Epoch 389\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5939e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.593896646818686e-05\n",
            "----------------------------------------\n",
            "Epoch 390\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5789e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5789407186383366e-05\n",
            "----------------------------------------\n",
            "Epoch 391\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5644e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5643708177398852e-05\n",
            "----------------------------------------\n",
            "Epoch 392\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5500e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5499624871587048e-05\n",
            "----------------------------------------\n",
            "Epoch 393\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5355e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.535532471342399e-05\n",
            "----------------------------------------\n",
            "Epoch 394\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5213e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5212757055899176e-05\n",
            "----------------------------------------\n",
            "Epoch 395\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5072e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5071699817538887e-05\n",
            "----------------------------------------\n",
            "Epoch 396\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4933e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.493262536643489e-05\n",
            "----------------------------------------\n",
            "Epoch 397\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4794e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4794294381524616e-05\n",
            "----------------------------------------\n",
            "Epoch 398\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4656e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4656222111300847e-05\n",
            "----------------------------------------\n",
            "Epoch 399\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4520e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.452047769151297e-05\n",
            "----------------------------------------\n",
            "Epoch 400\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4386e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.438598920121162e-05\n",
            "----------------------------------------\n",
            "Epoch 401\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4252e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4252336504901778e-05\n",
            "----------------------------------------\n",
            "Epoch 402\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4121e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4120600911082388e-05\n",
            "----------------------------------------\n",
            "Epoch 403\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3989e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3989189377619764e-05\n",
            "----------------------------------------\n",
            "Epoch 404\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3861e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3861140146277532e-05\n",
            "----------------------------------------\n",
            "Epoch 405\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3732e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3732133377032427e-05\n",
            "----------------------------------------\n",
            "Epoch 406\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3607e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3606626665311617e-05\n",
            "----------------------------------------\n",
            "Epoch 407\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3482e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.348188999480035e-05\n",
            "----------------------------------------\n",
            "Epoch 408\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3355e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3355472861304606e-05\n",
            "----------------------------------------\n",
            "Epoch 409\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3230e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3229766814136952e-05\n",
            "----------------------------------------\n",
            "Epoch 410\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3104e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.310436656111011e-05\n",
            "----------------------------------------\n",
            "Epoch 411\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2980e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.297972945239816e-05\n",
            "----------------------------------------\n",
            "Epoch 412\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2856e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2856254769874023e-05\n",
            "----------------------------------------\n",
            "Epoch 413\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2731e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2730639048937134e-05\n",
            "----------------------------------------\n",
            "Epoch 414\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2608e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.260753688836752e-05\n",
            "----------------------------------------\n",
            "Epoch 415\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2485e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2484807601067045e-05\n",
            "----------------------------------------\n",
            "Epoch 416\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2367e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.236660147184186e-05\n",
            "----------------------------------------\n",
            "Epoch 417\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2249e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.224896625176123e-05\n",
            "----------------------------------------\n",
            "Epoch 418\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2130e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2130331333236658e-05\n",
            "----------------------------------------\n",
            "Epoch 419\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2013e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.201331193753736e-05\n",
            "----------------------------------------\n",
            "Epoch 420\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1899e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.189871441964559e-05\n",
            "----------------------------------------\n",
            "Epoch 421\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1786e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.178604235923495e-05\n",
            "----------------------------------------\n",
            "Epoch 422\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1672e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1672329830814667e-05\n",
            "----------------------------------------\n",
            "Epoch 423\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1561e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.156052820502702e-05\n",
            "----------------------------------------\n",
            "Epoch 424\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1448e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1447777429019927e-05\n",
            "----------------------------------------\n",
            "Epoch 425\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1340e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1340203138769182e-05\n",
            "----------------------------------------\n",
            "Epoch 426\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1228e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.122849526024304e-05\n",
            "----------------------------------------\n",
            "Epoch 427\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1121e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1121202014858878e-05\n",
            "----------------------------------------\n",
            "Epoch 428\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1015e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.101464024934643e-05\n",
            "----------------------------------------\n",
            "Epoch 429\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0908e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.090780466993527e-05\n",
            "----------------------------------------\n",
            "Epoch 430\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0803e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0802669561112895e-05\n",
            "----------------------------------------\n",
            "Epoch 431\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0699e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0699277092889588e-05\n",
            "----------------------------------------\n",
            "Epoch 432\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0596e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0596134505556497e-05\n",
            "----------------------------------------\n",
            "Epoch 433\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0495e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0494959421655e-05\n",
            "----------------------------------------\n",
            "Epoch 434\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0394e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0393504773549608e-05\n",
            "----------------------------------------\n",
            "Epoch 435\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0293e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0292973013855927e-05\n",
            "----------------------------------------\n",
            "Epoch 436\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0192e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0192416726704118e-05\n",
            "----------------------------------------\n",
            "Epoch 437\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0092e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.009183223398696e-05\n",
            "----------------------------------------\n",
            "Epoch 438\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9992e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.999158992109217e-05\n",
            "----------------------------------------\n",
            "Epoch 439\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9891e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9891422384120074e-05\n",
            "----------------------------------------\n",
            "Epoch 440\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9792e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.979216785937777e-05\n",
            "----------------------------------------\n",
            "Epoch 441\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9692e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9692416269238992e-05\n",
            "----------------------------------------\n",
            "Epoch 442\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9593e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9592931401757175e-05\n",
            "----------------------------------------\n",
            "Epoch 443\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9495e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9494554860774023e-05\n",
            "----------------------------------------\n",
            "Epoch 444\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9398e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9397953354304037e-05\n",
            "----------------------------------------\n",
            "Epoch 445\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9302e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.930209545470215e-05\n",
            "----------------------------------------\n",
            "Epoch 446\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9207e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9206778996021408e-05\n",
            "----------------------------------------\n",
            "Epoch 447\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9113e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9113254234977327e-05\n",
            "----------------------------------------\n",
            "Epoch 448\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9020e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9019701785588482e-05\n",
            "----------------------------------------\n",
            "Epoch 449\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8927e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8926752179458734e-05\n",
            "----------------------------------------\n",
            "Epoch 450\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8836e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8835796686918132e-05\n",
            "----------------------------------------\n",
            "Epoch 451\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8745e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.87450241339437e-05\n",
            "----------------------------------------\n",
            "Epoch 452\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8655e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8654538954842562e-05\n",
            "----------------------------------------\n",
            "Epoch 453\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8565e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8565150988417155e-05\n",
            "----------------------------------------\n",
            "Epoch 454\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8477e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.847709347057959e-05\n",
            "----------------------------------------\n",
            "Epoch 455\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8390e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8389731639103224e-05\n",
            "----------------------------------------\n",
            "Epoch 456\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8303e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8302783463865627e-05\n",
            "----------------------------------------\n",
            "Epoch 457\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8215e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8214857097142336e-05\n",
            "----------------------------------------\n",
            "Epoch 458\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8129e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8128720934006212e-05\n",
            "----------------------------------------\n",
            "Epoch 459\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8043e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.804266788917738e-05\n",
            "----------------------------------------\n",
            "Epoch 460\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7957e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7957274338526418e-05\n",
            "----------------------------------------\n",
            "Epoch 461\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7873e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7873255212023606e-05\n",
            "----------------------------------------\n",
            "Epoch 462\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7791e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.779063946834054e-05\n",
            "----------------------------------------\n",
            "Epoch 463\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7708e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.770802216897308e-05\n",
            "----------------------------------------\n",
            "Epoch 464\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7627e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.762659969324139e-05\n",
            "----------------------------------------\n",
            "Epoch 465\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7546e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7546331469657008e-05\n",
            "----------------------------------------\n",
            "Epoch 466\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7469e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.746864035205386e-05\n",
            "----------------------------------------\n",
            "Epoch 467\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7392e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.739151021747901e-05\n",
            "----------------------------------------\n",
            "Epoch 468\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7314e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.73139029512958e-05\n",
            "----------------------------------------\n",
            "Epoch 469\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7237e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7236710599925994e-05\n",
            "----------------------------------------\n",
            "Epoch 470\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7159e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.715911809498747e-05\n",
            "----------------------------------------\n",
            "Epoch 471\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7082e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.708191352659031e-05\n",
            "----------------------------------------\n",
            "Epoch 472\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7005e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7005262697572185e-05\n",
            "----------------------------------------\n",
            "Epoch 473\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6928e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6927973083020383e-05\n",
            "----------------------------------------\n",
            "Epoch 474\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6853e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6852547893865636e-05\n",
            "----------------------------------------\n",
            "Epoch 475\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6778e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6778307054558186e-05\n",
            "----------------------------------------\n",
            "Epoch 476\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6705e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.670502756884706e-05\n",
            "----------------------------------------\n",
            "Epoch 477\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6633e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.663301490671409e-05\n",
            "----------------------------------------\n",
            "Epoch 478\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6562e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6561971869183986e-05\n",
            "----------------------------------------\n",
            "Epoch 479\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6492e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6491785031238073e-05\n",
            "----------------------------------------\n",
            "Epoch 480\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6423e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.642274963991396e-05\n",
            "----------------------------------------\n",
            "Epoch 481\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6354e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6353731192078812e-05\n",
            "----------------------------------------\n",
            "Epoch 482\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6285e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6285349431132467e-05\n",
            "----------------------------------------\n",
            "Epoch 483\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6217e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6217367325363588e-05\n",
            "----------------------------------------\n",
            "Epoch 484\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6150e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6150050515679848e-05\n",
            "----------------------------------------\n",
            "Epoch 485\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6083e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6082915428412144e-05\n",
            "----------------------------------------\n",
            "Epoch 486\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6016e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.6015979152122895e-05\n",
            "----------------------------------------\n",
            "Epoch 487\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5950e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.594963551359844e-05\n",
            "----------------------------------------\n",
            "Epoch 488\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5884e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5883806895374583e-05\n",
            "----------------------------------------\n",
            "Epoch 489\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5818e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5818153318074477e-05\n",
            "----------------------------------------\n",
            "Epoch 490\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5753e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.575308359263744e-05\n",
            "----------------------------------------\n",
            "Epoch 491\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5689e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5688642018873506e-05\n",
            "----------------------------------------\n",
            "Epoch 492\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5624e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.562365422278097e-05\n",
            "----------------------------------------\n",
            "Epoch 493\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5560e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.555953189594072e-05\n",
            "----------------------------------------\n",
            "Epoch 494\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5496e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.54958877636794e-05\n",
            "----------------------------------------\n",
            "Epoch 495\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5432e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5432485288769504e-05\n",
            "----------------------------------------\n",
            "Epoch 496\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5369e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5369176800630954e-05\n",
            "----------------------------------------\n",
            "Epoch 497\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5306e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.530629581785336e-05\n",
            "----------------------------------------\n",
            "Epoch 498\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5244e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5244048797021676e-05\n",
            "----------------------------------------\n",
            "Epoch 499\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5182e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5182087726207302e-05\n",
            "----------------------------------------\n",
            "Epoch 500\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5120e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5120433276237812e-05\n",
            "----------------------------------------\n",
            "Epoch 501\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5059e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.5058893194364405e-05\n",
            "----------------------------------------\n",
            "Epoch 502\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4998e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4997875960890615e-05\n",
            "----------------------------------------\n",
            "Epoch 503\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4938e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4937688852518146e-05\n",
            "----------------------------------------\n",
            "Epoch 504\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4878e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4878287872223396e-05\n",
            "----------------------------------------\n",
            "Epoch 505\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4819e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.481914846659287e-05\n",
            "----------------------------------------\n",
            "Epoch 506\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4760e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4760053905106438e-05\n",
            "----------------------------------------\n",
            "Epoch 507\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4701e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.470114402608226e-05\n",
            "----------------------------------------\n",
            "Epoch 508\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4643e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4642555226838194e-05\n",
            "----------------------------------------\n",
            "Epoch 509\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4585e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4585212200931878e-05\n",
            "----------------------------------------\n",
            "Epoch 510\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4527e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4527255570958493e-05\n",
            "----------------------------------------\n",
            "Epoch 511\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4470e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4469658713834085e-05\n",
            "----------------------------------------\n",
            "Epoch 512\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4413e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4413036097390941e-05\n",
            "----------------------------------------\n",
            "Epoch 513\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4357e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4356986633671797e-05\n",
            "----------------------------------------\n",
            "Epoch 514\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4301e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4300961843194654e-05\n",
            "----------------------------------------\n",
            "Epoch 515\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4246e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4245965779599444e-05\n",
            "----------------------------------------\n",
            "Epoch 516\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4191e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4191465180584123e-05\n",
            "----------------------------------------\n",
            "Epoch 517\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4137e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4137418715206713e-05\n",
            "----------------------------------------\n",
            "Epoch 518\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4084e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4083840394217231e-05\n",
            "----------------------------------------\n",
            "Epoch 519\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4031e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4031369381442996e-05\n",
            "----------------------------------------\n",
            "Epoch 520\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3980e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3979767288086743e-05\n",
            "----------------------------------------\n",
            "Epoch 521\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3929e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3929029429790715e-05\n",
            "----------------------------------------\n",
            "Epoch 522\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3880e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3879787641502867e-05\n",
            "----------------------------------------\n",
            "Epoch 523\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3833e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3833282945940465e-05\n",
            "----------------------------------------\n",
            "Epoch 524\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3789e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3789311085996936e-05\n",
            "----------------------------------------\n",
            "Epoch 525\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3749e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3748512057519106e-05\n",
            "----------------------------------------\n",
            "Epoch 526\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3712e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3711506120304935e-05\n",
            "----------------------------------------\n",
            "Epoch 527\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3677e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3677399224067588e-05\n",
            "----------------------------------------\n",
            "Epoch 528\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3640e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3640232934691128e-05\n",
            "----------------------------------------\n",
            "Epoch 529\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3595e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3595288371342423e-05\n",
            "----------------------------------------\n",
            "Epoch 530\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3536e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3535786642943022e-05\n",
            "----------------------------------------\n",
            "Epoch 531\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3462e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3461942184752149e-05\n",
            "----------------------------------------\n",
            "Epoch 532\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3383e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3382525330336567e-05\n",
            "----------------------------------------\n",
            "Epoch 533\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3313e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.331273786771144e-05\n",
            "----------------------------------------\n",
            "Epoch 534\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3259e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3259455289393232e-05\n",
            "----------------------------------------\n",
            "Epoch 535\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3219e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.321907805264832e-05\n",
            "----------------------------------------\n",
            "Epoch 536\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3182e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3182016322868137e-05\n",
            "----------------------------------------\n",
            "Epoch 537\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3138e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3137942094978146e-05\n",
            "----------------------------------------\n",
            "Epoch 538\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3083e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.30827082220143e-05\n",
            "----------------------------------------\n",
            "Epoch 539\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3020e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3019555271513447e-05\n",
            "----------------------------------------\n",
            "Epoch 540\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2958e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2957513681047242e-05\n",
            "----------------------------------------\n",
            "Epoch 541\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2904e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2903573660642584e-05\n",
            "----------------------------------------\n",
            "Epoch 542\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2858e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2857931159112953e-05\n",
            "----------------------------------------\n",
            "Epoch 543\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2815e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2814590467297593e-05\n",
            "----------------------------------------\n",
            "Epoch 544\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2767e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2767143180473518e-05\n",
            "----------------------------------------\n",
            "Epoch 545\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2714e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2713672943365728e-05\n",
            "----------------------------------------\n",
            "Epoch 546\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2657e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.265748857122879e-05\n",
            "----------------------------------------\n",
            "Epoch 547\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2603e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2603020204993312e-05\n",
            "----------------------------------------\n",
            "Epoch 548\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2554e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.25537145001066e-05\n",
            "----------------------------------------\n",
            "Epoch 549\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2509e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2508510697431409e-05\n",
            "----------------------------------------\n",
            "Epoch 550\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2464e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2463536351825626e-05\n",
            "----------------------------------------\n",
            "Epoch 551\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2417e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2416606631806624e-05\n",
            "----------------------------------------\n",
            "Epoch 552\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2366e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2366392164307541e-05\n",
            "----------------------------------------\n",
            "Epoch 553\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2315e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2315357659531196e-05\n",
            "----------------------------------------\n",
            "Epoch 554\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2266e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2266280248494186e-05\n",
            "----------------------------------------\n",
            "Epoch 555\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2220e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2220086530058517e-05\n",
            "----------------------------------------\n",
            "Epoch 556\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2176e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2175940709417794e-05\n",
            "----------------------------------------\n",
            "Epoch 557\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2132e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2131848979581588e-05\n",
            "----------------------------------------\n",
            "Epoch 558\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2086e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.20855292439563e-05\n",
            "----------------------------------------\n",
            "Epoch 559\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2037e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2037222013742187e-05\n",
            "----------------------------------------\n",
            "Epoch 560\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1989e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1988777494849121e-05\n",
            "----------------------------------------\n",
            "Epoch 561\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1942e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1941542208167131e-05\n",
            "----------------------------------------\n",
            "Epoch 562\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1896e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.189563170775708e-05\n",
            "----------------------------------------\n",
            "Epoch 563\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1850e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1850412578222707e-05\n",
            "----------------------------------------\n",
            "Epoch 564\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1806e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.180627193819347e-05\n",
            "----------------------------------------\n",
            "Epoch 565\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1763e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1763210772942293e-05\n",
            "----------------------------------------\n",
            "Epoch 566\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1720e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1719552989049111e-05\n",
            "----------------------------------------\n",
            "Epoch 567\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1676e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1676227263652522e-05\n",
            "----------------------------------------\n",
            "Epoch 568\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1634e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1633581025971187e-05\n",
            "----------------------------------------\n",
            "Epoch 569\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1592e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.159172264729097e-05\n",
            "----------------------------------------\n",
            "Epoch 570\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1550e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1550036513231593e-05\n",
            "----------------------------------------\n",
            "Epoch 571\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1508e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1508163951290118e-05\n",
            "----------------------------------------\n",
            "Epoch 572\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1466e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1466228662072152e-05\n",
            "----------------------------------------\n",
            "Epoch 573\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1424e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1424334363443679e-05\n",
            "----------------------------------------\n",
            "Epoch 574\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1383e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.138264559405212e-05\n",
            "----------------------------------------\n",
            "Epoch 575\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1341e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1341226534236857e-05\n",
            "----------------------------------------\n",
            "Epoch 576\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1300e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.130041380680045e-05\n",
            "----------------------------------------\n",
            "Epoch 577\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1260e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.126008620819722e-05\n",
            "----------------------------------------\n",
            "Epoch 578\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1220e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1220092209194774e-05\n",
            "----------------------------------------\n",
            "Epoch 579\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1180e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1180171453493145e-05\n",
            "----------------------------------------\n",
            "Epoch 580\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1140e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1140379866225069e-05\n",
            "----------------------------------------\n",
            "Epoch 581\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1101e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1100899660488096e-05\n",
            "----------------------------------------\n",
            "Epoch 582\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1062e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1061638923151898e-05\n",
            "----------------------------------------\n",
            "Epoch 583\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1023e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1023103736370482e-05\n",
            "----------------------------------------\n",
            "Epoch 584\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0985e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0984890401719405e-05\n",
            "----------------------------------------\n",
            "Epoch 585\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0947e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0946870864490308e-05\n",
            "----------------------------------------\n",
            "Epoch 586\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0909e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0909314778314988e-05\n",
            "----------------------------------------\n",
            "Epoch 587\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0872e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0871802540498322e-05\n",
            "----------------------------------------\n",
            "Epoch 588\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0834e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0834353312926589e-05\n",
            "----------------------------------------\n",
            "Epoch 589\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0797e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0797101491303865e-05\n",
            "----------------------------------------\n",
            "Epoch 590\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0760e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0760289699887326e-05\n",
            "----------------------------------------\n",
            "Epoch 591\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0724e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0724417154202008e-05\n",
            "----------------------------------------\n",
            "Epoch 592\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0689e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0688570969159785e-05\n",
            "----------------------------------------\n",
            "Epoch 593\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0653e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0652522610515287e-05\n",
            "----------------------------------------\n",
            "Epoch 594\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0616e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0615996888453128e-05\n",
            "----------------------------------------\n",
            "Epoch 595\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0580e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0579707040886388e-05\n",
            "----------------------------------------\n",
            "Epoch 596\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0544e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0544111172965866e-05\n",
            "----------------------------------------\n",
            "Epoch 597\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0509e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0508621555403671e-05\n",
            "----------------------------------------\n",
            "Epoch 598\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0473e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0473302119117834e-05\n",
            "----------------------------------------\n",
            "Epoch 599\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0438e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0438119444852171e-05\n",
            "----------------------------------------\n",
            "Epoch 600\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0403e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0403257132164128e-05\n",
            "----------------------------------------\n",
            "Epoch 601\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0369e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0368533288679419e-05\n",
            "----------------------------------------\n",
            "Epoch 602\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0334e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0334106903667875e-05\n",
            "----------------------------------------\n",
            "Epoch 603\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0300e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.029996942972543e-05\n",
            "----------------------------------------\n",
            "Epoch 604\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0266e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0265960908875135e-05\n",
            "----------------------------------------\n",
            "Epoch 605\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0232e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0232084873461173e-05\n",
            "----------------------------------------\n",
            "Epoch 606\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0198e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0198205279745892e-05\n",
            "----------------------------------------\n",
            "Epoch 607\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0165e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0164531088021264e-05\n",
            "----------------------------------------\n",
            "Epoch 608\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0131e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.013142253059981e-05\n",
            "----------------------------------------\n",
            "Epoch 609\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0098e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0098139454524867e-05\n",
            "----------------------------------------\n",
            "Epoch 610\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0065e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.00647081914433e-05\n",
            "----------------------------------------\n",
            "Epoch 611\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0031e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.003143903870121e-05\n",
            "----------------------------------------\n",
            "Epoch 612\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9998e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.999838436869885e-06\n",
            "----------------------------------------\n",
            "Epoch 613\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9687e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.968666052112891e-06\n",
            "----------------------------------------\n",
            "Epoch 614\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9378e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.937831374207047e-06\n",
            "----------------------------------------\n",
            "Epoch 615\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9071e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.907147060115282e-06\n",
            "----------------------------------------\n",
            "Epoch 616\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8767e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.87674094776269e-06\n",
            "----------------------------------------\n",
            "Epoch 617\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8466e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.846583033751407e-06\n",
            "----------------------------------------\n",
            "Epoch 618\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8165e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.816473742809703e-06\n",
            "----------------------------------------\n",
            "Epoch 619\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7866e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.786617656248949e-06\n",
            "----------------------------------------\n",
            "Epoch 620\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7569e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.756938815910973e-06\n",
            "----------------------------------------\n",
            "Epoch 621\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7274e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.727424303635867e-06\n",
            "----------------------------------------\n",
            "Epoch 622\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6982e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.698172373983967e-06\n",
            "----------------------------------------\n",
            "Epoch 623\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6691e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.66908274436466e-06\n",
            "----------------------------------------\n",
            "Epoch 624\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6400e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.639998531985638e-06\n",
            "----------------------------------------\n",
            "Epoch 625\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6112e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.611246451765864e-06\n",
            "----------------------------------------\n",
            "Epoch 626\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5825e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.582513522807958e-06\n",
            "----------------------------------------\n",
            "Epoch 627\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5539e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.55389304348151e-06\n",
            "----------------------------------------\n",
            "Epoch 628\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5255e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.525491614583638e-06\n",
            "----------------------------------------\n",
            "Epoch 629\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4971e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.497144979917675e-06\n",
            "----------------------------------------\n",
            "Epoch 630\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4690e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.468958087616972e-06\n",
            "----------------------------------------\n",
            "Epoch 631\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4409e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.440863781604734e-06\n",
            "----------------------------------------\n",
            "Epoch 632\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4127e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.412672557181043e-06\n",
            "----------------------------------------\n",
            "Epoch 633\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3845e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.384519028883956e-06\n",
            "----------------------------------------\n",
            "Epoch 634\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3565e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.356526185389734e-06\n",
            "----------------------------------------\n",
            "Epoch 635\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3288e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.32876652182987e-06\n",
            "----------------------------------------\n",
            "Epoch 636\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3013e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.301317104443821e-06\n",
            "----------------------------------------\n",
            "Epoch 637\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.2742e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.274194194131709e-06\n",
            "----------------------------------------\n",
            "Epoch 638\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.2470e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.24695524175791e-06\n",
            "----------------------------------------\n",
            "Epoch 639\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.2201e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.220055651422837e-06\n",
            "----------------------------------------\n",
            "Epoch 640\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1933e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.193288169708878e-06\n",
            "----------------------------------------\n",
            "Epoch 641\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1666e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.166582800732414e-06\n",
            "----------------------------------------\n",
            "Epoch 642\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1400e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.140031188250686e-06\n",
            "----------------------------------------\n",
            "Epoch 643\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1137e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.113718415575919e-06\n",
            "----------------------------------------\n",
            "Epoch 644\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0876e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.087572255137824e-06\n",
            "----------------------------------------\n",
            "Epoch 645\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0617e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.061680786827047e-06\n",
            "----------------------------------------\n",
            "Epoch 646\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0359e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.035936826516608e-06\n",
            "----------------------------------------\n",
            "Epoch 647\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0103e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.01031868737301e-06\n",
            "----------------------------------------\n",
            "Epoch 648\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9845e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.984519395677459e-06\n",
            "----------------------------------------\n",
            "Epoch 649\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9588e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.958789341063496e-06\n",
            "----------------------------------------\n",
            "Epoch 650\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9331e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.93305912837144e-06\n",
            "----------------------------------------\n",
            "Epoch 651\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9075e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.907523770881543e-06\n",
            "----------------------------------------\n",
            "Epoch 652\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8821e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.88208840945759e-06\n",
            "----------------------------------------\n",
            "Epoch 653\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8569e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.856907933899592e-06\n",
            "----------------------------------------\n",
            "Epoch 654\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8316e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.831641721042123e-06\n",
            "----------------------------------------\n",
            "Epoch 655\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8065e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.80651531990067e-06\n",
            "----------------------------------------\n",
            "Epoch 656\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7816e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.781562201349515e-06\n",
            "----------------------------------------\n",
            "Epoch 657\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7571e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.757052820987406e-06\n",
            "----------------------------------------\n",
            "Epoch 658\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7326e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.732621727888613e-06\n",
            "----------------------------------------\n",
            "Epoch 659\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7082e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.708218839392676e-06\n",
            "----------------------------------------\n",
            "Epoch 660\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6836e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.683634746674374e-06\n",
            "----------------------------------------\n",
            "Epoch 661\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6591e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.659147994333974e-06\n",
            "----------------------------------------\n",
            "Epoch 662\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6347e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.634701623340153e-06\n",
            "----------------------------------------\n",
            "Epoch 663\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6103e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.610322283631594e-06\n",
            "----------------------------------------\n",
            "Epoch 664\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5860e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.586021961310189e-06\n",
            "----------------------------------------\n",
            "Epoch 665\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5618e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.561826339949466e-06\n",
            "----------------------------------------\n",
            "Epoch 666\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5377e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.537703614020985e-06\n",
            "----------------------------------------\n",
            "Epoch 667\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5138e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.513778561470222e-06\n",
            "----------------------------------------\n",
            "Epoch 668\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4902e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.490245877767958e-06\n",
            "----------------------------------------\n",
            "Epoch 669\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4662e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.466220177384649e-06\n",
            "----------------------------------------\n",
            "Epoch 670\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4429e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.442868717934825e-06\n",
            "----------------------------------------\n",
            "Epoch 671\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4192e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.419158074182264e-06\n",
            "----------------------------------------\n",
            "Epoch 672\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3952e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.395199273154264e-06\n",
            "----------------------------------------\n",
            "Epoch 673\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3718e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.371771104755943e-06\n",
            "----------------------------------------\n",
            "Epoch 674\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3482e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.34815357058824e-06\n",
            "----------------------------------------\n",
            "Epoch 675\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3246e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.3246232449536e-06\n",
            "----------------------------------------\n",
            "Epoch 676\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3013e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.301295469311155e-06\n",
            "----------------------------------------\n",
            "Epoch 677\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2779e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.277887737956466e-06\n",
            "----------------------------------------\n",
            "Epoch 678\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2550e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.255021414070895e-06\n",
            "----------------------------------------\n",
            "Epoch 679\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2327e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.23265362469655e-06\n",
            "----------------------------------------\n",
            "Epoch 680\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2103e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.210254343107245e-06\n",
            "----------------------------------------\n",
            "Epoch 681\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1880e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.188036870540324e-06\n",
            "----------------------------------------\n",
            "Epoch 682\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1660e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.16604475997928e-06\n",
            "----------------------------------------\n",
            "Epoch 683\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1441e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.144077488635671e-06\n",
            "----------------------------------------\n",
            "Epoch 684\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1223e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.122265090159514e-06\n",
            "----------------------------------------\n",
            "Epoch 685\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1006e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.10058347179006e-06\n",
            "----------------------------------------\n",
            "Epoch 686\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0790e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.079020880662618e-06\n",
            "----------------------------------------\n",
            "Epoch 687\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0578e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.057766052877307e-06\n",
            "----------------------------------------\n",
            "Epoch 688\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0363e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.036280377318548e-06\n",
            "----------------------------------------\n",
            "Epoch 689\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0151e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.015065637825286e-06\n",
            "----------------------------------------\n",
            "Epoch 690\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9940e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.994002564768659e-06\n",
            "----------------------------------------\n",
            "Epoch 691\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9730e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.973035385261864e-06\n",
            "----------------------------------------\n",
            "Epoch 692\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9522e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.952192644660669e-06\n",
            "----------------------------------------\n",
            "Epoch 693\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9314e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.93140048381596e-06\n",
            "----------------------------------------\n",
            "Epoch 694\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9107e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.910724071018069e-06\n",
            "----------------------------------------\n",
            "Epoch 695\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8902e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.89016715285379e-06\n",
            "----------------------------------------\n",
            "Epoch 696\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8697e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.869659562954376e-06\n",
            "----------------------------------------\n",
            "Epoch 697\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8492e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.849235328898694e-06\n",
            "----------------------------------------\n",
            "Epoch 698\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8289e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.828937689531348e-06\n",
            "----------------------------------------\n",
            "Epoch 699\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8087e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.808684675910992e-06\n",
            "----------------------------------------\n",
            "Epoch 700\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7885e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.788530282395569e-06\n",
            "----------------------------------------\n",
            "Epoch 701\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7685e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.768465746182588e-06\n",
            "----------------------------------------\n",
            "Epoch 702\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7485e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.7484588213387e-06\n",
            "----------------------------------------\n",
            "Epoch 703\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7286e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.728554176904503e-06\n",
            "----------------------------------------\n",
            "Epoch 704\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7088e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.708776137790376e-06\n",
            "----------------------------------------\n",
            "Epoch 705\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6891e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.689133910635681e-06\n",
            "----------------------------------------\n",
            "Epoch 706\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6696e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.669557248638832e-06\n",
            "----------------------------------------\n",
            "Epoch 707\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6500e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.650008925786037e-06\n",
            "----------------------------------------\n",
            "Epoch 708\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6305e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.630532593999313e-06\n",
            "----------------------------------------\n",
            "Epoch 709\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6112e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.611218365355172e-06\n",
            "----------------------------------------\n",
            "Epoch 710\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5918e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.5918418710316424e-06\n",
            "----------------------------------------\n",
            "Epoch 711\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5726e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.572628322041073e-06\n",
            "----------------------------------------\n",
            "Epoch 712\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5535e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.553477746666121e-06\n",
            "----------------------------------------\n",
            "Epoch 713\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5344e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.534419900088396e-06\n",
            "----------------------------------------\n",
            "Epoch 714\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5154e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.515434514529349e-06\n",
            "----------------------------------------\n",
            "Epoch 715\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4966e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.496596332069689e-06\n",
            "----------------------------------------\n",
            "Epoch 716\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4779e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.477856884793299e-06\n",
            "----------------------------------------\n",
            "Epoch 717\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4592e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.459168653946338e-06\n",
            "----------------------------------------\n",
            "Epoch 718\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4405e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.440486642149688e-06\n",
            "----------------------------------------\n",
            "Epoch 719\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4220e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.4219528544730574e-06\n",
            "----------------------------------------\n",
            "Epoch 720\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4034e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.403362910834853e-06\n",
            "----------------------------------------\n",
            "Epoch 721\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3849e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.384889696145958e-06\n",
            "----------------------------------------\n",
            "Epoch 722\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3665e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.36648746031029e-06\n",
            "----------------------------------------\n",
            "Epoch 723\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3481e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.348145234779892e-06\n",
            "----------------------------------------\n",
            "Epoch 724\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3299e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.329863347652183e-06\n",
            "----------------------------------------\n",
            "Epoch 725\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3117e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.311652236359258e-06\n",
            "----------------------------------------\n",
            "Epoch 726\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2935e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.2935198664173674e-06\n",
            "----------------------------------------\n",
            "Epoch 727\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2755e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.275487632998186e-06\n",
            "----------------------------------------\n",
            "Epoch 728\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2575e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.257526515993993e-06\n",
            "----------------------------------------\n",
            "Epoch 729\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2396e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.239634009678592e-06\n",
            "----------------------------------------\n",
            "Epoch 730\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2218e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.221835917382804e-06\n",
            "----------------------------------------\n",
            "Epoch 731\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2041e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.20408456614118e-06\n",
            "----------------------------------------\n",
            "Epoch 732\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1864e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.18640496646378e-06\n",
            "----------------------------------------\n",
            "Epoch 733\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1688e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.1687991913397746e-06\n",
            "----------------------------------------\n",
            "Epoch 734\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1513e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.151250847977357e-06\n",
            "----------------------------------------\n",
            "Epoch 735\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1338e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.133762459958404e-06\n",
            "----------------------------------------\n",
            "Epoch 736\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1164e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.1163926856956925e-06\n",
            "----------------------------------------\n",
            "Epoch 737\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0990e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.0990107375325055e-06\n",
            "----------------------------------------\n",
            "Epoch 738\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0818e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.081753703571002e-06\n",
            "----------------------------------------\n",
            "Epoch 739\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0645e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.064534968864768e-06\n",
            "----------------------------------------\n",
            "Epoch 740\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0474e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.047395876215032e-06\n",
            "----------------------------------------\n",
            "Epoch 741\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0303e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.030344032509025e-06\n",
            "----------------------------------------\n",
            "Epoch 742\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0133e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.013344800074168e-06\n",
            "----------------------------------------\n",
            "Epoch 743\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9964e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.996435181977323e-06\n",
            "----------------------------------------\n",
            "Epoch 744\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9796e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.979591902911158e-06\n",
            "----------------------------------------\n",
            "Epoch 745\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9628e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.962772387680999e-06\n",
            "----------------------------------------\n",
            "Epoch 746\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9460e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.945954116777822e-06\n",
            "----------------------------------------\n",
            "Epoch 747\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9292e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.929200718724065e-06\n",
            "----------------------------------------\n",
            "Epoch 748\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9124e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.912411147351215e-06\n",
            "----------------------------------------\n",
            "Epoch 749\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8956e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.895597880154173e-06\n",
            "----------------------------------------\n",
            "Epoch 750\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8788e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.878817339184308e-06\n",
            "----------------------------------------\n",
            "Epoch 751\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8620e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.862031104265756e-06\n",
            "----------------------------------------\n",
            "Epoch 752\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8453e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.845251969490648e-06\n",
            "----------------------------------------\n",
            "Epoch 753\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8285e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.828538497285855e-06\n",
            "----------------------------------------\n",
            "Epoch 754\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8118e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.811849649776431e-06\n",
            "----------------------------------------\n",
            "Epoch 755\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7955e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.795455037503317e-06\n",
            "----------------------------------------\n",
            "Epoch 756\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7791e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.779103456600541e-06\n",
            "----------------------------------------\n",
            "Epoch 757\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7628e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.7628466955375215e-06\n",
            "----------------------------------------\n",
            "Epoch 758\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7467e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.746739735400868e-06\n",
            "----------------------------------------\n",
            "Epoch 759\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7307e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.730741851678048e-06\n",
            "----------------------------------------\n",
            "Epoch 760\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7149e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.714892501900936e-06\n",
            "----------------------------------------\n",
            "Epoch 761\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6992e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.699195131003278e-06\n",
            "----------------------------------------\n",
            "Epoch 762\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6837e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.68368906645652e-06\n",
            "----------------------------------------\n",
            "Epoch 763\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6685e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.668450419808167e-06\n",
            "----------------------------------------\n",
            "Epoch 764\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6536e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.653582638385843e-06\n",
            "----------------------------------------\n",
            "Epoch 765\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6392e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.6392174317431225e-06\n",
            "----------------------------------------\n",
            "Epoch 766\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6255e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.625474652504363e-06\n",
            "----------------------------------------\n",
            "Epoch 767\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6126e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.61255237816874e-06\n",
            "----------------------------------------\n",
            "Epoch 768\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6006e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.600555761343627e-06\n",
            "----------------------------------------\n",
            "Epoch 769\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5896e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.589622677653054e-06\n",
            "----------------------------------------\n",
            "Epoch 770\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5796e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.579551759164323e-06\n",
            "----------------------------------------\n",
            "Epoch 771\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5701e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.5701050876781375e-06\n",
            "----------------------------------------\n",
            "Epoch 772\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5604e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.560404292768259e-06\n",
            "----------------------------------------\n",
            "Epoch 773\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5491e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.549056206772362e-06\n",
            "----------------------------------------\n",
            "Epoch 774\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5347e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.53467573158713e-06\n",
            "----------------------------------------\n",
            "Epoch 775\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5160e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.5159924642491045e-06\n",
            "----------------------------------------\n",
            "Epoch 776\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4928e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.492759010520037e-06\n",
            "----------------------------------------\n",
            "Epoch 777\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4645e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.464486044523303e-06\n",
            "----------------------------------------\n",
            "Epoch 778\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6160e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.6159711745993e-06\n",
            "----------------------------------------\n",
            "Epoch 779\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3493e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.349339091096235e-06\n",
            "----------------------------------------\n",
            "Epoch 780\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5970e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.597001741826103e-06\n",
            "----------------------------------------\n",
            "Epoch 781\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5323e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.5322558845222235e-06\n",
            "----------------------------------------\n",
            "Epoch 782\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7347e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.734664198914599e-06\n",
            "----------------------------------------\n",
            "Epoch 783\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1505e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.1505067244313395e-06\n",
            "----------------------------------------\n",
            "Epoch 784\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3819e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.381854300748931e-06\n",
            "----------------------------------------\n",
            "Epoch 785\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7326e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.732638662315612e-06\n",
            "----------------------------------------\n",
            "Epoch 786\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7506e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.750590011138753e-06\n",
            "----------------------------------------\n",
            "Epoch 787\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3037e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.303683752874833e-06\n",
            "----------------------------------------\n",
            "Epoch 788\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7223e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.722321137708598e-06\n",
            "----------------------------------------\n",
            "Epoch 789\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3917e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.391651437897074e-06\n",
            "----------------------------------------\n",
            "Epoch 790\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3856e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.385599653483475e-06\n",
            "----------------------------------------\n",
            "Epoch 791\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5396e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.539573153944552e-06\n",
            "----------------------------------------\n",
            "Epoch 792\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2217e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.221744449630972e-06\n",
            "----------------------------------------\n",
            "Epoch 793\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4467e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.446671425943872e-06\n",
            "----------------------------------------\n",
            "Epoch 794\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2824e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.28237274480974e-06\n",
            "----------------------------------------\n",
            "Epoch 795\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2483e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.248331746412401e-06\n",
            "----------------------------------------\n",
            "Epoch 796\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3423e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.342271104255554e-06\n",
            "----------------------------------------\n",
            "Epoch 797\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1497e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.14965648874693e-06\n",
            "----------------------------------------\n",
            "Epoch 798\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2785e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.278458539976854e-06\n",
            "----------------------------------------\n",
            "Epoch 799\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1667e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.166698402074632e-06\n",
            "----------------------------------------\n",
            "Epoch 800\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1542e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.154249412641356e-06\n",
            "----------------------------------------\n",
            "Epoch 801\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1907e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.190700510876305e-06\n",
            "----------------------------------------\n",
            "Epoch 802\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0763e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.076339519591238e-06\n",
            "----------------------------------------\n",
            "Epoch 803\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1493e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.149259105283037e-06\n",
            "----------------------------------------\n",
            "Epoch 804\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0688e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.068758248930306e-06\n",
            "----------------------------------------\n",
            "Epoch 805\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0671e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.0671356897930855e-06\n",
            "----------------------------------------\n",
            "Epoch 806\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0739e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.07392324556847e-06\n",
            "----------------------------------------\n",
            "Epoch 807\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0055e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.005498723006554e-06\n",
            "----------------------------------------\n",
            "Epoch 808\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.0435e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.043512302261288e-06\n",
            "----------------------------------------\n",
            "Epoch 809\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9858e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.985775077132882e-06\n",
            "----------------------------------------\n",
            "Epoch 810\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9848e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.98478150017932e-06\n",
            "----------------------------------------\n",
            "Epoch 811\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9782e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.9781873529238695e-06\n",
            "----------------------------------------\n",
            "Epoch 812\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9349e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.934863310040185e-06\n",
            "----------------------------------------\n",
            "Epoch 813\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9515e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.951504487124688e-06\n",
            "----------------------------------------\n",
            "Epoch 814\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9098e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.909803378560522e-06\n",
            "----------------------------------------\n",
            "Epoch 815\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.9068e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.906827554194401e-06\n",
            "----------------------------------------\n",
            "Epoch 816\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8945e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.894468212921086e-06\n",
            "----------------------------------------\n",
            "Epoch 817\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8652e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.865191462813886e-06\n",
            "----------------------------------------\n",
            "Epoch 818\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8692e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.869179198792307e-06\n",
            "----------------------------------------\n",
            "Epoch 819\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8381e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.838147352209301e-06\n",
            "----------------------------------------\n",
            "Epoch 820\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8325e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.832452398396235e-06\n",
            "----------------------------------------\n",
            "Epoch 821\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8181e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.818100011606705e-06\n",
            "----------------------------------------\n",
            "Epoch 822\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7964e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.796381683228385e-06\n",
            "----------------------------------------\n",
            "Epoch 823\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7928e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.792779359558954e-06\n",
            "----------------------------------------\n",
            "Epoch 824\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7687e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.768697480019575e-06\n",
            "----------------------------------------\n",
            "Epoch 825\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7603e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.760280849667393e-06\n",
            "----------------------------------------\n",
            "Epoch 826\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7458e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.7458282528529195e-06\n",
            "----------------------------------------\n",
            "Epoch 827\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7280e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.728027895908281e-06\n",
            "----------------------------------------\n",
            "Epoch 828\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7206e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.7205883564861304e-06\n",
            "----------------------------------------\n",
            "Epoch 829\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7008e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.700783911632315e-06\n",
            "----------------------------------------\n",
            "Epoch 830\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6911e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.691086035410192e-06\n",
            "----------------------------------------\n",
            "Epoch 831\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6769e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.676898239823332e-06\n",
            "----------------------------------------\n",
            "Epoch 832\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6614e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.66135172285795e-06\n",
            "----------------------------------------\n",
            "Epoch 833\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6516e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.651636642250814e-06\n",
            "----------------------------------------\n",
            "Epoch 834\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6348e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.634772019611635e-06\n",
            "----------------------------------------\n",
            "Epoch 835\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6239e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.62385819704428e-06\n",
            "----------------------------------------\n",
            "Epoch 836\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.6103e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.610310135528389e-06\n",
            "----------------------------------------\n",
            "Epoch 837\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5961e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.596129889991644e-06\n",
            "----------------------------------------\n",
            "Epoch 838\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5853e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.585349290528626e-06\n",
            "----------------------------------------\n",
            "Epoch 839\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5703e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.570305097053238e-06\n",
            "----------------------------------------\n",
            "Epoch 840\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5590e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.559012039114551e-06\n",
            "----------------------------------------\n",
            "Epoch 841\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5462e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.546236889684056e-06\n",
            "----------------------------------------\n",
            "Epoch 842\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5330e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.53303355013229e-06\n",
            "----------------------------------------\n",
            "Epoch 843\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5221e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.522146231936849e-06\n",
            "----------------------------------------\n",
            "Epoch 844\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.5087e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.508701476645154e-06\n",
            "----------------------------------------\n",
            "Epoch 845\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4973e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.497330719966634e-06\n",
            "----------------------------------------\n",
            "Epoch 846\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4851e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.485099001966675e-06\n",
            "----------------------------------------\n",
            "Epoch 847\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4725e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.472533504436025e-06\n",
            "----------------------------------------\n",
            "Epoch 848\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4614e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.461375486021435e-06\n",
            "----------------------------------------\n",
            "Epoch 849\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4487e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.448714082019441e-06\n",
            "----------------------------------------\n",
            "Epoch 850\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4372e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.437238528576331e-06\n",
            "----------------------------------------\n",
            "Epoch 851\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4254e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.425352672076715e-06\n",
            "----------------------------------------\n",
            "Epoch 852\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4133e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.4132852699277435e-06\n",
            "----------------------------------------\n",
            "Epoch 853\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4021e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.402053641687304e-06\n",
            "----------------------------------------\n",
            "Epoch 854\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3899e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.389855603477778e-06\n",
            "----------------------------------------\n",
            "Epoch 855\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3784e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.3784393455637436e-06\n",
            "----------------------------------------\n",
            "Epoch 856\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3669e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.366877628887148e-06\n",
            "----------------------------------------\n",
            "Epoch 857\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3551e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.355131054173876e-06\n",
            "----------------------------------------\n",
            "Epoch 858\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3440e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.343965143251497e-06\n",
            "----------------------------------------\n",
            "Epoch 859\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3321e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.332149092016089e-06\n",
            "----------------------------------------\n",
            "Epoch 860\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3209e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.320890575137618e-06\n",
            "----------------------------------------\n",
            "Epoch 861\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3095e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.3095205231500804e-06\n",
            "----------------------------------------\n",
            "Epoch 862\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2979e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.297908946892838e-06\n",
            "----------------------------------------\n",
            "Epoch 863\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2867e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.286653656274361e-06\n",
            "----------------------------------------\n",
            "Epoch 864\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2751e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.275106637347438e-06\n",
            "----------------------------------------\n",
            "Epoch 865\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2637e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.263737809006187e-06\n",
            "----------------------------------------\n",
            "Epoch 866\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2525e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.252515736782866e-06\n",
            "----------------------------------------\n",
            "Epoch 867\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2412e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.241180270279359e-06\n",
            "----------------------------------------\n",
            "Epoch 868\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2299e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.22994663835292e-06\n",
            "----------------------------------------\n",
            "Epoch 869\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2187e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.218703514673314e-06\n",
            "----------------------------------------\n",
            "Epoch 870\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.2076e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.207587632752226e-06\n",
            "----------------------------------------\n",
            "Epoch 871\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1965e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.196516661550964e-06\n",
            "----------------------------------------\n",
            "Epoch 872\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1853e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.1852984025598685e-06\n",
            "----------------------------------------\n",
            "Epoch 873\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1743e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.174250659546256e-06\n",
            "----------------------------------------\n",
            "Epoch 874\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1632e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.163163180810256e-06\n",
            "----------------------------------------\n",
            "Epoch 875\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1521e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.152121827766469e-06\n",
            "----------------------------------------\n",
            "Epoch 876\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1412e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.141164166020394e-06\n",
            "----------------------------------------\n",
            "Epoch 877\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1302e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.130220575599021e-06\n",
            "----------------------------------------\n",
            "Epoch 878\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1194e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.119373186024773e-06\n",
            "----------------------------------------\n",
            "Epoch 879\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.1086e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.108597314541698e-06\n",
            "----------------------------------------\n",
            "Epoch 880\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0979e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.09790063602426e-06\n",
            "----------------------------------------\n",
            "Epoch 881\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0873e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.087251541828544e-06\n",
            "----------------------------------------\n",
            "Epoch 882\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0766e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.076555318539462e-06\n",
            "----------------------------------------\n",
            "Epoch 883\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0660e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.066002312950064e-06\n",
            "----------------------------------------\n",
            "Epoch 884\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0555e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.055524649919032e-06\n",
            "----------------------------------------\n",
            "Epoch 885\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0449e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.044882275100243e-06\n",
            "----------------------------------------\n",
            "Epoch 886\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0343e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.034258117403481e-06\n",
            "----------------------------------------\n",
            "Epoch 887\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0238e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.023816203431153e-06\n",
            "----------------------------------------\n",
            "Epoch 888\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0134e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.013399852650723e-06\n",
            "----------------------------------------\n",
            "Epoch 889\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0029e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.002923051920174e-06\n",
            "----------------------------------------\n",
            "Epoch 890\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9925e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.992482435251689e-06\n",
            "----------------------------------------\n",
            "Epoch 891\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9821e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.9821346300589355e-06\n",
            "----------------------------------------\n",
            "Epoch 892\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9718e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.97180439716603e-06\n",
            "----------------------------------------\n",
            "Epoch 893\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9615e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.96151021619495e-06\n",
            "----------------------------------------\n",
            "Epoch 894\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9512e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.951152700059951e-06\n",
            "----------------------------------------\n",
            "Epoch 895\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9409e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.940938900394557e-06\n",
            "----------------------------------------\n",
            "Epoch 896\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9307e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.9307071378884275e-06\n",
            "----------------------------------------\n",
            "Epoch 897\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9205e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.920486168857144e-06\n",
            "----------------------------------------\n",
            "Epoch 898\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9103e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.9102940291907676e-06\n",
            "----------------------------------------\n",
            "Epoch 899\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.9001e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.9001117349174336e-06\n",
            "----------------------------------------\n",
            "Epoch 900\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8899e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.8899168716507156e-06\n",
            "----------------------------------------\n",
            "Epoch 901\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8797e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.879722911972981e-06\n",
            "----------------------------------------\n",
            "Epoch 902\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8695e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.869546529152836e-06\n",
            "----------------------------------------\n",
            "Epoch 903\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8594e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.8593994169870915e-06\n",
            "----------------------------------------\n",
            "Epoch 904\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8493e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.849283125814158e-06\n",
            "----------------------------------------\n",
            "Epoch 905\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8392e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.839166701820462e-06\n",
            "----------------------------------------\n",
            "Epoch 906\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8291e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.829071932964402e-06\n",
            "----------------------------------------\n",
            "Epoch 907\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8190e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.8190269004042555e-06\n",
            "----------------------------------------\n",
            "Epoch 908\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.8092e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.809175938133802e-06\n",
            "----------------------------------------\n",
            "Epoch 909\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7993e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.7993275674220465e-06\n",
            "----------------------------------------\n",
            "Epoch 910\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7895e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.789473477068608e-06\n",
            "----------------------------------------\n",
            "Epoch 911\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7796e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.779610564338512e-06\n",
            "----------------------------------------\n",
            "Epoch 912\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7698e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.7697641865245785e-06\n",
            "----------------------------------------\n",
            "Epoch 913\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7599e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.759901287749582e-06\n",
            "----------------------------------------\n",
            "Epoch 914\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7500e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.749977931509845e-06\n",
            "----------------------------------------\n",
            "Epoch 915\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7401e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.74005711536166e-06\n",
            "----------------------------------------\n",
            "Epoch 916\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7302e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.730181272237483e-06\n",
            "----------------------------------------\n",
            "Epoch 917\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7203e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.720314048829986e-06\n",
            "----------------------------------------\n",
            "Epoch 918\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7104e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.710434841245707e-06\n",
            "----------------------------------------\n",
            "Epoch 919\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7006e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.700605443710088e-06\n",
            "----------------------------------------\n",
            "Epoch 920\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6908e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.690822098434268e-06\n",
            "----------------------------------------\n",
            "Epoch 921\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6811e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.681076685665487e-06\n",
            "----------------------------------------\n",
            "Epoch 922\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6714e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.671354238553826e-06\n",
            "----------------------------------------\n",
            "Epoch 923\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6616e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.661644617101923e-06\n",
            "----------------------------------------\n",
            "Epoch 924\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6520e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.651969175425576e-06\n",
            "----------------------------------------\n",
            "Epoch 925\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6423e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.642336961972815e-06\n",
            "----------------------------------------\n",
            "Epoch 926\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6327e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.632713792896418e-06\n",
            "----------------------------------------\n",
            "Epoch 927\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6231e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.623122168402589e-06\n",
            "----------------------------------------\n",
            "Epoch 928\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6136e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.613588195885479e-06\n",
            "----------------------------------------\n",
            "Epoch 929\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.6041e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.604082886974572e-06\n",
            "----------------------------------------\n",
            "Epoch 930\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5946e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.594605343767997e-06\n",
            "----------------------------------------\n",
            "Epoch 931\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5852e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.585214720786681e-06\n",
            "----------------------------------------\n",
            "Epoch 932\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5759e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.575876582608836e-06\n",
            "----------------------------------------\n",
            "Epoch 933\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5666e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.566565402371411e-06\n",
            "----------------------------------------\n",
            "Epoch 934\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5573e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.5572927468761126e-06\n",
            "----------------------------------------\n",
            "Epoch 935\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5480e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.548037740526441e-06\n",
            "----------------------------------------\n",
            "Epoch 936\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5388e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.538814458172767e-06\n",
            "----------------------------------------\n",
            "Epoch 937\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5296e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.529617422222607e-06\n",
            "----------------------------------------\n",
            "Epoch 938\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5205e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.520465589268317e-06\n",
            "----------------------------------------\n",
            "Epoch 939\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5113e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.511329747184247e-06\n",
            "----------------------------------------\n",
            "Epoch 940\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5022e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.502203075066162e-06\n",
            "----------------------------------------\n",
            "Epoch 941\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4931e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.493116872740971e-06\n",
            "----------------------------------------\n",
            "Epoch 942\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4840e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.484040054513565e-06\n",
            "----------------------------------------\n",
            "Epoch 943\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4750e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.4749985676383265e-06\n",
            "----------------------------------------\n",
            "Epoch 944\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4660e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.465997593583453e-06\n",
            "----------------------------------------\n",
            "Epoch 945\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4570e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.457006201090774e-06\n",
            "----------------------------------------\n",
            "Epoch 946\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4480e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.448045306877989e-06\n",
            "----------------------------------------\n",
            "Epoch 947\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4391e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.439108439532241e-06\n",
            "----------------------------------------\n",
            "Epoch 948\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4302e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.430171713457378e-06\n",
            "----------------------------------------\n",
            "Epoch 949\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4213e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.4212711533638305e-06\n",
            "----------------------------------------\n",
            "Epoch 950\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4124e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.4124345145791485e-06\n",
            "----------------------------------------\n",
            "Epoch 951\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4036e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.403607418654391e-06\n",
            "----------------------------------------\n",
            "Epoch 952\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3948e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.394815631471154e-06\n",
            "----------------------------------------\n",
            "Epoch 953\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3860e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.386026739928362e-06\n",
            "----------------------------------------\n",
            "Epoch 954\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3772e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.377233455496574e-06\n",
            "----------------------------------------\n",
            "Epoch 955\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3684e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.368446346895171e-06\n",
            "----------------------------------------\n",
            "Epoch 956\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3597e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.359722221080673e-06\n",
            "----------------------------------------\n",
            "Epoch 957\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3510e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.351017474159306e-06\n",
            "----------------------------------------\n",
            "Epoch 958\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3423e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.342336084592606e-06\n",
            "----------------------------------------\n",
            "Epoch 959\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3337e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.333688648587514e-06\n",
            "----------------------------------------\n",
            "Epoch 960\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3251e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.325057787878992e-06\n",
            "----------------------------------------\n",
            "Epoch 961\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3164e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.3164381596847845e-06\n",
            "----------------------------------------\n",
            "Epoch 962\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3078e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.307827435558673e-06\n",
            "----------------------------------------\n",
            "Epoch 963\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2992e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.2992476350854304e-06\n",
            "----------------------------------------\n",
            "Epoch 964\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2907e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.29071877412872e-06\n",
            "----------------------------------------\n",
            "Epoch 965\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2822e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.2821963090279245e-06\n",
            "----------------------------------------\n",
            "Epoch 966\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2737e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.273668232812994e-06\n",
            "----------------------------------------\n",
            "Epoch 967\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2652e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.265167145533412e-06\n",
            "----------------------------------------\n",
            "Epoch 968\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2567e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.256689727540196e-06\n",
            "----------------------------------------\n",
            "Epoch 969\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2483e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.248296632267837e-06\n",
            "----------------------------------------\n",
            "Epoch 970\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2398e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.239814806018294e-06\n",
            "----------------------------------------\n",
            "Epoch 971\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2314e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.231404450146819e-06\n",
            "----------------------------------------\n",
            "Epoch 972\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2230e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.223002413154543e-06\n",
            "----------------------------------------\n",
            "Epoch 973\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2146e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.214621937106034e-06\n",
            "----------------------------------------\n",
            "Epoch 974\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2063e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.206264709457703e-06\n",
            "----------------------------------------\n",
            "Epoch 975\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1980e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.197956119099178e-06\n",
            "----------------------------------------\n",
            "Epoch 976\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1896e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.189641136705306e-06\n",
            "----------------------------------------\n",
            "Epoch 977\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1814e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.181395767840048e-06\n",
            "----------------------------------------\n",
            "Epoch 978\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1732e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.173233441268033e-06\n",
            "----------------------------------------\n",
            "Epoch 979\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1650e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.165035590883192e-06\n",
            "----------------------------------------\n",
            "Epoch 980\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1569e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.156930354061651e-06\n",
            "----------------------------------------\n",
            "Epoch 981\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1489e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.148853966873219e-06\n",
            "----------------------------------------\n",
            "Epoch 982\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1408e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.140782895061452e-06\n",
            "----------------------------------------\n",
            "Epoch 983\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1326e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.132614890319409e-06\n",
            "----------------------------------------\n",
            "Epoch 984\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1245e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.124466018701765e-06\n",
            "----------------------------------------\n",
            "Epoch 985\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1164e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.11641935301327e-06\n",
            "----------------------------------------\n",
            "Epoch 986\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1084e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.108387582364282e-06\n",
            "----------------------------------------\n",
            "Epoch 987\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1005e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.100472480701735e-06\n",
            "----------------------------------------\n",
            "Epoch 988\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0924e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.092407132352681e-06\n",
            "----------------------------------------\n",
            "Epoch 989\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0845e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.0844629952304134e-06\n",
            "----------------------------------------\n",
            "Epoch 990\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0766e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.076563157941235e-06\n",
            "----------------------------------------\n",
            "Epoch 991\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0687e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.068657002009449e-06\n",
            "----------------------------------------\n",
            "Epoch 992\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0608e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.060771900774584e-06\n",
            "----------------------------------------\n",
            "Epoch 993\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0529e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.052896907629822e-06\n",
            "----------------------------------------\n",
            "Epoch 994\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0450e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.045022629156914e-06\n",
            "----------------------------------------\n",
            "Epoch 995\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0372e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.037157314750649e-06\n",
            "----------------------------------------\n",
            "Epoch 996\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0293e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.029300901823153e-06\n",
            "----------------------------------------\n",
            "Epoch 997\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0214e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.021449635842377e-06\n",
            "----------------------------------------\n",
            "Epoch 998\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0136e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.013638560524795e-06\n",
            "----------------------------------------\n",
            "Epoch 999\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0058e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.005830628139071e-06\n",
            "----------------------------------------\n",
            "Epoch 1000\n",
            "IS variance:  tensor(4.4229e-05, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9982e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.9981560197614436e-06\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-5.5204e-04, -1.1648e-01],\n",
            "        [-4.5293e-01, -3.8106e-01],\n",
            "        [-2.5930e-01, -8.7127e-02],\n",
            "        [ 4.4813e-01, -1.6232e-01],\n",
            "        [-8.1455e-03,  7.8029e-02],\n",
            "        [ 1.1004e-01,  3.4091e-03],\n",
            "        [ 2.9693e-01,  4.3980e-01],\n",
            "        [ 6.2576e-01, -5.5355e-01],\n",
            "        [-1.0444e-01,  5.5520e-01],\n",
            "        [-3.0028e-01, -1.3046e-01],\n",
            "        [-5.3901e-01, -2.4041e-01],\n",
            "        [ 1.0723e-01,  1.4509e-01],\n",
            "        [-4.1380e-01,  2.4634e-01],\n",
            "        [ 1.7922e-01,  3.3803e-01],\n",
            "        [-3.7337e-01, -1.3246e-01],\n",
            "        [-6.9054e-01, -6.8765e-01]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.5092,  0.4352,  0.7202,  0.4023,  0.4572, -0.6651, -0.0837,  0.3502,\n",
            "        -0.6959,  0.6241, -0.0528, -0.1184, -0.6749,  0.5630, -0.3820,  0.4062],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[-0.1059,  0.0348, -0.1211, -0.1186, -0.2161,  0.2042, -0.0126,  0.1244,\n",
            "          0.2356,  0.0704,  0.2263, -0.0731,  0.1759, -0.1584, -0.1582, -0.0644],\n",
            "        [ 0.0482,  0.1365,  0.2008, -0.1384,  0.1320,  0.0593, -0.0738,  0.0601,\n",
            "         -0.0785, -0.0454,  0.1469, -0.2405, -0.0496,  0.0664,  0.0934,  0.0403],\n",
            "        [ 0.0710,  0.0682, -0.0611,  0.0600,  0.1012, -0.2105, -0.2516,  0.1933,\n",
            "         -0.1968, -0.0189,  0.1533, -0.1566, -0.1075,  0.1345, -0.1637,  0.0113],\n",
            "        [-0.1001,  0.0853, -0.1011,  0.1986,  0.1516, -0.1680, -0.0438, -0.0598,\n",
            "          0.1153, -0.1011, -0.1849,  0.0017, -0.0356,  0.0592,  0.1066, -0.2067],\n",
            "        [ 0.0485,  0.2396, -0.0166,  0.2245,  0.0383,  0.1002,  0.1858,  0.0793,\n",
            "          0.0560, -0.1151,  0.1724, -0.0213,  0.0387,  0.1261,  0.0363, -0.0657],\n",
            "        [ 0.1195,  0.1694, -0.1735, -0.2022, -0.1880, -0.1023, -0.0673,  0.1393,\n",
            "         -0.0087, -0.0683, -0.0222,  0.1336, -0.1221,  0.2136, -0.2079,  0.1589],\n",
            "        [-0.0781, -0.1996,  0.1168,  0.1149,  0.0527,  0.1633, -0.1541,  0.2709,\n",
            "          0.0268,  0.0679,  0.1485,  0.0339,  0.0422,  0.0662, -0.0014,  0.0425],\n",
            "        [-0.0461, -0.1450, -0.1952,  0.1243,  0.1511,  0.0309,  0.1699,  0.1166,\n",
            "         -0.2554, -0.2381,  0.0199,  0.1038, -0.1051, -0.0508, -0.2054, -0.1726],\n",
            "        [-0.1728,  0.1716,  0.3580, -0.1372, -0.0661,  0.1081,  0.1373,  0.1145,\n",
            "          0.0249,  0.1201, -0.0654,  0.0284,  0.0638, -0.1162,  0.0172,  0.1396],\n",
            "        [ 0.1042,  0.1746,  0.3908,  0.0058,  0.1786, -0.1368, -0.0759, -0.0043,\n",
            "          0.0850,  0.0507, -0.0090, -0.1311,  0.1271,  0.0124,  0.1397,  0.2339],\n",
            "        [ 0.2133,  0.0509,  0.2322, -0.1479, -0.2051, -0.0269, -0.1886, -0.1002,\n",
            "         -0.1174, -0.0421,  0.2283, -0.1851,  0.1653,  0.1272, -0.2184, -0.2322],\n",
            "        [ 0.2097,  0.2253,  0.3185,  0.1029,  0.0387,  0.0292,  0.0161,  0.0778,\n",
            "         -0.0773, -0.1021,  0.1186, -0.0954, -0.0134, -0.0401, -0.1185,  0.2487],\n",
            "        [ 0.0689, -0.0969, -0.0106,  0.0204,  0.0230,  0.2184, -0.2816, -0.0923,\n",
            "          0.2334,  0.1604,  0.0403, -0.1636,  0.1268, -0.0127,  0.1386,  0.1866],\n",
            "        [ 0.2480,  0.1819,  0.0612, -0.1424,  0.0304, -0.0997, -0.2740,  0.3344,\n",
            "          0.0511, -0.1114,  0.0226, -0.0524,  0.0449,  0.1124, -0.2474,  0.1864],\n",
            "        [ 0.1872,  0.2367, -0.2487,  0.0882,  0.0338,  0.1907,  0.1273, -0.1082,\n",
            "         -0.0604,  0.0663,  0.0350, -0.1319,  0.1821, -0.0847, -0.2468, -0.2229],\n",
            "        [ 0.2759,  0.0425, -0.4191, -0.1359, -0.0629,  0.0942, -0.0608, -0.3922,\n",
            "          0.2084, -0.1408, -0.2414, -0.0204,  0.0059, -0.0352,  0.0191,  0.0218],\n",
            "        [-0.1869, -0.0160,  0.1168,  0.0116, -0.0226, -0.1437,  0.0360,  0.0223,\n",
            "          0.1339,  0.1268,  0.1077, -0.1652,  0.2013, -0.1835, -0.0167,  0.2096],\n",
            "        [ 0.0782, -0.1589, -0.2673, -0.0680, -0.0904,  0.1165, -0.0635, -0.1919,\n",
            "         -0.1050,  0.2005, -0.1449, -0.0545,  0.0925,  0.0203,  0.1139,  0.1484],\n",
            "        [ 0.1595,  0.0643, -0.3432,  0.1120,  0.0515, -0.1801, -0.0238,  0.2495,\n",
            "          0.0111, -0.0368,  0.1770, -0.0914, -0.0333,  0.0507, -0.2133, -0.0470],\n",
            "        [-0.1760, -0.0565, -0.0034, -0.0679,  0.2087, -0.0109, -0.3103, -0.1276,\n",
            "         -0.0425,  0.2443,  0.0807, -0.1902,  0.1366, -0.1344,  0.2271,  0.0099],\n",
            "        [ 0.0740, -0.2291,  0.3031, -0.0556,  0.2903, -0.1666, -0.2427,  0.0299,\n",
            "         -0.1052,  0.2062,  0.1986,  0.2072, -0.1006, -0.2910, -0.1149,  0.2293],\n",
            "        [-0.2314,  0.0108,  0.1455,  0.0379,  0.0895,  0.1859,  0.1108,  0.0656,\n",
            "         -0.1844, -0.0440,  0.1627,  0.1713, -0.1886,  0.1660,  0.2276, -0.0430],\n",
            "        [-0.1424, -0.2437, -0.2496,  0.1048,  0.1263, -0.0228, -0.1983, -0.1492,\n",
            "          0.0542, -0.0686, -0.0583,  0.2295,  0.1212, -0.0762, -0.2098, -0.1740],\n",
            "        [-0.0499, -0.0182, -0.0033, -0.2612,  0.1046,  0.1527, -0.2250,  0.1123,\n",
            "          0.3199,  0.0036, -0.0979,  0.0189,  0.4894,  0.1434,  0.0015,  0.1857],\n",
            "        [-0.0472, -0.2177, -0.3356, -0.2372, -0.1786, -0.0646,  0.1518,  0.1087,\n",
            "          0.1342,  0.0026, -0.0035, -0.0459,  0.0608,  0.1693, -0.1012, -0.1482],\n",
            "        [ 0.2154,  0.1541, -0.2505,  0.0957, -0.0772, -0.0424, -0.0055,  0.0201,\n",
            "          0.1128, -0.0870,  0.1013, -0.2620,  0.1731,  0.1945, -0.1662, -0.1743],\n",
            "        [ 0.0804,  0.2025,  0.1082,  0.1488,  0.0771, -0.1524, -0.0055,  0.1546,\n",
            "          0.0714,  0.0475,  0.0299, -0.0967, -0.2741, -0.1917, -0.2107,  0.1779],\n",
            "        [ 0.0226,  0.2049, -0.0964, -0.1163, -0.3449,  0.1606, -0.2469, -0.1766,\n",
            "          0.0112,  0.1923,  0.1600,  0.0973,  0.0950,  0.1102,  0.2348,  0.0826],\n",
            "        [-0.1786, -0.0534, -0.1874, -0.2366, -0.2194, -0.2432, -0.1082, -0.0627,\n",
            "         -0.2210, -0.1653,  0.0558, -0.0767,  0.0716, -0.0069,  0.0766,  0.1254],\n",
            "        [ 0.0998, -0.0821, -0.0266,  0.3557, -0.1546,  0.1037, -0.1897, -0.1107,\n",
            "          0.2187, -0.1475,  0.0765, -0.0652, -0.1617, -0.2278, -0.1803, -0.0159],\n",
            "        [ 0.0723, -0.0547,  0.1122, -0.1597, -0.2257,  0.0456,  0.0558, -0.1876,\n",
            "         -0.1533,  0.0010, -0.0860,  0.1341, -0.0255, -0.1400, -0.2324, -0.0714],\n",
            "        [-0.1025,  0.2154, -0.1576, -0.0725, -0.3789, -0.1893,  0.0019, -0.0331,\n",
            "         -0.0127, -0.2737, -0.1962, -0.3930,  0.1908, -0.0846,  0.0596,  0.0310]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.0779,  0.0979, -0.1855, -0.0197, -0.2061, -0.0197, -0.0312, -0.1266,\n",
            "         0.1745,  0.1747,  0.1095,  0.0302, -0.2134,  0.1202,  0.1321, -0.0621,\n",
            "        -0.0627,  0.1290, -0.1620,  0.2175, -0.1138, -0.1350,  0.0453, -0.1198,\n",
            "         0.1928,  0.0903, -0.2025, -0.2451, -0.0756, -0.0444,  0.1104,  0.2030],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[-0.0050,  0.1012, -0.1624,  0.0061,  0.0033, -0.0260,  0.0611, -0.0235,\n",
            "          0.0087,  0.0359, -0.1329,  0.0868, -0.0292,  0.0288, -0.0205, -0.0243,\n",
            "          0.0543,  0.1526,  0.1952, -0.0964,  0.0931,  0.0370, -0.0143, -0.0043,\n",
            "          0.0086, -0.0133, -0.3197,  0.1091, -0.1526,  0.0273,  0.1383, -0.1366]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.0110], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 200 0.99"
      ],
      "metadata": {
        "id": "TRlCPzkUEvi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(200, env, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e = experiment_actions(200, env, P_pi_e)"
      ],
      "metadata": {
        "id": "Hn4yWSa6Ey1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b = experiment_actions(200, env_30, P_pi_b)\n",
        "P_pi_e = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e = experiment_actions(200, env, P_pi_e)\n",
        "model_200_0p99 = CustomizableFeatureNet(input_dim=2, hidden_dims=[6, 6], output_dim=1, dtype = torch.float64)\n",
        "test_200_0p99 = SCOPE_straight(model_200_0p99, 0.99, 10000, pi_b, P_pi_b, P_pi_e, dtype = torch.float64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFPqTUz9EzVg",
        "outputId": "c24dbb18-401e-4404-e503-19176586c47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.8547, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.8546716723749704\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.8345, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.8344950296004069\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.8157, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.815702575610253\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.7972, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.7971985257757953\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.7790, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.7790125020004666\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.7612, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.761163863570924\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.7437, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.7436643420651126\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.7265, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.7265202843486895\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.7097, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.7097352689463908\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6937, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6936950996361412\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6787, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6787496091902381\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6641, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6641326426056419\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6491, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6491106577802083\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6334, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6333667722630629\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6176, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6176165169242289\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.6020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.6019587691009125\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5865, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.5864614333403655\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5712, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.5711710016875733\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5561, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.5561201093237699\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5413, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.5413312509285723\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5268, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.5268211222619228\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.5128, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.5127662288046468\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4991, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.4990903180712609\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4857, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.48571725956488965\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4727, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.4726511735617668\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4599, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.45988918164358644\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4474, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.44742813206934556\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4353, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.4352655622205185\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4234, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.4233983242252763\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4118, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.4118227108286335\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.4021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.4021134038855513\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3926, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.39257963185991446\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3831, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.38308203997671986\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3737, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.3736948442815689\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3645, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.36445031056541616\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3554, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.35536750421714974\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3465, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.34645815470296126\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3377, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.3377294703442143\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3287, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.32871648276595317\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3179, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.31790126888277037\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.3071, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.3070625734259544\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2963, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.29627239376597064\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2856, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.28558616595403796\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2750, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.27504909881481915\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2647, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.26469327157199624\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2545, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.25454693646675536\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2446, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.24462840025074462\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2350, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.234953140051005\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2255, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.22553328412755216\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2164, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.21637794682926711\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2077, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.20765838692198108\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1994, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.19944057999559758\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1914, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.19136714645965625\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1835, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.18345732333640694\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1757, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.17572677515007384\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1682, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.16818803679903824\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1609, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.16090624467500525\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1541, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.1541089350798193\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1475, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.14753160424431366\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1412, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.14117376185585162\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1350, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.1350122598852806\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1291, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.12905755269560512\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1233, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.12331488088602341\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1178, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.11778112624286458\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1125, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.11245276892457491\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1074, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.10736790659156419\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.1025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.1025438984825482\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0979, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.09787412170464208\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0933, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.09334454892496756\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0890, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.088957830242839\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0847, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.08471570257624039\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0806, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.08061900739904902\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0767, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0766678274095598\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0729, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.07286188557593551\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0692, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.06919982278324037\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0657, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.06567997995547233\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0623, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.06230029651117082\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0591, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.059058377477843316\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0559, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.05594751305442548\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0530, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.052966288055453295\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0501, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.050113888317941106\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0474, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.04738722108288303\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0448, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.04478295871729696\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0423, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.04229763834861956\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0399, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.03992773570038501\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0377, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.037669457929341316\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0355, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0355193769124232\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0335, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.03347395745564995\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0315, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.031529609108374254\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0297, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.02968271250845121\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0279, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.027932768635906338\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0263, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.026305800771803944\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0248, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.024764322857021973\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0233, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.02330437486812094\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0219, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.02192264482312758\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0206, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.020615676357044713\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0194, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.01938006119066497\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0182, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.018212120834255302\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0171, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.017108966523152948\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0161, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.016066395022520556\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0151, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.015082014883762413\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0142, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.014153956836780278\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0133, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.013279728808191522\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0125, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.012457231985769056\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0117, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.011683105457310276\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0113, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.011342913577924043\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0112, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.011180702497889685\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0110, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.011022237359410294\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0109, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010869776597278611\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0107, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010719522645914525\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0106, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010574777320221747\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0104, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010437090352892598\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0103, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010304455074789026\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0102, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010176327263684923\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.010054057177073139\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0099, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.009935777820803718\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0098, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.009820751026385733\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0097, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00970874948631789\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0096, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.009599568012298087\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0095, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00949302114531486\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0094, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.009388941046875874\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0093, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00928717563506485\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0092, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.009187586934257173\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0091, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.009090049611678653\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0090, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008994449677683479\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0089, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008900683329802086\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0088, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008808655923198792\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0087, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008718281052589538\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0086, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00862947973253226\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0085, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008542179664715939\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0085, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008456314584310247\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0084, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008371823668269196\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0083, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008288661209704297\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0082, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008206768155038686\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0081, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008127881769166252\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0081, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.008053068775143953\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0080, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007975731942808041\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0079, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007896228550827554\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0078, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007821717120735011\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0077, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00774887220816159\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0077, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007676765820170244\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0076, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007605394924886864\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0075, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007534756645308239\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0075, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007464847531560026\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0074, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007395663626410915\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0073, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007327200524725006\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0073, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007259453427351505\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0072, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007192417678624616\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0071, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007126087383820872\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0071, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.007060456797068244\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0070, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006995519991076635\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0069, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006931270845352345\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0069, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006867703074766262\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0068, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006804810257151416\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0067, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006744395701342055\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0067, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006683942467567038\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0066, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006622084782085688\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0066, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006562973766531543\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0065, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006504374055503997\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0064, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0064462946883922885\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0064, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006388741330523116\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0063, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00633171551006158\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0063, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006275219631772799\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0062, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006219282434222159\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0062, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006164465507695038\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0061, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0061101379558747585\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0061, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006056278845595384\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0060, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.006002893239057439\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0059, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005949985106260439\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0059, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005898682865317154\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0058, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005846501613345426\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0058, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005795741430318968\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0057, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005745371177043594\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0057, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005695388445082429\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0056, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005645809115981477\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0056, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005597565871908052\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0055, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005548486469223402\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0055, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005500687588614943\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0055, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005453322596869906\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0054, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005406428182001911\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0054, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00535991919315069\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0053, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005313930317373878\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0053, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005268645073306434\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0052, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005223825966489567\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0052, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0051793524821357765\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0051, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005135232092539864\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0051, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005091471085501814\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0050, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005048074801492975\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0050, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005005651857176431\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0050, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00496293353017183\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0049, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004921147681785879\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0049, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004879736006744593\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0048, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004838633802147718\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0048, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004797849085388216\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0048, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004758342649854319\n",
            "----------------------------------------\n",
            "Epoch 201\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0047, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004717834883760589\n",
            "----------------------------------------\n",
            "Epoch 202\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0047, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004678563087059537\n",
            "----------------------------------------\n",
            "Epoch 203\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0046, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004639583265879986\n",
            "----------------------------------------\n",
            "Epoch 204\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0046, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004600902497375698\n",
            "----------------------------------------\n",
            "Epoch 205\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0046, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004562558611391957\n",
            "----------------------------------------\n",
            "Epoch 206\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0045, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004524752132582197\n",
            "----------------------------------------\n",
            "Epoch 207\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0045, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004487267497615992\n",
            "----------------------------------------\n",
            "Epoch 208\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0045, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004450305403283982\n",
            "----------------------------------------\n",
            "Epoch 209\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0044, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004413621927511726\n",
            "----------------------------------------\n",
            "Epoch 210\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0044, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004380276312199833\n",
            "----------------------------------------\n",
            "Epoch 211\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0043, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004347794941432448\n",
            "----------------------------------------\n",
            "Epoch 212\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0043, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004312246269516688\n",
            "----------------------------------------\n",
            "Epoch 213\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0043, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004277212033990483\n",
            "----------------------------------------\n",
            "Epoch 214\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0042, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004240766660121044\n",
            "----------------------------------------\n",
            "Epoch 215\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0042, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004203105753520934\n",
            "----------------------------------------\n",
            "Epoch 216\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0042, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004167187609976871\n",
            "----------------------------------------\n",
            "Epoch 217\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0041, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0041274968982973614\n",
            "----------------------------------------\n",
            "Epoch 218\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0041, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004088275959400935\n",
            "----------------------------------------\n",
            "Epoch 219\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0040, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004049404203066784\n",
            "----------------------------------------\n",
            "Epoch 220\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0040, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.004009802907216467\n",
            "----------------------------------------\n",
            "Epoch 221\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0040, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003969598803202217\n",
            "----------------------------------------\n",
            "Epoch 222\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003931009339283031\n",
            "----------------------------------------\n",
            "Epoch 223\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0038909835262761774\n",
            "----------------------------------------\n",
            "Epoch 224\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0039, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003852249326284995\n",
            "----------------------------------------\n",
            "Epoch 225\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0038154266741377988\n",
            "----------------------------------------\n",
            "Epoch 226\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0038, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0037782966894316958\n",
            "----------------------------------------\n",
            "Epoch 227\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0037406923524199525\n",
            "----------------------------------------\n",
            "Epoch 228\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0037027104211728615\n",
            "----------------------------------------\n",
            "Epoch 229\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0037, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0036644387059539844\n",
            "----------------------------------------\n",
            "Epoch 230\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0036259561758741343\n",
            "----------------------------------------\n",
            "Epoch 231\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0036, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0035873340671417693\n",
            "----------------------------------------\n",
            "Epoch 232\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0035498575816885532\n",
            "----------------------------------------\n",
            "Epoch 233\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0035137252880671163\n",
            "----------------------------------------\n",
            "Epoch 234\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0035, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0034771764375151324\n",
            "----------------------------------------\n",
            "Epoch 235\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0034406799530181617\n",
            "----------------------------------------\n",
            "Epoch 236\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0034050940073021858\n",
            "----------------------------------------\n",
            "Epoch 237\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0034, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003369205994145485\n",
            "----------------------------------------\n",
            "Epoch 238\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0033330940652911457\n",
            "----------------------------------------\n",
            "Epoch 239\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0032975089801288247\n",
            "----------------------------------------\n",
            "Epoch 240\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0033, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003263995372325778\n",
            "----------------------------------------\n",
            "Epoch 241\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003229933705532338\n",
            "----------------------------------------\n",
            "Epoch 242\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003194965188322567\n",
            "----------------------------------------\n",
            "Epoch 243\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0032, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0031614555083920876\n",
            "----------------------------------------\n",
            "Epoch 244\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003128298186928097\n",
            "----------------------------------------\n",
            "Epoch 245\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.003095087022228678\n",
            "----------------------------------------\n",
            "Epoch 246\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0031, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0030624423837157322\n",
            "----------------------------------------\n",
            "Epoch 247\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0030296016042369524\n",
            "----------------------------------------\n",
            "Epoch 248\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0029968038042988312\n",
            "----------------------------------------\n",
            "Epoch 249\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0030, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0029648621633291913\n",
            "----------------------------------------\n",
            "Epoch 250\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0029336828362678978\n",
            "----------------------------------------\n",
            "Epoch 251\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002902063357916683\n",
            "----------------------------------------\n",
            "Epoch 252\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0029, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002871873313538257\n",
            "----------------------------------------\n",
            "Epoch 253\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002841618716728247\n",
            "----------------------------------------\n",
            "Epoch 254\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0028118252567851626\n",
            "----------------------------------------\n",
            "Epoch 255\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0027819871259834672\n",
            "----------------------------------------\n",
            "Epoch 256\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0028, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0027521081311857945\n",
            "----------------------------------------\n",
            "Epoch 257\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0027222301530419775\n",
            "----------------------------------------\n",
            "Epoch 258\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002694197551154959\n",
            "----------------------------------------\n",
            "Epoch 259\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0027, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0026660688744658124\n",
            "----------------------------------------\n",
            "Epoch 260\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002637505433064731\n",
            "----------------------------------------\n",
            "Epoch 261\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0026084026861855005\n",
            "----------------------------------------\n",
            "Epoch 262\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002580850796633104\n",
            "----------------------------------------\n",
            "Epoch 263\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0026, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00255380875616087\n",
            "----------------------------------------\n",
            "Epoch 264\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0025267630039775843\n",
            "----------------------------------------\n",
            "Epoch 265\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00250001731427122\n",
            "----------------------------------------\n",
            "Epoch 266\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0025, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0024733673118707213\n",
            "----------------------------------------\n",
            "Epoch 267\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002447453594632654\n",
            "----------------------------------------\n",
            "Epoch 268\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0024215791143245126\n",
            "----------------------------------------\n",
            "Epoch 269\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002396174798345287\n",
            "----------------------------------------\n",
            "Epoch 270\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0024, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0023707946409597946\n",
            "----------------------------------------\n",
            "Epoch 271\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0023454885158656815\n",
            "----------------------------------------\n",
            "Epoch 272\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0023205889623067757\n",
            "----------------------------------------\n",
            "Epoch 273\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0022961210399871264\n",
            "----------------------------------------\n",
            "Epoch 274\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0023, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002272196914480386\n",
            "----------------------------------------\n",
            "Epoch 275\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0022482202971244496\n",
            "----------------------------------------\n",
            "Epoch 276\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0022242332146298702\n",
            "----------------------------------------\n",
            "Epoch 277\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0022015418178001874\n",
            "----------------------------------------\n",
            "Epoch 278\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0021786352210143543\n",
            "----------------------------------------\n",
            "Epoch 279\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0022, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0021552920226886076\n",
            "----------------------------------------\n",
            "Epoch 280\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0021327727917707317\n",
            "----------------------------------------\n",
            "Epoch 281\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002110641087452761\n",
            "----------------------------------------\n",
            "Epoch 282\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0020887613356420746\n",
            "----------------------------------------\n",
            "Epoch 283\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0021, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0020668416786557365\n",
            "----------------------------------------\n",
            "Epoch 284\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002044915099760511\n",
            "----------------------------------------\n",
            "Epoch 285\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002023322519608584\n",
            "----------------------------------------\n",
            "Epoch 286\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.002002763615305977\n",
            "----------------------------------------\n",
            "Epoch 287\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001982121914126294\n",
            "----------------------------------------\n",
            "Epoch 288\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0020, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0019612276218746844\n",
            "----------------------------------------\n",
            "Epoch 289\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001940268246372342\n",
            "----------------------------------------\n",
            "Epoch 290\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001920379034445509\n",
            "----------------------------------------\n",
            "Epoch 291\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0019004476057686171\n",
            "----------------------------------------\n",
            "Epoch 292\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0018805071867076574\n",
            "----------------------------------------\n",
            "Epoch 293\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0019, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0018611948164111507\n",
            "----------------------------------------\n",
            "Epoch 294\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0018419817386779612\n",
            "----------------------------------------\n",
            "Epoch 295\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0018230014607385399\n",
            "----------------------------------------\n",
            "Epoch 296\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0018041461237785325\n",
            "----------------------------------------\n",
            "Epoch 297\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0017855587760095897\n",
            "----------------------------------------\n",
            "Epoch 298\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0018, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0017669611615841435\n",
            "----------------------------------------\n",
            "Epoch 299\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001749272264864262\n",
            "----------------------------------------\n",
            "Epoch 300\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0017315572465489608\n",
            "----------------------------------------\n",
            "Epoch 301\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0017134210936991957\n",
            "----------------------------------------\n",
            "Epoch 302\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0016959944185007871\n",
            "----------------------------------------\n",
            "Epoch 303\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001678989113777594\n",
            "----------------------------------------\n",
            "Epoch 304\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0017, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0016620447517144107\n",
            "----------------------------------------\n",
            "Epoch 305\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0016450720030536927\n",
            "----------------------------------------\n",
            "Epoch 306\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001628099743668732\n",
            "----------------------------------------\n",
            "Epoch 307\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0016111489750585289\n",
            "----------------------------------------\n",
            "Epoch 308\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0015954247655366439\n",
            "----------------------------------------\n",
            "Epoch 309\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001579632828789209\n",
            "----------------------------------------\n",
            "Epoch 310\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0016, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0015634376479176488\n",
            "----------------------------------------\n",
            "Epoch 311\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0015471617564655573\n",
            "----------------------------------------\n",
            "Epoch 312\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0015319451038816152\n",
            "----------------------------------------\n",
            "Epoch 313\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001516574396806452\n",
            "----------------------------------------\n",
            "Epoch 314\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0015010398515153598\n",
            "----------------------------------------\n",
            "Epoch 315\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0014853835457157199\n",
            "----------------------------------------\n",
            "Epoch 316\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0014697745268949162\n",
            "----------------------------------------\n",
            "Epoch 317\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0015, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001454085361399833\n",
            "----------------------------------------\n",
            "Epoch 318\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0014382371258495094\n",
            "----------------------------------------\n",
            "Epoch 319\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0014231376927888912\n",
            "----------------------------------------\n",
            "Epoch 320\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0014082641480695005\n",
            "----------------------------------------\n",
            "Epoch 321\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0013940163722198813\n",
            "----------------------------------------\n",
            "Epoch 322\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0013775471158843954\n",
            "----------------------------------------\n",
            "Epoch 323\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0014, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001362554621608391\n",
            "----------------------------------------\n",
            "Epoch 324\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0013456469770667032\n",
            "----------------------------------------\n",
            "Epoch 325\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0013282705151685204\n",
            "----------------------------------------\n",
            "Epoch 326\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0013104633615438981\n",
            "----------------------------------------\n",
            "Epoch 327\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0012922268404032041\n",
            "----------------------------------------\n",
            "Epoch 328\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.001273853847479378\n",
            "----------------------------------------\n",
            "Epoch 329\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0013, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0012557244089278668\n",
            "----------------------------------------\n",
            "Epoch 330\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0012373629313586227\n",
            "----------------------------------------\n",
            "Epoch 331\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0012177641314448534\n",
            "----------------------------------------\n",
            "Epoch 332\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0011977798007275515\n",
            "----------------------------------------\n",
            "Epoch 333\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0011782542286178535\n",
            "----------------------------------------\n",
            "Epoch 334\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0012, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0011585501652845904\n",
            "----------------------------------------\n",
            "Epoch 335\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0011346905256367043\n",
            "----------------------------------------\n",
            "Epoch 336\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0011041609069554013\n",
            "----------------------------------------\n",
            "Epoch 337\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0011, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0010744883525759382\n",
            "----------------------------------------\n",
            "Epoch 338\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0010441761847287943\n",
            "----------------------------------------\n",
            "Epoch 339\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0010143799094075097\n",
            "----------------------------------------\n",
            "Epoch 340\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0009848751401308508\n",
            "----------------------------------------\n",
            "Epoch 341\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0010, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000955346152842433\n",
            "----------------------------------------\n",
            "Epoch 342\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000923429748031222\n",
            "----------------------------------------\n",
            "Epoch 343\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000891988612060904\n",
            "----------------------------------------\n",
            "Epoch 344\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0009, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0008620014901299793\n",
            "----------------------------------------\n",
            "Epoch 345\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0008330483619741713\n",
            "----------------------------------------\n",
            "Epoch 346\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000804933611751626\n",
            "----------------------------------------\n",
            "Epoch 347\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007774862353237556\n",
            "----------------------------------------\n",
            "Epoch 348\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0008, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007508325484410617\n",
            "----------------------------------------\n",
            "Epoch 349\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007255744901805848\n",
            "----------------------------------------\n",
            "Epoch 350\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0007013321125494831\n",
            "----------------------------------------\n",
            "Epoch 351\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006782316782719347\n",
            "----------------------------------------\n",
            "Epoch 352\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0007, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006564792524678985\n",
            "----------------------------------------\n",
            "Epoch 353\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006357222359547283\n",
            "----------------------------------------\n",
            "Epoch 354\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0006060479559767884\n",
            "----------------------------------------\n",
            "Epoch 355\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005757659998306523\n",
            "----------------------------------------\n",
            "Epoch 356\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0006, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005527655179251005\n",
            "----------------------------------------\n",
            "Epoch 357\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005354388693546488\n",
            "----------------------------------------\n",
            "Epoch 358\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005215574141305902\n",
            "----------------------------------------\n",
            "Epoch 359\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0005106774329698534\n",
            "----------------------------------------\n",
            "Epoch 360\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000501490453558698\n",
            "----------------------------------------\n",
            "Epoch 361\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004928403230214485\n",
            "----------------------------------------\n",
            "Epoch 362\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00048424476755108556\n",
            "----------------------------------------\n",
            "Epoch 363\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004746704877561292\n",
            "----------------------------------------\n",
            "Epoch 364\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004641593668757794\n",
            "----------------------------------------\n",
            "Epoch 365\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0005, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00045332896497080353\n",
            "----------------------------------------\n",
            "Epoch 366\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004425779553906703\n",
            "----------------------------------------\n",
            "Epoch 367\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004314530582880676\n",
            "----------------------------------------\n",
            "Epoch 368\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0004202141836297342\n",
            "----------------------------------------\n",
            "Epoch 369\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00040927741231566267\n",
            "----------------------------------------\n",
            "Epoch 370\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003990015951528856\n",
            "----------------------------------------\n",
            "Epoch 371\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00038955485631928796\n",
            "----------------------------------------\n",
            "Epoch 372\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00038100944125670107\n",
            "----------------------------------------\n",
            "Epoch 373\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003733366079823045\n",
            "----------------------------------------\n",
            "Epoch 374\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003665685855314465\n",
            "----------------------------------------\n",
            "Epoch 375\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00036033234734610727\n",
            "----------------------------------------\n",
            "Epoch 376\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0004, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00035458788503525316\n",
            "----------------------------------------\n",
            "Epoch 377\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.000348058630473583\n",
            "----------------------------------------\n",
            "Epoch 378\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00033947873307019547\n",
            "----------------------------------------\n",
            "Epoch 379\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00033025998788080327\n",
            "----------------------------------------\n",
            "Epoch 380\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003205026866896507\n",
            "----------------------------------------\n",
            "Epoch 381\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00031046374798354893\n",
            "----------------------------------------\n",
            "Epoch 382\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0003001810798512492\n",
            "----------------------------------------\n",
            "Epoch 383\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00028982515711268263\n",
            "----------------------------------------\n",
            "Epoch 384\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00027966795437917686\n",
            "----------------------------------------\n",
            "Epoch 385\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00027030744434853053\n",
            "----------------------------------------\n",
            "Epoch 386\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002627508230190878\n",
            "----------------------------------------\n",
            "Epoch 387\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0003, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002555573467011567\n",
            "----------------------------------------\n",
            "Epoch 388\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002486300651710869\n",
            "----------------------------------------\n",
            "Epoch 389\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002421377087176469\n",
            "----------------------------------------\n",
            "Epoch 390\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00023608910347038263\n",
            "----------------------------------------\n",
            "Epoch 391\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002304596087958113\n",
            "----------------------------------------\n",
            "Epoch 392\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00022509120144213618\n",
            "----------------------------------------\n",
            "Epoch 393\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00021993870692853243\n",
            "----------------------------------------\n",
            "Epoch 394\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002149928057170318\n",
            "----------------------------------------\n",
            "Epoch 395\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00021025006153355157\n",
            "----------------------------------------\n",
            "Epoch 396\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0002057025268646185\n",
            "----------------------------------------\n",
            "Epoch 397\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00020131042512004977\n",
            "----------------------------------------\n",
            "Epoch 398\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001970658355520743\n",
            "----------------------------------------\n",
            "Epoch 399\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00019297794080183142\n",
            "----------------------------------------\n",
            "Epoch 400\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00018914210640103555\n",
            "----------------------------------------\n",
            "Epoch 401\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00018551038882905365\n",
            "----------------------------------------\n",
            "Epoch 402\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001820520038178839\n",
            "----------------------------------------\n",
            "Epoch 403\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017876948726092996\n",
            "----------------------------------------\n",
            "Epoch 404\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001756626742500793\n",
            "----------------------------------------\n",
            "Epoch 405\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00017273369147808137\n",
            "----------------------------------------\n",
            "Epoch 406\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016997469611855585\n",
            "----------------------------------------\n",
            "Epoch 407\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016734929977990464\n",
            "----------------------------------------\n",
            "Epoch 408\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016485231447057835\n",
            "----------------------------------------\n",
            "Epoch 409\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00016249194240447365\n",
            "----------------------------------------\n",
            "Epoch 410\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001602472935378069\n",
            "----------------------------------------\n",
            "Epoch 411\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015810488819045652\n",
            "----------------------------------------\n",
            "Epoch 412\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015605248952082668\n",
            "----------------------------------------\n",
            "Epoch 413\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015406964834989092\n",
            "----------------------------------------\n",
            "Epoch 414\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00015220250459948174\n",
            "----------------------------------------\n",
            "Epoch 415\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0002, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001504320532635276\n",
            "----------------------------------------\n",
            "Epoch 416\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001487337350639054\n",
            "----------------------------------------\n",
            "Epoch 417\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001470971649972191\n",
            "----------------------------------------\n",
            "Epoch 418\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001455207784503063\n",
            "----------------------------------------\n",
            "Epoch 419\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014400657688069777\n",
            "----------------------------------------\n",
            "Epoch 420\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00014254782019648297\n",
            "----------------------------------------\n",
            "Epoch 421\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001411392067566479\n",
            "----------------------------------------\n",
            "Epoch 422\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013977735136350703\n",
            "----------------------------------------\n",
            "Epoch 423\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013845931181420874\n",
            "----------------------------------------\n",
            "Epoch 424\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013718221015337258\n",
            "----------------------------------------\n",
            "Epoch 425\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001359333556896158\n",
            "----------------------------------------\n",
            "Epoch 426\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001347192116079215\n",
            "----------------------------------------\n",
            "Epoch 427\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013353819263737756\n",
            "----------------------------------------\n",
            "Epoch 428\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013238820356297005\n",
            "----------------------------------------\n",
            "Epoch 429\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013126718144396013\n",
            "----------------------------------------\n",
            "Epoch 430\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00013017346693689578\n",
            "----------------------------------------\n",
            "Epoch 431\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012910573585163182\n",
            "----------------------------------------\n",
            "Epoch 432\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012806230548860037\n",
            "----------------------------------------\n",
            "Epoch 433\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012704166120768427\n",
            "----------------------------------------\n",
            "Epoch 434\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012604250441508081\n",
            "----------------------------------------\n",
            "Epoch 435\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001250636492978868\n",
            "----------------------------------------\n",
            "Epoch 436\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001241040224849837\n",
            "----------------------------------------\n",
            "Epoch 437\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012316266113518632\n",
            "----------------------------------------\n",
            "Epoch 438\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012223870754101974\n",
            "----------------------------------------\n",
            "Epoch 439\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001213314099062881\n",
            "----------------------------------------\n",
            "Epoch 440\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00012043013120516517\n",
            "----------------------------------------\n",
            "Epoch 441\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011951294546292964\n",
            "----------------------------------------\n",
            "Epoch 442\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011860824418407979\n",
            "----------------------------------------\n",
            "Epoch 443\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011771542270500585\n",
            "----------------------------------------\n",
            "Epoch 444\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011683455388081289\n",
            "----------------------------------------\n",
            "Epoch 445\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011596566650270851\n",
            "----------------------------------------\n",
            "Epoch 446\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011510881796107951\n",
            "----------------------------------------\n",
            "Epoch 447\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001142640798546647\n",
            "----------------------------------------\n",
            "Epoch 448\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011343154017157597\n",
            "----------------------------------------\n",
            "Epoch 449\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011261141396677496\n",
            "----------------------------------------\n",
            "Epoch 450\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011180367323267924\n",
            "----------------------------------------\n",
            "Epoch 451\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011100854528419645\n",
            "----------------------------------------\n",
            "Epoch 452\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00011022970784588333\n",
            "----------------------------------------\n",
            "Epoch 453\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001094688779074053\n",
            "----------------------------------------\n",
            "Epoch 454\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010872155927584015\n",
            "----------------------------------------\n",
            "Epoch 455\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010798761796236049\n",
            "----------------------------------------\n",
            "Epoch 456\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010726910582915247\n",
            "----------------------------------------\n",
            "Epoch 457\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010656538911198648\n",
            "----------------------------------------\n",
            "Epoch 458\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010587508340766908\n",
            "----------------------------------------\n",
            "Epoch 459\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010519822356570929\n",
            "----------------------------------------\n",
            "Epoch 460\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001045348510920827\n",
            "----------------------------------------\n",
            "Epoch 461\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010388501014020814\n",
            "----------------------------------------\n",
            "Epoch 462\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010324874420823071\n",
            "----------------------------------------\n",
            "Epoch 463\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010262609330777083\n",
            "----------------------------------------\n",
            "Epoch 464\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010201709167558936\n",
            "----------------------------------------\n",
            "Epoch 465\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010142176598871864\n",
            "----------------------------------------\n",
            "Epoch 466\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0001008401340523783\n",
            "----------------------------------------\n",
            "Epoch 467\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0001, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.00010027222489698721\n",
            "----------------------------------------\n",
            "Epoch 468\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9718e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.971796432054721e-05\n",
            "----------------------------------------\n",
            "Epoch 469\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.9177e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.917734263558461e-05\n",
            "----------------------------------------\n",
            "Epoch 470\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8650e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.865044255195552e-05\n",
            "----------------------------------------\n",
            "Epoch 471\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.8137e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.813712776023634e-05\n",
            "----------------------------------------\n",
            "Epoch 472\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7637e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.763735251167343e-05\n",
            "----------------------------------------\n",
            "Epoch 473\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7151e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.715105542997485e-05\n",
            "----------------------------------------\n",
            "Epoch 474\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6678e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.667815116795096e-05\n",
            "----------------------------------------\n",
            "Epoch 475\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.6218e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.62183189122225e-05\n",
            "----------------------------------------\n",
            "Epoch 476\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5771e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.577145001745294e-05\n",
            "----------------------------------------\n",
            "Epoch 477\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.5338e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.53376261741958e-05\n",
            "----------------------------------------\n",
            "Epoch 478\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4917e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.491671238201975e-05\n",
            "----------------------------------------\n",
            "Epoch 479\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4508e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.450775290575526e-05\n",
            "----------------------------------------\n",
            "Epoch 480\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.4059e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.405935422179418e-05\n",
            "----------------------------------------\n",
            "Epoch 481\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3597e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.359653799068847e-05\n",
            "----------------------------------------\n",
            "Epoch 482\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3126e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.312629750715886e-05\n",
            "----------------------------------------\n",
            "Epoch 483\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.2658e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.265782626688734e-05\n",
            "----------------------------------------\n",
            "Epoch 484\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.2200e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.219950800967883e-05\n",
            "----------------------------------------\n",
            "Epoch 485\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1758e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.175833767677823e-05\n",
            "----------------------------------------\n",
            "Epoch 486\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.1340e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.133958048126332e-05\n",
            "----------------------------------------\n",
            "Epoch 487\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0949e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.094910363344034e-05\n",
            "----------------------------------------\n",
            "Epoch 488\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0593e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.059317598010678e-05\n",
            "----------------------------------------\n",
            "Epoch 489\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.0265e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.026539483706539e-05\n",
            "----------------------------------------\n",
            "Epoch 490\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9965e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.99645076167691e-05\n",
            "----------------------------------------\n",
            "Epoch 491\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9688e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.968811981540364e-05\n",
            "----------------------------------------\n",
            "Epoch 492\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9432e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.943193887280256e-05\n",
            "----------------------------------------\n",
            "Epoch 493\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9194e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.919390185573453e-05\n",
            "----------------------------------------\n",
            "Epoch 494\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8971e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.897073091099512e-05\n",
            "----------------------------------------\n",
            "Epoch 495\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8759e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.875940048325994e-05\n",
            "----------------------------------------\n",
            "Epoch 496\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8557e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.855731514465743e-05\n",
            "----------------------------------------\n",
            "Epoch 497\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8363e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.836257676649113e-05\n",
            "----------------------------------------\n",
            "Epoch 498\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.8175e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.817460024874166e-05\n",
            "----------------------------------------\n",
            "Epoch 499\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7992e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.799184564323814e-05\n",
            "----------------------------------------\n",
            "Epoch 500\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7813e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.781347917973461e-05\n",
            "----------------------------------------\n",
            "Epoch 501\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7639e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.763938405703171e-05\n",
            "----------------------------------------\n",
            "Epoch 502\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7470e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.746971101505755e-05\n",
            "----------------------------------------\n",
            "Epoch 503\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7305e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.730476729562923e-05\n",
            "----------------------------------------\n",
            "Epoch 504\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7145e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.714491574094922e-05\n",
            "----------------------------------------\n",
            "Epoch 505\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6990e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.69899424100473e-05\n",
            "----------------------------------------\n",
            "Epoch 506\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6840e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.684038753342556e-05\n",
            "----------------------------------------\n",
            "Epoch 507\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6697e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.669677563473562e-05\n",
            "----------------------------------------\n",
            "Epoch 508\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6559e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.65590087099747e-05\n",
            "----------------------------------------\n",
            "Epoch 509\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6431e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.64311804421734e-05\n",
            "----------------------------------------\n",
            "Epoch 510\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6310e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.630993948637582e-05\n",
            "----------------------------------------\n",
            "Epoch 511\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6193e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.61928003915945e-05\n",
            "----------------------------------------\n",
            "Epoch 512\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.6079e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.607930191262107e-05\n",
            "----------------------------------------\n",
            "Epoch 513\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5969e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.596908086196596e-05\n",
            "----------------------------------------\n",
            "Epoch 514\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5862e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.586194157876907e-05\n",
            "----------------------------------------\n",
            "Epoch 515\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5758e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.575758313234516e-05\n",
            "----------------------------------------\n",
            "Epoch 516\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5656e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.565554644026795e-05\n",
            "----------------------------------------\n",
            "Epoch 517\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5556e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.555563211143476e-05\n",
            "----------------------------------------\n",
            "Epoch 518\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5458e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.545768507962494e-05\n",
            "----------------------------------------\n",
            "Epoch 519\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5362e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.53615860155631e-05\n",
            "----------------------------------------\n",
            "Epoch 520\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5267e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.526724609894265e-05\n",
            "----------------------------------------\n",
            "Epoch 521\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5175e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.517459033074577e-05\n",
            "----------------------------------------\n",
            "Epoch 522\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.5084e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.508358727779532e-05\n",
            "----------------------------------------\n",
            "Epoch 523\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4994e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.499420292741217e-05\n",
            "----------------------------------------\n",
            "Epoch 524\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4906e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.490631406836577e-05\n",
            "----------------------------------------\n",
            "Epoch 525\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4820e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.48198521573866e-05\n",
            "----------------------------------------\n",
            "Epoch 526\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4735e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.473474125613863e-05\n",
            "----------------------------------------\n",
            "Epoch 527\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4651e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.465089753141982e-05\n",
            "----------------------------------------\n",
            "Epoch 528\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4568e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.456823000753702e-05\n",
            "----------------------------------------\n",
            "Epoch 529\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4487e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.448664227857084e-05\n",
            "----------------------------------------\n",
            "Epoch 530\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4406e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.440646452620853e-05\n",
            "----------------------------------------\n",
            "Epoch 531\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4327e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.432741854775503e-05\n",
            "----------------------------------------\n",
            "Epoch 532\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4249e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.424895572316617e-05\n",
            "----------------------------------------\n",
            "Epoch 533\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4171e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.417103313033832e-05\n",
            "----------------------------------------\n",
            "Epoch 534\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4094e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.409357514206891e-05\n",
            "----------------------------------------\n",
            "Epoch 535\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4017e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.401651964757775e-05\n",
            "----------------------------------------\n",
            "Epoch 536\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3940e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.39398826000485e-05\n",
            "----------------------------------------\n",
            "Epoch 537\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3864e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.386352707294488e-05\n",
            "----------------------------------------\n",
            "Epoch 538\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3787e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.378747020662781e-05\n",
            "----------------------------------------\n",
            "Epoch 539\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3712e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.371166187470494e-05\n",
            "----------------------------------------\n",
            "Epoch 540\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3636e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.363593039234458e-05\n",
            "----------------------------------------\n",
            "Epoch 541\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3561e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.356096997799295e-05\n",
            "----------------------------------------\n",
            "Epoch 542\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3486e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.348626756731773e-05\n",
            "----------------------------------------\n",
            "Epoch 543\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3412e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.34115507702473e-05\n",
            "----------------------------------------\n",
            "Epoch 544\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3338e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.333765923415715e-05\n",
            "----------------------------------------\n",
            "Epoch 545\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3264e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.326407319956006e-05\n",
            "----------------------------------------\n",
            "Epoch 546\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3191e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.319064510354874e-05\n",
            "----------------------------------------\n",
            "Epoch 547\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3117e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.31173842354559e-05\n",
            "----------------------------------------\n",
            "Epoch 548\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.3044e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.3044288367534e-05\n",
            "----------------------------------------\n",
            "Epoch 549\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2971e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.297136597554558e-05\n",
            "----------------------------------------\n",
            "Epoch 550\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2899e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.289854047052946e-05\n",
            "----------------------------------------\n",
            "Epoch 551\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2826e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.282615017540665e-05\n",
            "----------------------------------------\n",
            "Epoch 552\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2754e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.275382279707811e-05\n",
            "----------------------------------------\n",
            "Epoch 553\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2682e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.268159947965978e-05\n",
            "----------------------------------------\n",
            "Epoch 554\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2610e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.2609715342976e-05\n",
            "----------------------------------------\n",
            "Epoch 555\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2538e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.253812088678584e-05\n",
            "----------------------------------------\n",
            "Epoch 556\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2467e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.246666018143786e-05\n",
            "----------------------------------------\n",
            "Epoch 557\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2395e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.239520193852635e-05\n",
            "----------------------------------------\n",
            "Epoch 558\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2324e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.232394522966604e-05\n",
            "----------------------------------------\n",
            "Epoch 559\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2253e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.225303437098097e-05\n",
            "----------------------------------------\n",
            "Epoch 560\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2182e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.218213516197201e-05\n",
            "----------------------------------------\n",
            "Epoch 561\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2111e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.211136303639088e-05\n",
            "----------------------------------------\n",
            "Epoch 562\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2041e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.20408098629851e-05\n",
            "----------------------------------------\n",
            "Epoch 563\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1970e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.197028144371378e-05\n",
            "----------------------------------------\n",
            "Epoch 564\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1900e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.18999610969623e-05\n",
            "----------------------------------------\n",
            "Epoch 565\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1830e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.182980165694709e-05\n",
            "----------------------------------------\n",
            "Epoch 566\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1760e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.175965442802085e-05\n",
            "----------------------------------------\n",
            "Epoch 567\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1690e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.168959240002508e-05\n",
            "----------------------------------------\n",
            "Epoch 568\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1620e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.161989120029467e-05\n",
            "----------------------------------------\n",
            "Epoch 569\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1550e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.155021883859295e-05\n",
            "----------------------------------------\n",
            "Epoch 570\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1481e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.148057507956001e-05\n",
            "----------------------------------------\n",
            "Epoch 571\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1411e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.14109700299364e-05\n",
            "----------------------------------------\n",
            "Epoch 572\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1342e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.13416664639278e-05\n",
            "----------------------------------------\n",
            "Epoch 573\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1273e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.127252182735469e-05\n",
            "----------------------------------------\n",
            "Epoch 574\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1203e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.120346165988403e-05\n",
            "----------------------------------------\n",
            "Epoch 575\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1134e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.113443062908621e-05\n",
            "----------------------------------------\n",
            "Epoch 576\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.1066e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.106550543101298e-05\n",
            "----------------------------------------\n",
            "Epoch 577\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0997e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.099682260577192e-05\n",
            "----------------------------------------\n",
            "Epoch 578\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0928e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.092826554598626e-05\n",
            "----------------------------------------\n",
            "Epoch 579\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0860e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.085976961380725e-05\n",
            "----------------------------------------\n",
            "Epoch 580\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0792e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.079150126311066e-05\n",
            "----------------------------------------\n",
            "Epoch 581\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0723e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.072333723707361e-05\n",
            "----------------------------------------\n",
            "Epoch 582\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0655e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.065519037997213e-05\n",
            "----------------------------------------\n",
            "Epoch 583\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0587e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.058712436869413e-05\n",
            "----------------------------------------\n",
            "Epoch 584\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0519e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.051934615795742e-05\n",
            "----------------------------------------\n",
            "Epoch 585\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0452e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.045160626786896e-05\n",
            "----------------------------------------\n",
            "Epoch 586\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0384e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.038401577358519e-05\n",
            "----------------------------------------\n",
            "Epoch 587\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0316e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.031647978007902e-05\n",
            "----------------------------------------\n",
            "Epoch 588\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0249e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.02491419478247e-05\n",
            "----------------------------------------\n",
            "Epoch 589\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0182e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.018191075322592e-05\n",
            "----------------------------------------\n",
            "Epoch 590\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0115e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.011493082633144e-05\n",
            "----------------------------------------\n",
            "Epoch 591\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.0048e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.004806183112009e-05\n",
            "----------------------------------------\n",
            "Epoch 592\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9981e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.998122930650098e-05\n",
            "----------------------------------------\n",
            "Epoch 593\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9915e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.99148289766466e-05\n",
            "----------------------------------------\n",
            "Epoch 594\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9849e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.984852973227115e-05\n",
            "----------------------------------------\n",
            "Epoch 595\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9782e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.978218028028097e-05\n",
            "----------------------------------------\n",
            "Epoch 596\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9716e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.971614917644634e-05\n",
            "----------------------------------------\n",
            "Epoch 597\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9650e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.965019920845874e-05\n",
            "----------------------------------------\n",
            "Epoch 598\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9584e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.958443164274485e-05\n",
            "----------------------------------------\n",
            "Epoch 599\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9519e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.951878356755504e-05\n",
            "----------------------------------------\n",
            "Epoch 600\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9453e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.94531375994869e-05\n",
            "----------------------------------------\n",
            "Epoch 601\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9388e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.93877841178286e-05\n",
            "----------------------------------------\n",
            "Epoch 602\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9322e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.9322464705968e-05\n",
            "----------------------------------------\n",
            "Epoch 603\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9257e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.925724218326754e-05\n",
            "----------------------------------------\n",
            "Epoch 604\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9192e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.919221080575382e-05\n",
            "----------------------------------------\n",
            "Epoch 605\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9127e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.912728412545479e-05\n",
            "----------------------------------------\n",
            "Epoch 606\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.9063e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.906252054365199e-05\n",
            "----------------------------------------\n",
            "Epoch 607\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8998e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.89978091748467e-05\n",
            "----------------------------------------\n",
            "Epoch 608\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8933e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.893315825922105e-05\n",
            "----------------------------------------\n",
            "Epoch 609\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8869e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.8868732797984e-05\n",
            "----------------------------------------\n",
            "Epoch 610\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8804e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.880435738684894e-05\n",
            "----------------------------------------\n",
            "Epoch 611\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8740e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.874014359531318e-05\n",
            "----------------------------------------\n",
            "Epoch 612\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8676e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.867603733819493e-05\n",
            "----------------------------------------\n",
            "Epoch 613\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8612e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.861196749723857e-05\n",
            "----------------------------------------\n",
            "Epoch 614\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8548e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.854825663507851e-05\n",
            "----------------------------------------\n",
            "Epoch 615\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8484e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.848447603705441e-05\n",
            "----------------------------------------\n",
            "Epoch 616\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8421e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.842072258408149e-05\n",
            "----------------------------------------\n",
            "Epoch 617\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8357e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.835723162157456e-05\n",
            "----------------------------------------\n",
            "Epoch 618\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8294e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.829383167876025e-05\n",
            "----------------------------------------\n",
            "Epoch 619\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8230e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.823046770215592e-05\n",
            "----------------------------------------\n",
            "Epoch 620\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8167e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.816716925972018e-05\n",
            "----------------------------------------\n",
            "Epoch 621\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8104e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.810427547969237e-05\n",
            "----------------------------------------\n",
            "Epoch 622\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.8041e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.804127199941432e-05\n",
            "----------------------------------------\n",
            "Epoch 623\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7978e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.797831514664392e-05\n",
            "----------------------------------------\n",
            "Epoch 624\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7916e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.791557922057309e-05\n",
            "----------------------------------------\n",
            "Epoch 625\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7853e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.785297924061715e-05\n",
            "----------------------------------------\n",
            "Epoch 626\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7790e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.779044294233166e-05\n",
            "----------------------------------------\n",
            "Epoch 627\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7728e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.772802723658548e-05\n",
            "----------------------------------------\n",
            "Epoch 628\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7666e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.766567362418125e-05\n",
            "----------------------------------------\n",
            "Epoch 629\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7603e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.760338771666893e-05\n",
            "----------------------------------------\n",
            "Epoch 630\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7541e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.754135776581682e-05\n",
            "----------------------------------------\n",
            "Epoch 631\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7479e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.747941058721277e-05\n",
            "----------------------------------------\n",
            "Epoch 632\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7418e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.741750380699572e-05\n",
            "----------------------------------------\n",
            "Epoch 633\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7356e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.735574758320975e-05\n",
            "----------------------------------------\n",
            "Epoch 634\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7294e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.729403497873552e-05\n",
            "----------------------------------------\n",
            "Epoch 635\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7233e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.72326171852106e-05\n",
            "----------------------------------------\n",
            "Epoch 636\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7171e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.717135198065069e-05\n",
            "----------------------------------------\n",
            "Epoch 637\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7110e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.711004071606e-05\n",
            "----------------------------------------\n",
            "Epoch 638\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7049e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.704897613239586e-05\n",
            "----------------------------------------\n",
            "Epoch 639\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6988e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.698797958170155e-05\n",
            "----------------------------------------\n",
            "Epoch 640\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6928e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.692766199002758e-05\n",
            "----------------------------------------\n",
            "Epoch 641\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6868e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.686785619342311e-05\n",
            "----------------------------------------\n",
            "Epoch 642\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6808e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.680840129182385e-05\n",
            "----------------------------------------\n",
            "Epoch 643\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6749e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.674919505809426e-05\n",
            "----------------------------------------\n",
            "Epoch 644\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6690e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.669041295195246e-05\n",
            "----------------------------------------\n",
            "Epoch 645\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6632e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.663191924715133e-05\n",
            "----------------------------------------\n",
            "Epoch 646\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6574e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.657374397508109e-05\n",
            "----------------------------------------\n",
            "Epoch 647\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6516e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.651586728813478e-05\n",
            "----------------------------------------\n",
            "Epoch 648\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6458e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.645847455847045e-05\n",
            "----------------------------------------\n",
            "Epoch 649\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6401e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.640117897393993e-05\n",
            "----------------------------------------\n",
            "Epoch 650\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6344e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.634449543611681e-05\n",
            "----------------------------------------\n",
            "Epoch 651\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6288e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.628807469075689e-05\n",
            "----------------------------------------\n",
            "Epoch 652\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6232e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.623190961624487e-05\n",
            "----------------------------------------\n",
            "Epoch 653\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6176e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.617600340950899e-05\n",
            "----------------------------------------\n",
            "Epoch 654\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6120e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.612035809976075e-05\n",
            "----------------------------------------\n",
            "Epoch 655\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6065e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.60650006824153e-05\n",
            "----------------------------------------\n",
            "Epoch 656\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.6010e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.601006382067951e-05\n",
            "----------------------------------------\n",
            "Epoch 657\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5955e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.595537700272362e-05\n",
            "----------------------------------------\n",
            "Epoch 658\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5901e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.590102536981254e-05\n",
            "----------------------------------------\n",
            "Epoch 659\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5847e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.584700085705074e-05\n",
            "----------------------------------------\n",
            "Epoch 660\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5793e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.579319175714673e-05\n",
            "----------------------------------------\n",
            "Epoch 661\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5740e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.573993315995991e-05\n",
            "----------------------------------------\n",
            "Epoch 662\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5687e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.568725034162923e-05\n",
            "----------------------------------------\n",
            "Epoch 663\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5635e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.563495296705626e-05\n",
            "----------------------------------------\n",
            "Epoch 664\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5583e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.558278964112198e-05\n",
            "----------------------------------------\n",
            "Epoch 665\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5531e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.553104616145819e-05\n",
            "----------------------------------------\n",
            "Epoch 666\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5480e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.547959048376596e-05\n",
            "----------------------------------------\n",
            "Epoch 667\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5428e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.542833196344196e-05\n",
            "----------------------------------------\n",
            "Epoch 668\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5377e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.537726762170067e-05\n",
            "----------------------------------------\n",
            "Epoch 669\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5326e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.53263944058334e-05\n",
            "----------------------------------------\n",
            "Epoch 670\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5276e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.527577161651583e-05\n",
            "----------------------------------------\n",
            "Epoch 671\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5225e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.522546797149643e-05\n",
            "----------------------------------------\n",
            "Epoch 672\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5175e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.517525925411886e-05\n",
            "----------------------------------------\n",
            "Epoch 673\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5125e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.512533856614406e-05\n",
            "----------------------------------------\n",
            "Epoch 674\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5076e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.507566883216213e-05\n",
            "----------------------------------------\n",
            "Epoch 675\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.5026e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.502614926838012e-05\n",
            "----------------------------------------\n",
            "Epoch 676\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4977e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.497683594369246e-05\n",
            "----------------------------------------\n",
            "Epoch 677\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4928e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.492766848678513e-05\n",
            "----------------------------------------\n",
            "Epoch 678\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4879e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.487877567595169e-05\n",
            "----------------------------------------\n",
            "Epoch 679\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4830e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.48300841867039e-05\n",
            "----------------------------------------\n",
            "Epoch 680\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4781e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.47814356545497e-05\n",
            "----------------------------------------\n",
            "Epoch 681\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4733e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.473297406011866e-05\n",
            "----------------------------------------\n",
            "Epoch 682\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4685e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.468470036065293e-05\n",
            "----------------------------------------\n",
            "Epoch 683\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4637e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.463669524411134e-05\n",
            "----------------------------------------\n",
            "Epoch 684\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4589e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.458870437657646e-05\n",
            "----------------------------------------\n",
            "Epoch 685\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4541e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.454090491363197e-05\n",
            "----------------------------------------\n",
            "Epoch 686\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4493e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.449324728676247e-05\n",
            "----------------------------------------\n",
            "Epoch 687\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4446e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.444569608043214e-05\n",
            "----------------------------------------\n",
            "Epoch 688\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4398e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.439827805483982e-05\n",
            "----------------------------------------\n",
            "Epoch 689\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4351e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.435106165701532e-05\n",
            "----------------------------------------\n",
            "Epoch 690\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4304e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.430394889765936e-05\n",
            "----------------------------------------\n",
            "Epoch 691\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4257e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.425694413848716e-05\n",
            "----------------------------------------\n",
            "Epoch 692\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4210e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.421001025948914e-05\n",
            "----------------------------------------\n",
            "Epoch 693\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4163e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.416326922399905e-05\n",
            "----------------------------------------\n",
            "Epoch 694\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4117e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.41166226281267e-05\n",
            "----------------------------------------\n",
            "Epoch 695\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4070e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.40702698167927e-05\n",
            "----------------------------------------\n",
            "Epoch 696\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.4024e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.402444513899813e-05\n",
            "----------------------------------------\n",
            "Epoch 697\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3979e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.397853663994796e-05\n",
            "----------------------------------------\n",
            "Epoch 698\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3933e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.393262510319124e-05\n",
            "----------------------------------------\n",
            "Epoch 699\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3887e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.38867956721549e-05\n",
            "----------------------------------------\n",
            "Epoch 700\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3841e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.384092967432305e-05\n",
            "----------------------------------------\n",
            "Epoch 701\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3795e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.379503743927028e-05\n",
            "----------------------------------------\n",
            "Epoch 702\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3749e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.374936994324875e-05\n",
            "----------------------------------------\n",
            "Epoch 703\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3704e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.370416236765796e-05\n",
            "----------------------------------------\n",
            "Epoch 704\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3659e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.365897554814992e-05\n",
            "----------------------------------------\n",
            "Epoch 705\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3614e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.361382937793085e-05\n",
            "----------------------------------------\n",
            "Epoch 706\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3569e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.356872829362663e-05\n",
            "----------------------------------------\n",
            "Epoch 707\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3524e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.352369604188031e-05\n",
            "----------------------------------------\n",
            "Epoch 708\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3479e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.347872210452653e-05\n",
            "----------------------------------------\n",
            "Epoch 709\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3434e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.343381190395092e-05\n",
            "----------------------------------------\n",
            "Epoch 710\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3389e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.338921794002941e-05\n",
            "----------------------------------------\n",
            "Epoch 711\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3345e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.334475026805207e-05\n",
            "----------------------------------------\n",
            "Epoch 712\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3300e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.330031474525746e-05\n",
            "----------------------------------------\n",
            "Epoch 713\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3256e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.325579643007365e-05\n",
            "----------------------------------------\n",
            "Epoch 714\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3211e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.321147441559433e-05\n",
            "----------------------------------------\n",
            "Epoch 715\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3167e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.31673901865099e-05\n",
            "----------------------------------------\n",
            "Epoch 716\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3123e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.312331743455844e-05\n",
            "----------------------------------------\n",
            "Epoch 717\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3079e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.307935533437162e-05\n",
            "----------------------------------------\n",
            "Epoch 718\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.3035e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.303526459552834e-05\n",
            "----------------------------------------\n",
            "Epoch 719\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2991e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.299139353457374e-05\n",
            "----------------------------------------\n",
            "Epoch 720\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2948e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.294772471202691e-05\n",
            "----------------------------------------\n",
            "Epoch 721\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2904e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.290398462527016e-05\n",
            "----------------------------------------\n",
            "Epoch 722\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2861e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.286051818357266e-05\n",
            "----------------------------------------\n",
            "Epoch 723\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2817e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.28170715621852e-05\n",
            "----------------------------------------\n",
            "Epoch 724\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2774e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.277366065767547e-05\n",
            "----------------------------------------\n",
            "Epoch 725\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2730e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.273023586741673e-05\n",
            "----------------------------------------\n",
            "Epoch 726\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2687e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.268687888914892e-05\n",
            "----------------------------------------\n",
            "Epoch 727\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2644e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.264367862974162e-05\n",
            "----------------------------------------\n",
            "Epoch 728\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2600e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.260049506115391e-05\n",
            "----------------------------------------\n",
            "Epoch 729\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2558e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.255755655945307e-05\n",
            "----------------------------------------\n",
            "Epoch 730\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2515e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.251456782847813e-05\n",
            "----------------------------------------\n",
            "Epoch 731\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2472e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.247169457300482e-05\n",
            "----------------------------------------\n",
            "Epoch 732\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2429e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.242900938247754e-05\n",
            "----------------------------------------\n",
            "Epoch 733\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2386e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.238636460542914e-05\n",
            "----------------------------------------\n",
            "Epoch 734\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2344e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.234366811998881e-05\n",
            "----------------------------------------\n",
            "Epoch 735\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2301e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.230097893721285e-05\n",
            "----------------------------------------\n",
            "Epoch 736\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2258e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.22584062940956e-05\n",
            "----------------------------------------\n",
            "Epoch 737\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2216e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.221595982763161e-05\n",
            "----------------------------------------\n",
            "Epoch 738\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2174e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.217353508631529e-05\n",
            "----------------------------------------\n",
            "Epoch 739\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2131e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.213115733253307e-05\n",
            "----------------------------------------\n",
            "Epoch 740\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2089e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.208897539836721e-05\n",
            "----------------------------------------\n",
            "Epoch 741\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2047e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.204681938003631e-05\n",
            "----------------------------------------\n",
            "Epoch 742\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.2004e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.200449733333167e-05\n",
            "----------------------------------------\n",
            "Epoch 743\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1963e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.196262037955988e-05\n",
            "----------------------------------------\n",
            "Epoch 744\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1921e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.192073434863781e-05\n",
            "----------------------------------------\n",
            "Epoch 745\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1879e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.187877877945702e-05\n",
            "----------------------------------------\n",
            "Epoch 746\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1837e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.183682124364233e-05\n",
            "----------------------------------------\n",
            "Epoch 747\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1795e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.179481177623264e-05\n",
            "----------------------------------------\n",
            "Epoch 748\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1753e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.175310520061581e-05\n",
            "----------------------------------------\n",
            "Epoch 749\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1711e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.171135208038667e-05\n",
            "----------------------------------------\n",
            "Epoch 750\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1670e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.166974000224407e-05\n",
            "----------------------------------------\n",
            "Epoch 751\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1628e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.16282361466964e-05\n",
            "----------------------------------------\n",
            "Epoch 752\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1587e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.158668163504469e-05\n",
            "----------------------------------------\n",
            "Epoch 753\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1545e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.154515974602277e-05\n",
            "----------------------------------------\n",
            "Epoch 754\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1504e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.150367589475269e-05\n",
            "----------------------------------------\n",
            "Epoch 755\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1462e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.146231263796181e-05\n",
            "----------------------------------------\n",
            "Epoch 756\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1421e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.142082155075568e-05\n",
            "----------------------------------------\n",
            "Epoch 757\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1380e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.137963229987375e-05\n",
            "----------------------------------------\n",
            "Epoch 758\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1338e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.13384081020667e-05\n",
            "----------------------------------------\n",
            "Epoch 759\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1297e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.129708171149402e-05\n",
            "----------------------------------------\n",
            "Epoch 760\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1256e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.125573500221475e-05\n",
            "----------------------------------------\n",
            "Epoch 761\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1215e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.121485843271929e-05\n",
            "----------------------------------------\n",
            "Epoch 762\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1174e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.117389043379512e-05\n",
            "----------------------------------------\n",
            "Epoch 763\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1133e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.113279923851174e-05\n",
            "----------------------------------------\n",
            "Epoch 764\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1092e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.109157661547698e-05\n",
            "----------------------------------------\n",
            "Epoch 765\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1051e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.105073100339374e-05\n",
            "----------------------------------------\n",
            "Epoch 766\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1010e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.101001933086154e-05\n",
            "----------------------------------------\n",
            "Epoch 767\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0969e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.096932921913845e-05\n",
            "----------------------------------------\n",
            "Epoch 768\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0928e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.092833140996792e-05\n",
            "----------------------------------------\n",
            "Epoch 769\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0887e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.088744398199539e-05\n",
            "----------------------------------------\n",
            "Epoch 770\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0847e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.084687646017949e-05\n",
            "----------------------------------------\n",
            "Epoch 771\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0806e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.080640963826513e-05\n",
            "----------------------------------------\n",
            "Epoch 772\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0766e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.076578818928316e-05\n",
            "----------------------------------------\n",
            "Epoch 773\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0725e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.072503237948853e-05\n",
            "----------------------------------------\n",
            "Epoch 774\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0685e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.068456421559013e-05\n",
            "----------------------------------------\n",
            "Epoch 775\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0644e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.064428617174603e-05\n",
            "----------------------------------------\n",
            "Epoch 776\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0604e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.060396011712034e-05\n",
            "----------------------------------------\n",
            "Epoch 777\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0563e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.056344883070356e-05\n",
            "----------------------------------------\n",
            "Epoch 778\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0523e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.05229575459881e-05\n",
            "----------------------------------------\n",
            "Epoch 779\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0483e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.048282489795915e-05\n",
            "----------------------------------------\n",
            "Epoch 780\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0442e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.044194587160469e-05\n",
            "----------------------------------------\n",
            "Epoch 781\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0400e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.040044298914236e-05\n",
            "----------------------------------------\n",
            "Epoch 782\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0359e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.035888653185844e-05\n",
            "----------------------------------------\n",
            "Epoch 783\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0317e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.031714620957267e-05\n",
            "----------------------------------------\n",
            "Epoch 784\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0276e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.027555083311591e-05\n",
            "----------------------------------------\n",
            "Epoch 785\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0234e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.023393576855524e-05\n",
            "----------------------------------------\n",
            "Epoch 786\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0192e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.019195336515679e-05\n",
            "----------------------------------------\n",
            "Epoch 787\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0150e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.015012689966169e-05\n",
            "----------------------------------------\n",
            "Epoch 788\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0108e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.010831522111086e-05\n",
            "----------------------------------------\n",
            "Epoch 789\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0066e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.006646607402812e-05\n",
            "----------------------------------------\n",
            "Epoch 790\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.0025e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.002475000513466e-05\n",
            "----------------------------------------\n",
            "Epoch 791\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9983e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.998284953674483e-05\n",
            "----------------------------------------\n",
            "Epoch 792\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9941e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.99412013432495e-05\n",
            "----------------------------------------\n",
            "Epoch 793\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9899e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.989949691870092e-05\n",
            "----------------------------------------\n",
            "Epoch 794\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9858e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.985772516945986e-05\n",
            "----------------------------------------\n",
            "Epoch 795\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9816e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.981585795748717e-05\n",
            "----------------------------------------\n",
            "Epoch 796\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9774e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.977422904280194e-05\n",
            "----------------------------------------\n",
            "Epoch 797\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9733e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.973252650350853e-05\n",
            "----------------------------------------\n",
            "Epoch 798\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9691e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.969086189223287e-05\n",
            "----------------------------------------\n",
            "Epoch 799\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9649e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.964947443171891e-05\n",
            "----------------------------------------\n",
            "Epoch 800\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9608e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.960792717960807e-05\n",
            "----------------------------------------\n",
            "Epoch 801\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9566e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.95662744421895e-05\n",
            "----------------------------------------\n",
            "Epoch 802\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9525e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.95247819575715e-05\n",
            "----------------------------------------\n",
            "Epoch 803\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9483e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.948342652339858e-05\n",
            "----------------------------------------\n",
            "Epoch 804\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9442e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.944199818752428e-05\n",
            "----------------------------------------\n",
            "Epoch 805\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9401e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.940080004375677e-05\n",
            "----------------------------------------\n",
            "Epoch 806\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9359e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.935938987605281e-05\n",
            "----------------------------------------\n",
            "Epoch 807\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9318e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.931795999867337e-05\n",
            "----------------------------------------\n",
            "Epoch 808\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9277e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.927685546350298e-05\n",
            "----------------------------------------\n",
            "Epoch 809\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9236e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.923569743238552e-05\n",
            "----------------------------------------\n",
            "Epoch 810\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9194e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.919435741421716e-05\n",
            "----------------------------------------\n",
            "Epoch 811\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9153e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.915322921044821e-05\n",
            "----------------------------------------\n",
            "Epoch 812\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9112e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.911223639449644e-05\n",
            "----------------------------------------\n",
            "Epoch 813\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9071e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.907111736062383e-05\n",
            "----------------------------------------\n",
            "Epoch 814\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.9030e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.903020941565885e-05\n",
            "----------------------------------------\n",
            "Epoch 815\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8989e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.898902656062401e-05\n",
            "----------------------------------------\n",
            "Epoch 816\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8948e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.894825659258152e-05\n",
            "----------------------------------------\n",
            "Epoch 817\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8907e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.89073684897917e-05\n",
            "----------------------------------------\n",
            "Epoch 818\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8866e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.886646639393106e-05\n",
            "----------------------------------------\n",
            "Epoch 819\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8826e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.882570615993206e-05\n",
            "----------------------------------------\n",
            "Epoch 820\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8785e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.878511052529032e-05\n",
            "----------------------------------------\n",
            "Epoch 821\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8744e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.87443744520686e-05\n",
            "----------------------------------------\n",
            "Epoch 822\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8704e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.870385021399202e-05\n",
            "----------------------------------------\n",
            "Epoch 823\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8663e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.866333771684233e-05\n",
            "----------------------------------------\n",
            "Epoch 824\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8623e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.862269535514759e-05\n",
            "----------------------------------------\n",
            "Epoch 825\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8582e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.85822115549493e-05\n",
            "----------------------------------------\n",
            "Epoch 826\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8542e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.854185539938882e-05\n",
            "----------------------------------------\n",
            "Epoch 827\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8501e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.850134441875238e-05\n",
            "----------------------------------------\n",
            "Epoch 828\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8461e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.846109214844832e-05\n",
            "----------------------------------------\n",
            "Epoch 829\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8421e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.842086307994197e-05\n",
            "----------------------------------------\n",
            "Epoch 830\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8381e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.838055537026003e-05\n",
            "----------------------------------------\n",
            "Epoch 831\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8340e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.834008429315148e-05\n",
            "----------------------------------------\n",
            "Epoch 832\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8300e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.829993387423964e-05\n",
            "----------------------------------------\n",
            "Epoch 833\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8260e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.825990117838436e-05\n",
            "----------------------------------------\n",
            "Epoch 834\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8220e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.821981150192269e-05\n",
            "----------------------------------------\n",
            "Epoch 835\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8180e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.81797591385016e-05\n",
            "----------------------------------------\n",
            "Epoch 836\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8140e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.813976753061587e-05\n",
            "----------------------------------------\n",
            "Epoch 837\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8100e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.809985320032082e-05\n",
            "----------------------------------------\n",
            "Epoch 838\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8060e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.805997832282596e-05\n",
            "----------------------------------------\n",
            "Epoch 839\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.8020e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.802005518375531e-05\n",
            "----------------------------------------\n",
            "Epoch 840\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7980e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.798020485112395e-05\n",
            "----------------------------------------\n",
            "Epoch 841\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7941e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.794068231846866e-05\n",
            "----------------------------------------\n",
            "Epoch 842\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7901e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.790099538589011e-05\n",
            "----------------------------------------\n",
            "Epoch 843\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7861e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.78611669673766e-05\n",
            "----------------------------------------\n",
            "Epoch 844\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7822e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.782166560645713e-05\n",
            "----------------------------------------\n",
            "Epoch 845\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7782e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.778223863089558e-05\n",
            "----------------------------------------\n",
            "Epoch 846\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7743e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.774256601037013e-05\n",
            "----------------------------------------\n",
            "Epoch 847\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7703e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.770300833185501e-05\n",
            "----------------------------------------\n",
            "Epoch 848\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7664e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.766367350624933e-05\n",
            "----------------------------------------\n",
            "Epoch 849\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7624e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.762419945269224e-05\n",
            "----------------------------------------\n",
            "Epoch 850\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7585e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.758497198482006e-05\n",
            "----------------------------------------\n",
            "Epoch 851\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7546e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.754574391137882e-05\n",
            "----------------------------------------\n",
            "Epoch 852\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7506e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.750636262256495e-05\n",
            "----------------------------------------\n",
            "Epoch 853\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7467e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.746729937916169e-05\n",
            "----------------------------------------\n",
            "Epoch 854\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7428e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.742822992405257e-05\n",
            "----------------------------------------\n",
            "Epoch 855\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7389e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.738911163743782e-05\n",
            "----------------------------------------\n",
            "Epoch 856\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7350e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.734984963249413e-05\n",
            "----------------------------------------\n",
            "Epoch 857\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7311e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.731110867152707e-05\n",
            "----------------------------------------\n",
            "Epoch 858\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7272e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.727227518297046e-05\n",
            "----------------------------------------\n",
            "Epoch 859\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7233e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.723333355156245e-05\n",
            "----------------------------------------\n",
            "Epoch 860\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7194e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.719424898851742e-05\n",
            "----------------------------------------\n",
            "Epoch 861\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7155e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.715530174201452e-05\n",
            "----------------------------------------\n",
            "Epoch 862\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7117e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.711666135414095e-05\n",
            "----------------------------------------\n",
            "Epoch 863\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7078e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.707789471002316e-05\n",
            "----------------------------------------\n",
            "Epoch 864\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7039e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.7039035552051e-05\n",
            "----------------------------------------\n",
            "Epoch 865\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.7001e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.700058118220002e-05\n",
            "----------------------------------------\n",
            "Epoch 866\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6962e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.696208024428752e-05\n",
            "----------------------------------------\n",
            "Epoch 867\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6923e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.692347863332894e-05\n",
            "----------------------------------------\n",
            "Epoch 868\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6885e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.688476200216631e-05\n",
            "----------------------------------------\n",
            "Epoch 869\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6847e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.684687467738607e-05\n",
            "----------------------------------------\n",
            "Epoch 870\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6809e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.680891160757381e-05\n",
            "----------------------------------------\n",
            "Epoch 871\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6771e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.6770828808024e-05\n",
            "----------------------------------------\n",
            "Epoch 872\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6733e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.673271440938649e-05\n",
            "----------------------------------------\n",
            "Epoch 873\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6694e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.669449742087043e-05\n",
            "----------------------------------------\n",
            "Epoch 874\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6657e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.665686114893137e-05\n",
            "----------------------------------------\n",
            "Epoch 875\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6619e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.661921922402521e-05\n",
            "----------------------------------------\n",
            "Epoch 876\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6581e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.658148754150641e-05\n",
            "----------------------------------------\n",
            "Epoch 877\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6544e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.654362013388895e-05\n",
            "----------------------------------------\n",
            "Epoch 878\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6506e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.650563585559169e-05\n",
            "----------------------------------------\n",
            "Epoch 879\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6468e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.646817479547013e-05\n",
            "----------------------------------------\n",
            "Epoch 880\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6431e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.643086098749567e-05\n",
            "----------------------------------------\n",
            "Epoch 881\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6393e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.63934056004345e-05\n",
            "----------------------------------------\n",
            "Epoch 882\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6356e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.635581800848889e-05\n",
            "----------------------------------------\n",
            "Epoch 883\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6318e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.631816350856613e-05\n",
            "----------------------------------------\n",
            "Epoch 884\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6281e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.628072534957856e-05\n",
            "----------------------------------------\n",
            "Epoch 885\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6244e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.62435946638251e-05\n",
            "----------------------------------------\n",
            "Epoch 886\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6206e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.620632569490642e-05\n",
            "----------------------------------------\n",
            "Epoch 887\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6169e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.61689286859026e-05\n",
            "----------------------------------------\n",
            "Epoch 888\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6132e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.613175814804443e-05\n",
            "----------------------------------------\n",
            "Epoch 889\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6095e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.609474918101735e-05\n",
            "----------------------------------------\n",
            "Epoch 890\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6058e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.60576312456152e-05\n",
            "----------------------------------------\n",
            "Epoch 891\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6020e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.602042781242605e-05\n",
            "----------------------------------------\n",
            "Epoch 892\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5984e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.598364643184979e-05\n",
            "----------------------------------------\n",
            "Epoch 893\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5947e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.594687070891443e-05\n",
            "----------------------------------------\n",
            "Epoch 894\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5910e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.590991284813077e-05\n",
            "----------------------------------------\n",
            "Epoch 895\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5873e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.587285543576676e-05\n",
            "----------------------------------------\n",
            "Epoch 896\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5836e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.583609680503256e-05\n",
            "----------------------------------------\n",
            "Epoch 897\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5800e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.580030724201949e-05\n",
            "----------------------------------------\n",
            "Epoch 898\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5764e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.576446841211593e-05\n",
            "----------------------------------------\n",
            "Epoch 899\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5729e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.572853013109186e-05\n",
            "----------------------------------------\n",
            "Epoch 900\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5693e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.569280611101695e-05\n",
            "----------------------------------------\n",
            "Epoch 901\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5657e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.565729036783484e-05\n",
            "----------------------------------------\n",
            "Epoch 902\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5622e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.562165867010835e-05\n",
            "----------------------------------------\n",
            "Epoch 903\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5586e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.558593185319648e-05\n",
            "----------------------------------------\n",
            "Epoch 904\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5551e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.555080362916665e-05\n",
            "----------------------------------------\n",
            "Epoch 905\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5516e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.551557346028762e-05\n",
            "----------------------------------------\n",
            "Epoch 906\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5480e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.548020045314145e-05\n",
            "----------------------------------------\n",
            "Epoch 907\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5445e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.544480302715971e-05\n",
            "----------------------------------------\n",
            "Epoch 908\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5409e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.540947445347577e-05\n",
            "----------------------------------------\n",
            "Epoch 909\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5374e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.53744990720556e-05\n",
            "----------------------------------------\n",
            "Epoch 910\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5339e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.533940155876152e-05\n",
            "----------------------------------------\n",
            "Epoch 911\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5304e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.530425233983515e-05\n",
            "----------------------------------------\n",
            "Epoch 912\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5269e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.526930560256859e-05\n",
            "----------------------------------------\n",
            "Epoch 913\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5234e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.523443227173981e-05\n",
            "----------------------------------------\n",
            "Epoch 914\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5200e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.519954124792607e-05\n",
            "----------------------------------------\n",
            "Epoch 915\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5165e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.516481694590605e-05\n",
            "----------------------------------------\n",
            "Epoch 916\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5130e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.51300498295321e-05\n",
            "----------------------------------------\n",
            "Epoch 917\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5095e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.509522727352372e-05\n",
            "----------------------------------------\n",
            "Epoch 918\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5061e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.506060290116982e-05\n",
            "----------------------------------------\n",
            "Epoch 919\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.5026e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.502606874122263e-05\n",
            "----------------------------------------\n",
            "Epoch 920\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4991e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.499145377947555e-05\n",
            "----------------------------------------\n",
            "Epoch 921\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4957e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.495699044441372e-05\n",
            "----------------------------------------\n",
            "Epoch 922\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4923e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.49225162463289e-05\n",
            "----------------------------------------\n",
            "Epoch 923\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4888e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.488793437359755e-05\n",
            "----------------------------------------\n",
            "Epoch 924\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4854e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.485377565372958e-05\n",
            "----------------------------------------\n",
            "Epoch 925\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4820e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.481952046111364e-05\n",
            "----------------------------------------\n",
            "Epoch 926\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4785e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.478514833724923e-05\n",
            "----------------------------------------\n",
            "Epoch 927\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4751e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.475067002185372e-05\n",
            "----------------------------------------\n",
            "Epoch 928\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4717e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.471672365422545e-05\n",
            "----------------------------------------\n",
            "Epoch 929\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4683e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.468274492986852e-05\n",
            "----------------------------------------\n",
            "Epoch 930\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4649e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.464861330768735e-05\n",
            "----------------------------------------\n",
            "Epoch 931\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4614e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.461436480983595e-05\n",
            "----------------------------------------\n",
            "Epoch 932\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4580e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.458005300784663e-05\n",
            "----------------------------------------\n",
            "Epoch 933\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4546e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.454631637651863e-05\n",
            "----------------------------------------\n",
            "Epoch 934\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4513e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.451254209531439e-05\n",
            "----------------------------------------\n",
            "Epoch 935\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4479e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.44786260391356e-05\n",
            "----------------------------------------\n",
            "Epoch 936\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4445e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.444460904647601e-05\n",
            "----------------------------------------\n",
            "Epoch 937\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4410e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.4410482830042e-05\n",
            "----------------------------------------\n",
            "Epoch 938\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4377e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.437671234459959e-05\n",
            "----------------------------------------\n",
            "Epoch 939\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4343e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.434325304540378e-05\n",
            "----------------------------------------\n",
            "Epoch 940\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4310e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.430969739458132e-05\n",
            "----------------------------------------\n",
            "Epoch 941\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4276e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.427605505972907e-05\n",
            "----------------------------------------\n",
            "Epoch 942\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4242e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.424231495354138e-05\n",
            "----------------------------------------\n",
            "Epoch 943\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4209e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.42090608627282e-05\n",
            "----------------------------------------\n",
            "Epoch 944\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4176e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.417581531506657e-05\n",
            "----------------------------------------\n",
            "Epoch 945\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4142e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.414246625805484e-05\n",
            "----------------------------------------\n",
            "Epoch 946\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4109e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.410897623089364e-05\n",
            "----------------------------------------\n",
            "Epoch 947\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4076e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.407554713324534e-05\n",
            "----------------------------------------\n",
            "Epoch 948\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4043e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.404284724234842e-05\n",
            "----------------------------------------\n",
            "Epoch 949\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.4010e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.401010000111365e-05\n",
            "----------------------------------------\n",
            "Epoch 950\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3977e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.397724566338711e-05\n",
            "----------------------------------------\n",
            "Epoch 951\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3944e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.394429921539483e-05\n",
            "----------------------------------------\n",
            "Epoch 952\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3911e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.391127441358725e-05\n",
            "----------------------------------------\n",
            "Epoch 953\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3878e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.387825697206434e-05\n",
            "----------------------------------------\n",
            "Epoch 954\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3846e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.384571352277782e-05\n",
            "----------------------------------------\n",
            "Epoch 955\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3813e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.3813021719726e-05\n",
            "----------------------------------------\n",
            "Epoch 956\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3780e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.378026261122327e-05\n",
            "----------------------------------------\n",
            "Epoch 957\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3748e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.374791493868998e-05\n",
            "----------------------------------------\n",
            "Epoch 958\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3715e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.371548793600908e-05\n",
            "----------------------------------------\n",
            "Epoch 959\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3683e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.368297611047616e-05\n",
            "----------------------------------------\n",
            "Epoch 960\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3650e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.36503739928302e-05\n",
            "----------------------------------------\n",
            "Epoch 961\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3618e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.361790158879536e-05\n",
            "----------------------------------------\n",
            "Epoch 962\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3586e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.35856512491698e-05\n",
            "----------------------------------------\n",
            "Epoch 963\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3553e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.355326459861853e-05\n",
            "----------------------------------------\n",
            "Epoch 964\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3521e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.35208018510986e-05\n",
            "----------------------------------------\n",
            "Epoch 965\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3489e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.348858431387677e-05\n",
            "----------------------------------------\n",
            "Epoch 966\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3456e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.345641255059946e-05\n",
            "----------------------------------------\n",
            "Epoch 967\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3424e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.342418436515275e-05\n",
            "----------------------------------------\n",
            "Epoch 968\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3392e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.339214610752716e-05\n",
            "----------------------------------------\n",
            "Epoch 969\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3360e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.336005173854665e-05\n",
            "----------------------------------------\n",
            "Epoch 970\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3328e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.332791494602804e-05\n",
            "----------------------------------------\n",
            "Epoch 971\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3296e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.329586610943645e-05\n",
            "----------------------------------------\n",
            "Epoch 972\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3264e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.326378799914014e-05\n",
            "----------------------------------------\n",
            "Epoch 973\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3232e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.323163705128207e-05\n",
            "----------------------------------------\n",
            "Epoch 974\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3199e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.319920692997565e-05\n",
            "----------------------------------------\n",
            "Epoch 975\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3167e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.316664450574696e-05\n",
            "----------------------------------------\n",
            "Epoch 976\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3134e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.313394682684595e-05\n",
            "----------------------------------------\n",
            "Epoch 977\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3101e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.310131156741689e-05\n",
            "----------------------------------------\n",
            "Epoch 978\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3069e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.306868202019133e-05\n",
            "----------------------------------------\n",
            "Epoch 979\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3036e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.30359033929156e-05\n",
            "----------------------------------------\n",
            "Epoch 980\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.3003e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.300332755139826e-05\n",
            "----------------------------------------\n",
            "Epoch 981\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2971e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.297070660687398e-05\n",
            "----------------------------------------\n",
            "Epoch 982\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2938e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.293795938141911e-05\n",
            "----------------------------------------\n",
            "Epoch 983\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2905e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.290510293071546e-05\n",
            "----------------------------------------\n",
            "Epoch 984\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2873e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.287269141890785e-05\n",
            "----------------------------------------\n",
            "Epoch 985\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2840e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.284016167924761e-05\n",
            "----------------------------------------\n",
            "Epoch 986\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2807e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.280749246085571e-05\n",
            "----------------------------------------\n",
            "Epoch 987\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2775e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.277472342909968e-05\n",
            "----------------------------------------\n",
            "Epoch 988\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2742e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.274196287951634e-05\n",
            "----------------------------------------\n",
            "Epoch 989\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2709e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.270948634130596e-05\n",
            "----------------------------------------\n",
            "Epoch 990\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2677e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.267691117380659e-05\n",
            "----------------------------------------\n",
            "Epoch 991\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2644e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.264430784199565e-05\n",
            "----------------------------------------\n",
            "Epoch 992\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2612e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.261202512170919e-05\n",
            "----------------------------------------\n",
            "Epoch 993\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2580e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.257962337063057e-05\n",
            "----------------------------------------\n",
            "Epoch 994\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2547e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.254714607148184e-05\n",
            "----------------------------------------\n",
            "Epoch 995\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2515e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.251456215273072e-05\n",
            "----------------------------------------\n",
            "Epoch 996\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2482e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.248226805508457e-05\n",
            "----------------------------------------\n",
            "Epoch 997\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2450e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.245003856723117e-05\n",
            "----------------------------------------\n",
            "Epoch 998\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2418e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.241771750427889e-05\n",
            "----------------------------------------\n",
            "Epoch 999\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2385e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.238530851327873e-05\n",
            "----------------------------------------\n",
            "Epoch 1000\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2353e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.235286385014605e-05\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.1982, -0.5572],\n",
            "        [-0.1288,  0.6843],\n",
            "        [-0.1759,  0.5148],\n",
            "        [ 0.1354,  0.2921],\n",
            "        [ 0.0147,  0.1771],\n",
            "        [-0.6556,  0.0107]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.4311,  0.6651, -0.2020,  0.5516, -0.4184,  1.0147],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 0.2854, -0.1002, -0.0428, -0.3139,  0.2698, -0.3971],\n",
            "        [ 0.2810, -0.1355, -0.1690, -0.0417,  0.1544, -0.1611],\n",
            "        [ 0.1248, -0.3361,  0.2905, -0.2386, -0.5225, -0.8673],\n",
            "        [ 0.0696, -0.1192, -0.2622, -0.3630, -0.4198,  0.3015],\n",
            "        [-0.0504,  0.0689, -0.1116,  0.1144, -0.6136, -0.3159],\n",
            "        [-0.2525, -0.3236, -0.1373, -0.1378, -0.2818, -0.2362]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.4125,  0.1437, -0.1493,  0.1047, -0.0062,  0.3237],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.3169,  0.3006, -0.3145, -0.4081,  0.0387,  0.2194]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.0140], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_200_0p99 = train_var_scope(model_200_0p99, 200, 0.001, test_200_0p99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5TXDNed9lBv",
        "outputId": "a3e112b9-3cde-44fb-c52a-ccd925e732c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.2321e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.232071548283213e-05\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.8212e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.821238477719147e-05\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.4394e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.439401945160914e-05\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0930e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.093024913095342e-05\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7785e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.778548904449979e-05\n",
            "----------------------------------------\n",
            "Epoch 6\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.4622e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.4622360365619785e-05\n",
            "----------------------------------------\n",
            "Epoch 7\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1637e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.163712912528531e-05\n",
            "----------------------------------------\n",
            "Epoch 8\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8728e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.872768128666346e-05\n",
            "----------------------------------------\n",
            "Epoch 9\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9401e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.940123982244951e-05\n",
            "----------------------------------------\n",
            "Epoch 10\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4341e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.434085554924648e-05\n",
            "----------------------------------------\n",
            "Epoch 11\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4416e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.441581653301738e-05\n",
            "----------------------------------------\n",
            "Epoch 12\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2003e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.2002592960711154e-05\n",
            "----------------------------------------\n",
            "Epoch 13\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9325e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9324880394725342e-05\n",
            "----------------------------------------\n",
            "Epoch 14\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8260e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.826008145046087e-05\n",
            "----------------------------------------\n",
            "Epoch 15\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6655e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6654770480795158e-05\n",
            "----------------------------------------\n",
            "Epoch 16\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4882e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4882377958571897e-05\n",
            "----------------------------------------\n",
            "Epoch 17\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3413e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3412574584680357e-05\n",
            "----------------------------------------\n",
            "Epoch 18\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2282e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.228214661454187e-05\n",
            "----------------------------------------\n",
            "Epoch 19\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1096e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1095740576177908e-05\n",
            "----------------------------------------\n",
            "Epoch 20\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9732e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9731898146340064e-05\n",
            "----------------------------------------\n",
            "Epoch 21\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8462e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.846166132217096e-05\n",
            "----------------------------------------\n",
            "Epoch 22\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7457e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.745727241245481e-05\n",
            "----------------------------------------\n",
            "Epoch 23\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.6586e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.658622499752148e-05\n",
            "----------------------------------------\n",
            "Epoch 24\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.5694e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.569448938710995e-05\n",
            "----------------------------------------\n",
            "Epoch 25\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4832e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4831801610583826e-05\n",
            "----------------------------------------\n",
            "Epoch 26\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.4172e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.4171757183129453e-05\n",
            "----------------------------------------\n",
            "Epoch 27\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.3447e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.3446537497075548e-05\n",
            "----------------------------------------\n",
            "Epoch 28\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.2688e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.2688020337093774e-05\n",
            "----------------------------------------\n",
            "Epoch 29\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1887e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.188671872960388e-05\n",
            "----------------------------------------\n",
            "Epoch 30\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.1173e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.1173157717969415e-05\n",
            "----------------------------------------\n",
            "Epoch 31\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0627e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0627027705963712e-05\n",
            "----------------------------------------\n",
            "Epoch 32\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.0176e-05, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.0176497106586096e-05\n",
            "----------------------------------------\n",
            "Epoch 33\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.7312e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.731234013682844e-06\n",
            "----------------------------------------\n",
            "Epoch 34\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(9.3134e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 9.313408623644975e-06\n",
            "----------------------------------------\n",
            "Epoch 35\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.9891e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.989130458391421e-06\n",
            "----------------------------------------\n",
            "Epoch 36\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.7417e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.74169899224684e-06\n",
            "----------------------------------------\n",
            "Epoch 37\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.4942e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.49423565511665e-06\n",
            "----------------------------------------\n",
            "Epoch 38\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(8.2226e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 8.22256456031814e-06\n",
            "----------------------------------------\n",
            "Epoch 39\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.7415e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.74154972183705e-06\n",
            "----------------------------------------\n",
            "Epoch 40\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(7.1792e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 7.179246606933622e-06\n",
            "----------------------------------------\n",
            "Epoch 41\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.6720e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.671963247185429e-06\n",
            "----------------------------------------\n",
            "Epoch 42\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(6.1794e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 6.1793651527326965e-06\n",
            "----------------------------------------\n",
            "Epoch 43\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.7239e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.723899093810125e-06\n",
            "----------------------------------------\n",
            "Epoch 44\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.3452e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.345220389815732e-06\n",
            "----------------------------------------\n",
            "Epoch 45\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(5.0412e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 5.04117343967594e-06\n",
            "----------------------------------------\n",
            "Epoch 46\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.7811e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.781139465130999e-06\n",
            "----------------------------------------\n",
            "Epoch 47\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.5570e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.55701526538185e-06\n",
            "----------------------------------------\n",
            "Epoch 48\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.3869e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.386887215283719e-06\n",
            "----------------------------------------\n",
            "Epoch 49\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.2724e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.272380360992422e-06\n",
            "----------------------------------------\n",
            "Epoch 50\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1881e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.188135022804692e-06\n",
            "----------------------------------------\n",
            "Epoch 51\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.1156e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.115572063188474e-06\n",
            "----------------------------------------\n",
            "Epoch 52\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0591e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.059125669463513e-06\n",
            "----------------------------------------\n",
            "Epoch 53\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(4.0228e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 4.022809203595509e-06\n",
            "----------------------------------------\n",
            "Epoch 54\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9924e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.992423520053543e-06\n",
            "----------------------------------------\n",
            "Epoch 55\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9525e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.952535016769134e-06\n",
            "----------------------------------------\n",
            "Epoch 56\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.9046e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.904606334132071e-06\n",
            "----------------------------------------\n",
            "Epoch 57\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8552e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.8551802709118265e-06\n",
            "----------------------------------------\n",
            "Epoch 58\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.8025e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.8025362456450284e-06\n",
            "----------------------------------------\n",
            "Epoch 59\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.7381e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.7381477724112374e-06\n",
            "----------------------------------------\n",
            "Epoch 60\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.6642e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.6641787715576744e-06\n",
            "----------------------------------------\n",
            "Epoch 61\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5884e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.588425868337358e-06\n",
            "----------------------------------------\n",
            "Epoch 62\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.5131e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.5130708766717514e-06\n",
            "----------------------------------------\n",
            "Epoch 63\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.4344e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.4343960164209754e-06\n",
            "----------------------------------------\n",
            "Epoch 64\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.3534e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.3534373587515606e-06\n",
            "----------------------------------------\n",
            "Epoch 65\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2765e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.2764955832449052e-06\n",
            "----------------------------------------\n",
            "Epoch 66\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.2060e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.206018049482695e-06\n",
            "----------------------------------------\n",
            "Epoch 67\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.1392e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.139198962697873e-06\n",
            "----------------------------------------\n",
            "Epoch 68\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0755e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.0754769723086325e-06\n",
            "----------------------------------------\n",
            "Epoch 69\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(3.0183e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 3.018287735407156e-06\n",
            "----------------------------------------\n",
            "Epoch 70\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9689e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9689023498630194e-06\n",
            "----------------------------------------\n",
            "Epoch 71\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.9244e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.9244430417502362e-06\n",
            "----------------------------------------\n",
            "Epoch 72\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8832e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.8832480728302296e-06\n",
            "----------------------------------------\n",
            "Epoch 73\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8471e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.847067127363581e-06\n",
            "----------------------------------------\n",
            "Epoch 74\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.8161e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.8161061856503806e-06\n",
            "----------------------------------------\n",
            "Epoch 75\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7880e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.787970151392337e-06\n",
            "----------------------------------------\n",
            "Epoch 76\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7612e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.761247115220838e-06\n",
            "----------------------------------------\n",
            "Epoch 77\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7367e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7367417045850254e-06\n",
            "----------------------------------------\n",
            "Epoch 78\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.7145e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.7144980849099483e-06\n",
            "----------------------------------------\n",
            "Epoch 79\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6928e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6928448014164476e-06\n",
            "----------------------------------------\n",
            "Epoch 80\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6710e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6710407284137763e-06\n",
            "----------------------------------------\n",
            "Epoch 81\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6499e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.6498825943202615e-06\n",
            "----------------------------------------\n",
            "Epoch 82\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6295e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.629492171779276e-06\n",
            "----------------------------------------\n",
            "Epoch 83\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.6089e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.608948923006397e-06\n",
            "----------------------------------------\n",
            "Epoch 84\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5881e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5881206262751213e-06\n",
            "----------------------------------------\n",
            "Epoch 85\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5678e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5678181528272734e-06\n",
            "----------------------------------------\n",
            "Epoch 86\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5482e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5482001563681935e-06\n",
            "----------------------------------------\n",
            "Epoch 87\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5287e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5287279986501263e-06\n",
            "----------------------------------------\n",
            "Epoch 88\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.5096e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.5095712984026917e-06\n",
            "----------------------------------------\n",
            "Epoch 89\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4913e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.491331441947872e-06\n",
            "----------------------------------------\n",
            "Epoch 90\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4740e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4739896222317836e-06\n",
            "----------------------------------------\n",
            "Epoch 91\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4572e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.457217900286923e-06\n",
            "----------------------------------------\n",
            "Epoch 92\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4413e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.441342888793847e-06\n",
            "----------------------------------------\n",
            "Epoch 93\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4266e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4266289316713183e-06\n",
            "----------------------------------------\n",
            "Epoch 94\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.4126e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.4126474266256146e-06\n",
            "----------------------------------------\n",
            "Epoch 95\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3992e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.399240460662034e-06\n",
            "----------------------------------------\n",
            "Epoch 96\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3867e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.386696964481814e-06\n",
            "----------------------------------------\n",
            "Epoch 97\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3750e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3750263241011085e-06\n",
            "----------------------------------------\n",
            "Epoch 98\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3638e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3638338599968297e-06\n",
            "----------------------------------------\n",
            "Epoch 99\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3530e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3530333904979355e-06\n",
            "----------------------------------------\n",
            "Epoch 100\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3428e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3427912684184003e-06\n",
            "----------------------------------------\n",
            "Epoch 101\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3329e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.332925437130794e-06\n",
            "----------------------------------------\n",
            "Epoch 102\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3230e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.323005556056758e-06\n",
            "----------------------------------------\n",
            "Epoch 103\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3132e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.3132345676878474e-06\n",
            "----------------------------------------\n",
            "Epoch 104\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.3037e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.303730947046211e-06\n",
            "----------------------------------------\n",
            "Epoch 105\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2943e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.294327325274627e-06\n",
            "----------------------------------------\n",
            "Epoch 106\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2850e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2849878033793764e-06\n",
            "----------------------------------------\n",
            "Epoch 107\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2758e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.275845989347572e-06\n",
            "----------------------------------------\n",
            "Epoch 108\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2669e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2668898882774385e-06\n",
            "----------------------------------------\n",
            "Epoch 109\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2580e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.258024029193613e-06\n",
            "----------------------------------------\n",
            "Epoch 110\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2493e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2493056076170312e-06\n",
            "----------------------------------------\n",
            "Epoch 111\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2408e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2408059588411795e-06\n",
            "----------------------------------------\n",
            "Epoch 112\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2325e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2324621972205252e-06\n",
            "----------------------------------------\n",
            "Epoch 113\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2242e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2242467217121105e-06\n",
            "----------------------------------------\n",
            "Epoch 114\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2162e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2162296678170867e-06\n",
            "----------------------------------------\n",
            "Epoch 115\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2084e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.208432399093976e-06\n",
            "----------------------------------------\n",
            "Epoch 116\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.2008e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.2007662587121027e-06\n",
            "----------------------------------------\n",
            "Epoch 117\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1932e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1932456163666733e-06\n",
            "----------------------------------------\n",
            "Epoch 118\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1859e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1858942998587142e-06\n",
            "----------------------------------------\n",
            "Epoch 119\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1787e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.17866634190406e-06\n",
            "----------------------------------------\n",
            "Epoch 120\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1715e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1715366384167817e-06\n",
            "----------------------------------------\n",
            "Epoch 121\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1645e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1645245303434507e-06\n",
            "----------------------------------------\n",
            "Epoch 122\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1576e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1576197295609702e-06\n",
            "----------------------------------------\n",
            "Epoch 123\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1508e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.15078833358545e-06\n",
            "----------------------------------------\n",
            "Epoch 124\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1440e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.144038688734293e-06\n",
            "----------------------------------------\n",
            "Epoch 125\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1374e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.137378277180507e-06\n",
            "----------------------------------------\n",
            "Epoch 126\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1308e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1307823284916546e-06\n",
            "----------------------------------------\n",
            "Epoch 127\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1242e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1242462436466606e-06\n",
            "----------------------------------------\n",
            "Epoch 128\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1178e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1177854794923314e-06\n",
            "----------------------------------------\n",
            "Epoch 129\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1114e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.111391574104614e-06\n",
            "----------------------------------------\n",
            "Epoch 130\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.1051e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.1050539609173344e-06\n",
            "----------------------------------------\n",
            "Epoch 131\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0988e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.098783865981421e-06\n",
            "----------------------------------------\n",
            "Epoch 132\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0926e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0925783522228365e-06\n",
            "----------------------------------------\n",
            "Epoch 133\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0864e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.08643391770894e-06\n",
            "----------------------------------------\n",
            "Epoch 134\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0804e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0803504347891387e-06\n",
            "----------------------------------------\n",
            "Epoch 135\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0743e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0743335448577e-06\n",
            "----------------------------------------\n",
            "Epoch 136\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0684e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0683753499056466e-06\n",
            "----------------------------------------\n",
            "Epoch 137\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0625e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0624733502552397e-06\n",
            "----------------------------------------\n",
            "Epoch 138\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0566e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0566320355733708e-06\n",
            "----------------------------------------\n",
            "Epoch 139\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0508e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.050846219446491e-06\n",
            "----------------------------------------\n",
            "Epoch 140\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0451e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.045110378020883e-06\n",
            "----------------------------------------\n",
            "Epoch 141\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0394e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0394270573886586e-06\n",
            "----------------------------------------\n",
            "Epoch 142\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0338e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0337944284764476e-06\n",
            "----------------------------------------\n",
            "Epoch 143\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0282e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.028206947796103e-06\n",
            "----------------------------------------\n",
            "Epoch 144\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0227e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0226652794487827e-06\n",
            "----------------------------------------\n",
            "Epoch 145\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0172e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0171694556371917e-06\n",
            "----------------------------------------\n",
            "Epoch 146\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0117e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.0117152581233164e-06\n",
            "----------------------------------------\n",
            "Epoch 147\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0063e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.006302387649375e-06\n",
            "----------------------------------------\n",
            "Epoch 148\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(2.0009e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 2.000932092002731e-06\n",
            "----------------------------------------\n",
            "Epoch 149\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9956e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9956018978720763e-06\n",
            "----------------------------------------\n",
            "Epoch 150\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9903e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9903109398601224e-06\n",
            "----------------------------------------\n",
            "Epoch 151\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9851e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9850604943223607e-06\n",
            "----------------------------------------\n",
            "Epoch 152\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9798e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9798491225865887e-06\n",
            "----------------------------------------\n",
            "Epoch 153\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9747e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9746756428191184e-06\n",
            "----------------------------------------\n",
            "Epoch 154\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9695e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9695409995541253e-06\n",
            "----------------------------------------\n",
            "Epoch 155\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9644e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.964444419430508e-06\n",
            "----------------------------------------\n",
            "Epoch 156\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9594e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9593845626157954e-06\n",
            "----------------------------------------\n",
            "Epoch 157\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9544e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9543617785214914e-06\n",
            "----------------------------------------\n",
            "Epoch 158\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9494e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9493754821316875e-06\n",
            "----------------------------------------\n",
            "Epoch 159\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9444e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9444243041901704e-06\n",
            "----------------------------------------\n",
            "Epoch 160\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9395e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9395082148359967e-06\n",
            "----------------------------------------\n",
            "Epoch 161\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9346e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9346268358555923e-06\n",
            "----------------------------------------\n",
            "Epoch 162\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9298e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9297789961677606e-06\n",
            "----------------------------------------\n",
            "Epoch 163\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9250e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.924964460907087e-06\n",
            "----------------------------------------\n",
            "Epoch 164\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9201e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.920134717357947e-06\n",
            "----------------------------------------\n",
            "Epoch 165\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9153e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9153403017164248e-06\n",
            "----------------------------------------\n",
            "Epoch 166\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9106e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.910565030908935e-06\n",
            "----------------------------------------\n",
            "Epoch 167\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9058e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.90580979004132e-06\n",
            "----------------------------------------\n",
            "Epoch 168\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.9011e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.9010773847426175e-06\n",
            "----------------------------------------\n",
            "Epoch 169\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8964e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8963655378845105e-06\n",
            "----------------------------------------\n",
            "Epoch 170\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8917e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8916745921999245e-06\n",
            "----------------------------------------\n",
            "Epoch 171\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8870e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8870063951529825e-06\n",
            "----------------------------------------\n",
            "Epoch 172\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8824e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8823607310205955e-06\n",
            "----------------------------------------\n",
            "Epoch 173\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8777e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8777372385904874e-06\n",
            "----------------------------------------\n",
            "Epoch 174\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8731e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8731361227469885e-06\n",
            "----------------------------------------\n",
            "Epoch 175\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8686e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.868562676277342e-06\n",
            "----------------------------------------\n",
            "Epoch 176\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8640e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.86401293933565e-06\n",
            "----------------------------------------\n",
            "Epoch 177\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8595e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8594864823457134e-06\n",
            "----------------------------------------\n",
            "Epoch 178\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8550e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8549834311021532e-06\n",
            "----------------------------------------\n",
            "Epoch 179\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8505e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8505037548690575e-06\n",
            "----------------------------------------\n",
            "Epoch 180\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8460e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8460474310255552e-06\n",
            "----------------------------------------\n",
            "Epoch 181\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8416e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.841614492540779e-06\n",
            "----------------------------------------\n",
            "Epoch 182\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8372e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8372048612368845e-06\n",
            "----------------------------------------\n",
            "Epoch 183\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8328e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8328184747169835e-06\n",
            "----------------------------------------\n",
            "Epoch 184\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8285e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8284553193490085e-06\n",
            "----------------------------------------\n",
            "Epoch 185\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8241e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8241153015093954e-06\n",
            "----------------------------------------\n",
            "Epoch 186\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8198e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8197872260624523e-06\n",
            "----------------------------------------\n",
            "Epoch 187\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8155e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8154502718086458e-06\n",
            "----------------------------------------\n",
            "Epoch 188\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8111e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.811132235138781e-06\n",
            "----------------------------------------\n",
            "Epoch 189\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8068e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.8068334997762367e-06\n",
            "----------------------------------------\n",
            "Epoch 190\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.8026e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.802554426585657e-06\n",
            "----------------------------------------\n",
            "Epoch 191\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7983e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7982952841613823e-06\n",
            "----------------------------------------\n",
            "Epoch 192\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7941e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7940563131123993e-06\n",
            "----------------------------------------\n",
            "Epoch 193\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7898e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7898377309640536e-06\n",
            "----------------------------------------\n",
            "Epoch 194\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7856e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7856396864458774e-06\n",
            "----------------------------------------\n",
            "Epoch 195\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7815e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.781462313106846e-06\n",
            "----------------------------------------\n",
            "Epoch 196\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7773e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7773057250698368e-06\n",
            "----------------------------------------\n",
            "Epoch 197\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7732e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7731699862984152e-06\n",
            "----------------------------------------\n",
            "Epoch 198\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7691e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.769055151931568e-06\n",
            "----------------------------------------\n",
            "Epoch 199\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7650e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7649612586442072e-06\n",
            "----------------------------------------\n",
            "Epoch 200\n",
            "IS variance:  tensor(6.4534e-06, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(1.7609e-06, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 1.7608883084495522e-06\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.1982, -0.5572],\n",
            "        [-0.1224,  0.6697],\n",
            "        [-0.1859,  0.4811],\n",
            "        [ 0.1263,  0.2894],\n",
            "        [ 0.0221,  0.1914],\n",
            "        [-0.5981,  0.0634]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.4311,  0.6609, -0.2139,  0.5443, -0.4112,  1.0631],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 0.2854, -0.1002, -0.0428, -0.3139,  0.2459, -0.3971],\n",
            "        [ 0.2810, -0.1530, -0.1758, -0.0745,  0.1052, -0.1795],\n",
            "        [ 0.1248, -0.3361,  0.2486, -0.2877, -0.5554, -0.9408],\n",
            "        [ 0.0696, -0.1333, -0.2595, -0.3630, -0.3887,  0.1157],\n",
            "        [-0.0504,  0.0526, -0.1121,  0.1065, -0.6129, -0.3498],\n",
            "        [-0.2525, -0.3633, -0.1271, -0.1265, -0.2754, -0.2697]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.4361,  0.1258, -0.1636,  0.0014, -0.0254,  0.3282],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[ 0.3004,  0.2740, -0.2649, -0.2538, -0.0028,  0.1882]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([0.0124], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 400 0.99"
      ],
      "metadata": {
        "id": "-Y6XSrxLTUPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_400 = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b_400 = experiment_actions(400, env, P_pi_b_400)\n",
        "P_pi_e_400 = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e_400 = experiment_actions(400, env, P_pi_e_400)\n",
        "model_400_0p99 = CustomizableFeatureNet(input_dim=2, hidden_dims=[6, 6], output_dim=1, dtype = torch.float64)\n",
        "test_400_0p99 = SCOPE_straight(model_400_0p99, 0.99, 1000, pi_b_400, P_pi_b_400, P_pi_e_400, dtype = torch.float64)\n",
        "model_400_0p99 = train_var_scope(model_400_0p99, 5, 0.001, test_400_0p99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-GtA_2_H3jW",
        "outputId": "255985ec-858b-495f-a37b-6c1d83b80245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(0.0002, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.0864, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.08636927251674584\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(0.0002, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2360, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.23601228060459742\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(0.0002, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2284, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.22842130839536168\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(0.0002, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2203, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.22034274292801542\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(0.0002, dtype=torch.float64)\n",
            "SCOPE Var loss:  tensor(0.2122, dtype=torch.float64, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.21219201524204384\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.2227, -0.2377],\n",
            "        [-0.1161, -0.0580],\n",
            "        [-0.1448,  0.0938],\n",
            "        [-0.1531,  0.6014],\n",
            "        [ 0.6863, -0.3983],\n",
            "        [-0.6208,  0.5967]], dtype=torch.float64)\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.1459,  0.6348,  0.5511, -0.0894, -0.6099, -0.4201],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 0.0188,  0.1783,  0.1906, -0.1831, -0.2326,  0.3907],\n",
            "        [ 0.1501,  0.2778,  0.3853,  0.2220,  0.2562,  0.2344],\n",
            "        [-0.2534,  0.3682,  0.3991,  0.0664, -0.1327,  0.1305],\n",
            "        [ 0.3641,  0.1544,  0.0996,  0.0388,  0.3387,  0.2223],\n",
            "        [-0.1971,  0.1551, -0.1208, -0.2238,  0.3284,  0.2085],\n",
            "        [-0.2006,  0.0626,  0.0105, -0.3599, -0.2018,  0.0474]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.0877, -0.3765,  0.2807, -0.3098,  0.2580, -0.0114],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[-0.3840,  0.2713,  0.0393, -0.2318,  0.2767,  0.1556]],\n",
            "       dtype=torch.float64)\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([-0.2633], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 600 0.99"
      ],
      "metadata": {
        "id": "pGLbkJw_OMtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_600 = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b_600 = experiment_actions(600, env_30, P_pi_b_600)\n",
        "P_pi_e_600 = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e_600 = experiment_actions(600, env, P_pi_e_600)\n",
        "model_600_0p99 = CustomizableFeatureNet(input_dim=2, hidden_dims=[6, 6], output_dim=1, dtype = torch.float32)\n",
        "test_600_0p99 = SCOPE_straight(model_600_0p99, 0.99, 10000, pi_b_600, P_pi_b_600, P_pi_e_600, dtype = torch.float32)\n",
        "model_600_0p99 = train_var_scope(model_600_0p99, 5, 0.001, test_600_0p99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DJzov1IOPEV",
        "outputId": "2289774b-c244-4899-98bb-4aa0a28de6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(0.0007)\n",
            "SCOPE Var loss:  tensor(0.0505, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.05046245828270912\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(0.0007)\n",
            "SCOPE Var loss:  tensor(0.0493, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0493154413998127\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(0.0007)\n",
            "SCOPE Var loss:  tensor(0.0481, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.04812745749950409\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(0.0007)\n",
            "SCOPE Var loss:  tensor(0.0469, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.046937569975852966\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(0.0007)\n",
            "SCOPE Var loss:  tensor(0.0458, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.04576045647263527\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[ 0.1204,  0.1797],\n",
            "        [ 0.1732,  0.2432],\n",
            "        [ 0.6242,  0.4461],\n",
            "        [ 0.4436,  0.0618],\n",
            "        [ 0.2962, -0.5830],\n",
            "        [-0.3988, -0.6057]])\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.5419,  0.4819,  0.6672, -0.3882, -0.3168, -0.5845])\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 0.3440, -0.1163, -0.2793,  0.0047, -0.1040, -0.2861],\n",
            "        [-0.2082, -0.2452, -0.2654, -0.3990, -0.1743, -0.2673],\n",
            "        [-0.1590, -0.3511, -0.3919,  0.1778, -0.1089, -0.2762],\n",
            "        [ 0.3015,  0.0899, -0.3342,  0.2264, -0.1954, -0.0921],\n",
            "        [ 0.3359,  0.2783,  0.3255, -0.1074, -0.0552,  0.3129],\n",
            "        [ 0.0065,  0.2449,  0.0709,  0.2463, -0.0068,  0.1182]])\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.3040, -0.2149, -0.0412, -0.2949, -0.2522,  0.2680])\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[-0.1696, -0.0297, -0.3140, -0.0297, -0.3058,  0.3434]])\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([-0.0148])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 800 0.99"
      ],
      "metadata": {
        "id": "ariP9U-abrzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_800 = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b_800 = experiment_actions(800, env_30, P_pi_b_800)\n",
        "P_pi_e_800 = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e_800 = experiment_actions(800, env_30, P_pi_e_800)\n",
        "model_800_0p99 = CustomizableFeatureNet(input_dim=2, hidden_dims=[6, 6], output_dim=1, dtype = torch.float32)\n",
        "test_800_0p99 = SCOPE_straight(model_800_0p99, 0.99, 10000, pi_b_800, P_pi_b_800, P_pi_e_800, dtype = torch.float32)\n",
        "model_800_0p99 = train_var_scope(model_800_0p99, 5, 0.001, test_800_0p99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY3ZZMqFbucT",
        "outputId": "d170bc23-4df3-467c-92a6-82dbe78457c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(6.2439e-07)\n",
            "SCOPE Var loss:  tensor(0.1004, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.10040785372257233\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(6.2439e-07)\n",
            "SCOPE Var loss:  tensor(0.1327, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.13271035254001617\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(6.2439e-07)\n",
            "SCOPE Var loss:  tensor(0.1297, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.12969112396240234\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(6.2439e-07)\n",
            "SCOPE Var loss:  tensor(0.1267, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.12670785188674927\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(6.2439e-07)\n",
            "SCOPE Var loss:  tensor(0.1238, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.12376594543457031\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[ 0.1256,  0.1763],\n",
            "        [ 0.1731,  0.2431],\n",
            "        [ 0.6243,  0.4461],\n",
            "        [ 0.4445,  0.0656],\n",
            "        [ 0.2962, -0.5871],\n",
            "        [-0.3988, -0.6057]])\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([-0.5420,  0.4819,  0.6672, -0.3851, -0.3175, -0.5845])\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[ 3.4210e-01, -1.1714e-01, -2.7928e-01,  1.6665e-04, -1.0937e-01,\n",
            "         -2.8613e-01],\n",
            "        [-2.0815e-01, -2.4523e-01, -2.6536e-01, -3.9900e-01, -1.7434e-01,\n",
            "         -2.6726e-01],\n",
            "        [-1.5169e-01, -3.5114e-01, -3.9192e-01,  1.8578e-01, -1.1592e-01,\n",
            "         -2.7624e-01],\n",
            "        [ 2.9939e-01,  9.2404e-02, -3.3420e-01,  2.2824e-01, -1.9231e-01,\n",
            "         -9.2125e-02],\n",
            "        [ 3.3559e-01,  2.7832e-01,  3.2552e-01, -1.0750e-01, -5.3136e-02,\n",
            "          3.1286e-01],\n",
            "        [ 1.6141e-02,  2.4356e-01,  7.9191e-02,  2.5596e-01, -3.4911e-03,\n",
            "          1.1824e-01]])\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([ 0.2993, -0.2149, -0.0318, -0.2958, -0.2523,  0.2699])\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[-0.1668, -0.0297, -0.3204, -0.0275, -0.3058,  0.3439]])\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([-0.0079])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 1000 0.99"
      ],
      "metadata": {
        "id": "AZrEQ4ieTxeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_pi_b_1000 = action_probs_top_n_epsilon(q_table, 1, 0.4)\n",
        "pi_b_1000 = experiment_actions(1000, env_30, P_pi_b_1000)\n",
        "P_pi_e_1000 = action_probs_top_n_epsilon(q_table, 2, 0.05)\n",
        "# pi_e_1000 = experiment_actions(1000, env, P_pi_e_1000)\n",
        "model_1000_0p99 = CustomizableFeatureNet(input_dim=2, hidden_dims=[6, 6], output_dim=1, dtype = torch.float32)\n",
        "test_1000_0p99 = SCOPE_straight(model_1000_0p99, 0.90, 10000, pi_b_1000, P_pi_b_1000, P_pi_e_1000, dtype = torch.float32)\n",
        "model_1000_0p99 = train_var_scope(model_1000_0p99, 5, 0.001, test_1000_0p99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06BDW8QSg_g",
        "outputId": "775bd8ae-65b2-4ede-9674-30b8610a6f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "IS variance:  tensor(1.3628e-08)\n",
            "SCOPE Var loss:  tensor(0.0071, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.0071372101083397865\n",
            "----------------------------------------\n",
            "Epoch 2\n",
            "IS variance:  tensor(1.3628e-08)\n",
            "SCOPE Var loss:  tensor(0.0059, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005885153077542782\n",
            "----------------------------------------\n",
            "Epoch 3\n",
            "IS variance:  tensor(1.3628e-08)\n",
            "SCOPE Var loss:  tensor(0.0057, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005744975060224533\n",
            "----------------------------------------\n",
            "Epoch 4\n",
            "IS variance:  tensor(1.3628e-08)\n",
            "SCOPE Var loss:  tensor(0.0056, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005605767946690321\n",
            "----------------------------------------\n",
            "Epoch 5\n",
            "IS variance:  tensor(1.3628e-08)\n",
            "SCOPE Var loss:  tensor(0.0055, grad_fn=<VarBackward0>)\n",
            "Total Loss: 0.005469260271638632\n",
            "----------------------------------------\n",
            "Parameter name: hidden_layers.0.weight\n",
            "Weights: tensor([[-0.3398,  0.4284],\n",
            "        [ 0.6470, -0.5601],\n",
            "        [-0.0836, -0.5419],\n",
            "        [ 0.2371, -0.0722],\n",
            "        [-0.2191, -0.1823],\n",
            "        [-0.6192, -0.4586]])\n",
            "Parameter name: hidden_layers.0.bias\n",
            "Weights: tensor([ 0.4420,  0.4668,  0.2511,  0.5432, -0.0262, -0.6044])\n",
            "Parameter name: hidden_layers.1.weight\n",
            "Weights: tensor([[-0.1204, -0.0890, -0.2383,  0.3484,  0.3641, -0.3094],\n",
            "        [-0.3600, -0.0599,  0.1120,  0.2181,  0.0257,  0.3971],\n",
            "        [-0.0396, -0.1132, -0.0198, -0.3503,  0.3299,  0.3989],\n",
            "        [-0.3419, -0.0945, -0.0509,  0.2863, -0.0306,  0.4073],\n",
            "        [ 0.2843,  0.4049,  0.2897, -0.2459,  0.1344,  0.0443],\n",
            "        [-0.0682,  0.2438, -0.2887,  0.2774,  0.4038, -0.2734]])\n",
            "Parameter name: hidden_layers.1.bias\n",
            "Weights: tensor([-0.1609,  0.3603,  0.3371, -0.4036,  0.0650,  0.0022])\n",
            "Parameter name: output_layer.weight\n",
            "Weights: tensor([[-0.3125, -0.0285,  0.1303,  0.3379,  0.3765, -0.1660]])\n",
            "Parameter name: output_layer.bias\n",
            "Weights: tensor([-0.1458])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tM7NaSOMSktR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}