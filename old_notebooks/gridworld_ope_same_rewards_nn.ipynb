{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr07-pBj7rpG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "from scipy.optimize import minimize\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3QR_86NGdz"
      },
      "source": [
        "# Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvuZyB-zTvOu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, height, width, start, end, bad_regions, good_regions, good_region_reward, bad_region_reward, final_reward, sparsity):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.bad_regions = bad_regions\n",
        "        self.good_regions = good_regions\n",
        "        self.good_region_reward = good_region_reward\n",
        "        self.bad_region_reward = bad_region_reward\n",
        "        self.final_reward = final_reward\n",
        "        self.sparsity = sparsity\n",
        "\n",
        "        self.state_rewards = self.generate_state_rewards()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start\n",
        "\n",
        "    def generate_state_rewards(self):\n",
        "        state_rewards = {}\n",
        "        for x in range(self.width):\n",
        "            for y in range(self.height):\n",
        "                if (x, y) in self.good_regions:\n",
        "                    state_rewards[(x, y)] = self.good_region_reward\n",
        "                elif (x, y) in self.bad_regions:\n",
        "                    state_rewards[(x, y)] = self.bad_region_reward\n",
        "                elif (x,y) == self.start:\n",
        "                  state_rewards[(x,y)] = 0\n",
        "                elif (x,y) == self.end:\n",
        "                  state_rewards[(x,y)] = self.final_reward\n",
        "                else:\n",
        "                    state_rewards[(x, y)] = 0.5 if np.random.random() < self.sparsity else 0.0\n",
        "        return state_rewards\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        # Get the reward based on the current state and policy context\n",
        "        reward = self.state_rewards.get((x, y), 0)\n",
        "\n",
        "        if action == \"up\" and y < self.height - 1:\n",
        "            y += 1\n",
        "        elif action == \"down\" and y > 0:\n",
        "            y -= 1\n",
        "        elif action == \"left\" and x > 0:\n",
        "            x -= 1\n",
        "        elif action == \"right\" and x < self.width - 1:\n",
        "            x += 1\n",
        "\n",
        "        # Update agent position\n",
        "        self.agent_position = (x, y)\n",
        "\n",
        "        # Get the reward based on the current state and policy context\n",
        "        reward = self.state_rewards.get(self.agent_position, 0)\n",
        "\n",
        "        # # Update state_rewards for self.agent_position if needed\n",
        "        # if self.agent_position in self.good_regions:\n",
        "        #     self.state_rewards[self.agent_position] = self.good_region_reward\n",
        "        # elif self.agent_position in self.bad_regions:\n",
        "        #     self.state_rewards[self.agent_position] = self.bad_region_reward\n",
        "\n",
        "        if self.agent_position in self.end:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        # Get the reward for the updated position and policy context\n",
        "        updated_reward = self.state_rewards.get(self.agent_position, 0)\n",
        "\n",
        "        # Check if the new position is the end state\n",
        "        done = (self.agent_position == self.end)\n",
        "\n",
        "        return self.agent_position, updated_reward, done\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQACkJ1xWBoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.0):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def select_action(self, policy_func):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "        else:\n",
        "            # Use the provided policy function to get the best action\n",
        "            action = policy_func()\n",
        "        return action\n",
        "\n",
        "# Define different policy functions outside the class\n",
        "\n",
        "def random_policy():\n",
        "    # Choose a random action\n",
        "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "\n",
        "# def behavior_policy(behav_policy):\n",
        "#     action_probs = behav_policy\n",
        "#     return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "# def evaluation_policy(eval_policy):\n",
        "#     action_probs = eval_policy\n",
        "#     return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def run_policy(policy):\n",
        "    action_probs = policy\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "\n",
        "def manhattan_distance(pos1, pos2):\n",
        "    # Compute the Manhattan distance between two positions\n",
        "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
        "\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipigR-ZNTo6"
      },
      "source": [
        "# Generating Policy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ActZ0YInJCE"
      },
      "outputs": [],
      "source": [
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVJx8AYLlZqX"
      },
      "outputs": [],
      "source": [
        "\n",
        "good_regions = [(3,3)]\n",
        "bad_regions = [(1,1),(2,2)]\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create an instance of the GridWorld class\n",
        "gridworld = GridWorld(height, width, start, end, bad_regions, good_regions, 1, -2, 3, 0.5)\n",
        "\n",
        "# Access the state_rewards dictionary\n",
        "rewards = gridworld.state_rewards\n",
        "\n",
        "# Print the rewards for each state\n",
        "for state, reward in rewards.items():\n",
        "    print(f\"State: {state}, Reward: {reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze57OKSiUSlA"
      },
      "outputs": [],
      "source": [
        "def create_policy_set(env, policy_func, policy, num_episodes):\n",
        "  # Create a list to store policies as trajectories\n",
        "  policies = []\n",
        "\n",
        "  # Run multiple episodes\n",
        "  for episode in range(num_episodes):\n",
        "      # Create a new Agent for each episode to generate a different policy\n",
        "\n",
        "      agent = Agent(epsilon=0.0)\n",
        "\n",
        "      # print(episode)\n",
        "      # Run an episode\n",
        "      env.reset()\n",
        "      done = False\n",
        "      trajectory = []  # Store the trajectory for the current episode\n",
        "      cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "      while not done:\n",
        "          state = env.agent_position  # Get the current state\n",
        "          # print(\"State: \",state)\n",
        "          action = agent.select_action(lambda: policy_func(policy))\n",
        "          # print(\"Action: \",action)\n",
        "          next_state, reward, done = env.step(action)\n",
        "          # print(\"Next State: \", next_state)\n",
        "\n",
        "          # Compute cumulative reward\n",
        "          cumulative_reward += reward\n",
        "\n",
        "          # # Compute feature function values (manhattan distances)\n",
        "          good_region_distances = [manhattan_distance(state, gr) for gr in env.good_regions]\n",
        "          bad_region_distances = [manhattan_distance(state, br) for br in env.bad_regions]\n",
        "\n",
        "          # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "          trajectory.append((state, action, reward, next_state, good_region_distances, bad_region_distances))\n",
        "\n",
        "      # Append the trajectory to the policies list\n",
        "      policies.append(trajectory)\n",
        "\n",
        "  return policies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akXuyGtRlBW5"
      },
      "outputs": [],
      "source": [
        "pi_b = create_policy_set(gridworld, run_policy, behav_policy, 200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_b[0][0][4]+pi_b[0][0][5]"
      ],
      "metadata": {
        "id": "G52Wc3evY5I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLc1oFcQZSFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZCK9qycHgbr"
      },
      "outputs": [],
      "source": [
        "def calc_V_pi_e(evaluation_policies):\n",
        "    all_timesteps = []\n",
        "    gamma = 0.9\n",
        "    for j in range(len(evaluation_policies)):\n",
        "        Timestep_values = []\n",
        "        for i in range(len(evaluation_policies[j])):\n",
        "          # print(i)\n",
        "          timestep = gamma ** (i) * evaluation_policies[j][i][2]\n",
        "          Timestep_values.append(timestep)\n",
        "\n",
        "        all_timesteps.append(Timestep_values)\n",
        "\n",
        "    V_est = sum([sum(sublist) for sublist in all_timesteps])/len(evaluation_policies)\n",
        "    return V_est"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UdUjHYmPsZl"
      },
      "source": [
        "# Saving and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYA9NMvYUl-y"
      },
      "outputs": [],
      "source": [
        "def filename(env, behav_policy, eval_policy, num_episodes, train_split, sparsity):\n",
        "    good_regions_str = \"_\".join([f\"gr_{pos[0]}_{pos[1]}\" for pos in env.good_regions])\n",
        "    bad_regions_str = \"_\".join([f\"br_{pos[0]}_{pos[1]}\" for pos in env.bad_regions])\n",
        "\n",
        "    behav_probs_str = \"_\".join([f\"{prob:.2f}\" for prob in behav_policy.values()])\n",
        "    eval_probs_str = \"_\".join([f\"{prob:.2f}\" for prob in eval_policy.values()])\n",
        "\n",
        "    file = f\"pi_b_{behav_probs_str}_pi_e_{eval_probs_str}_{good_regions_str}_{env.good_region_reward}_{bad_regions_str}_{env.bad_region_reward}_trajectories_{num_episodes}_train_split_{train_split}_sparsity_{sparsity}.txt\"\n",
        "    return file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tub26M0SkDKB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM1h_YzNk6F3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Define the path to your desired folder\n",
        "# folder_path = '/content/drive/MyDrive/gridworld_same_reward_OPE_experiments'\n",
        "\n",
        "# # Change the working directory to the specified folder\n",
        "# os.chdir(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfI_8J51Pwer"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_data_to_file(data, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "def load_data_from_file(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nNbcCLTOMxK"
      },
      "source": [
        "# OPE Calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608qtLdqhbOO"
      },
      "source": [
        "## Importance Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ykvUcj6YAJM"
      },
      "outputs": [],
      "source": [
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "  all_weights = []\n",
        "  for trajectory in behavior_policies:\n",
        "    cum_ratio = 1\n",
        "    cumul_weights = []\n",
        "    for step in trajectory:\n",
        "        ratio = eval_policy[step[1]]/behav_policy[step[1]]\n",
        "        # print(\"Ratio:\",ratio)\n",
        "        cum_ratio *= ratio\n",
        "        cumul_weights.append(cum_ratio)\n",
        "        # print(\"Cumul:\",cum_ratio)\n",
        "    all_weights.append(cumul_weights)\n",
        "\n",
        "  return all_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtD5zeIbVL9q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def per_step_IS(scope_set, num_bootstraps):\n",
        "#     all_timesteps = []\n",
        "#     gamma = 0.9\n",
        "#     # scope_set,_ = subset_policies(scope_set, phi_trajectories)\n",
        "#     scope_weights = calculate_importance_weights(eval_policy, behav_policy, scope_set)\n",
        "#     for j in range(len(scope_weights)):\n",
        "#         Timestep_values = []\n",
        "#         for i in range(len(scope_weights[j]) - 1):\n",
        "#             timestep = gamma ** (i) * scope_weights[j][i] * scope_set[j][i][2]\n",
        "#             Timestep_values.append(timestep)\n",
        "\n",
        "#         all_timesteps.append(Timestep_values)\n",
        "\n",
        "#     V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "#     # seed_value = 42\n",
        "#     # np.random.seed(seed_value)\n",
        "\n",
        "#     num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "#     seed_value = 0\n",
        "#     np.random.seed(seed_value)\n",
        "\n",
        "#     bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "#                          for _ in range(num_bootstraps)]\n",
        "\n",
        "#     V_per_sample = [sum(sample) / len(scope_set) for sample in bootstrap_samples]\n",
        "#     V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "#     std_deviation = np.std(V_per_sample)\n",
        "#     quartiles = np.percentile(V_per_sample, [0,25, 50, 75,100])\n",
        "#     max_value = np.max(V_per_sample)\n",
        "#     min_value = np.min(V_per_sample)\n",
        "#     mean = np.mean(V_per_sample)\n",
        "\n",
        "#     return {\n",
        "#         'std_deviation': std_deviation,\n",
        "#         'quartiles': quartiles,\n",
        "#         'max_value': max_value,\n",
        "#         'min_value': min_value,\n",
        "#         'mean': mean\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgadQNBwlT2e"
      },
      "source": [
        "## IS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kguK1QYiXeM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def per_step_IS1(scope_set, num_bootstraps):\n",
        "#     all_timesteps = []\n",
        "#     gamma = 0.9\n",
        "#     # scope_set,_ = subset_policies(scope_set, phi_trajectories)\n",
        "#     scope_weights = calculate_importance_weights(eval_policy, behav_policy, scope_set)\n",
        "#     for j in range(len(scope_weights)):\n",
        "#         Timestep_values = []\n",
        "#         for i in range(len(scope_weights[j]) - 1):\n",
        "#             timestep = gamma ** (i) * scope_weights[j][i] * scope_set[j][i][2]\n",
        "#             Timestep_values.append(timestep)\n",
        "\n",
        "#         all_timesteps.append(Timestep_values)\n",
        "\n",
        "#     V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "\n",
        "#     num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "\n",
        "#     V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "#     num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "#     std_devs = []\n",
        "#     means = []\n",
        "\n",
        "#     seed_value = 0\n",
        "#     np.random.seed(seed_value)\n",
        "\n",
        "#     for i in range(5):\n",
        "\n",
        "#       bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "#                             for _ in range(num_bootstraps)]\n",
        "\n",
        "#       V_per_sample = [sum(sample)/len(scope_set) for sample in bootstrap_samples]\n",
        "#       V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "#       std_deviation = np.std(V_per_sample)\n",
        "#       quartiles = np.percentile(V_per_sample, [0,25, 50, 75,100])\n",
        "#       max_value = np.max(V_per_sample)\n",
        "#       min_value = np.min(V_per_sample)\n",
        "#       mean = np.mean(V_per_sample)\n",
        "\n",
        "#       std_devs.append(std_deviation)\n",
        "#       means.append(mean)\n",
        "\n",
        "#     return {\n",
        "#         'std_devs_list': std_devs,\n",
        "#         'mean_list': means\n",
        "#     }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6NFn50rVOW4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def SCOPE(scope_policies, beta, num_bootstraps):\n",
        "#     all_timesteps = []\n",
        "#     gamma = 0.9\n",
        "#     # scope_policies,_ = subset_policies(scope_policies, phi_trajectories)\n",
        "#     scope_weights = calculate_importance_weights(eval_policy, behav_policy, scope_policies)\n",
        "#     for j in range(len(scope_weights)):\n",
        "#         Timestep_values = []\n",
        "#         for i in range(len(scope_weights[j]) - 1):\n",
        "#             features = scope_policies[j][i][5] + scope_policies[j][i][6]\n",
        "#             features_next = scope_policies[j][i + 1][5] + scope_policies[j][i + 1][6]\n",
        "#             timestep = gamma ** (i) * scope_weights[j][i] * (scope_policies[j][i][2] + gamma * phi(features_next, beta) - phi(features, beta))\n",
        "#             Timestep_values.append(timestep)\n",
        "\n",
        "#         all_timesteps.append(Timestep_values)\n",
        "\n",
        "\n",
        "#     V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "\n",
        "#     num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "#     bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "#                          for _ in range(num_bootstraps)]\n",
        "\n",
        "#     V_per_sample = [sum(sample)/len(scope_policies) for sample in bootstrap_samples]\n",
        "#     V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "#     std_deviation = np.std(V_per_sample)\n",
        "#     quartiles = np.percentile(V_per_sample, [0,25, 50, 75,100])\n",
        "#     max_value = np.max(V_per_sample)\n",
        "#     min_value = np.min(V_per_sample)\n",
        "#     mean = np.mean(V_per_sample)\n",
        "\n",
        "#     return {\n",
        "#         'std_deviation': std_deviation,\n",
        "#         'quartiles': quartiles,\n",
        "#         'max_value': max_value,\n",
        "#         'min_value': min_value,\n",
        "#         'mean': mean\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzDCmT7zm4IV"
      },
      "source": [
        "## SCOPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dmYiPP1EF-n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def SCOPE1(scope_policies, beta, num_bootstraps):\n",
        "#     all_timesteps = []\n",
        "#     gamma = 0.9\n",
        "#     # scope_policies,_ = subset_policies(scope_policies, phi_trajectories)\n",
        "#     scope_weights = calculate_importance_weights(eval_policy, behav_policy, scope_policies)\n",
        "#     for j in range(len(scope_weights)):\n",
        "#         Timestep_values = []\n",
        "#         for i in range(len(scope_weights[j]) - 1):\n",
        "#             features = scope_policies[j][i][5] + scope_policies[j][i][6]\n",
        "#             features_next = scope_policies[j][i + 1][5] + scope_policies[j][i + 1][6]\n",
        "#             timestep = gamma ** (i) * scope_weights[j][i] * (scope_policies[j][i][2] + gamma * phi(features_next, beta) - phi(features, beta))\n",
        "#             Timestep_values.append(timestep)\n",
        "\n",
        "#         all_timesteps.append(Timestep_values)\n",
        "\n",
        "\n",
        "#     V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "#     num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "#     std_devs = []\n",
        "#     means = []\n",
        "#     seed_value = 0\n",
        "#     np.random.seed(seed_value)\n",
        "#     for i in range(5):\n",
        "\n",
        "#       bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "#                             for _ in range(num_bootstraps)]\n",
        "\n",
        "#       V_per_sample = [sum(sample)/len(scope_policies) for sample in bootstrap_samples]\n",
        "#       V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "#       std_deviation = np.std(V_per_sample)\n",
        "#       quartiles = np.percentile(V_per_sample, [0,25, 50, 75,100])\n",
        "#       max_value = np.max(V_per_sample)\n",
        "#       min_value = np.min(V_per_sample)\n",
        "#       mean = np.mean(V_per_sample)\n",
        "\n",
        "#       std_devs.append(std_deviation)\n",
        "#       means.append(mean)\n",
        "\n",
        "#     return {\n",
        "#         'std_devs_list': std_devs,\n",
        "#         'mean_list': means\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL9BcVXloGpd"
      },
      "source": [
        "# Variance Preparation and Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn5ni6carcIY"
      },
      "source": [
        "## Phi functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfXiEdgqDE4D"
      },
      "outputs": [],
      "source": [
        "# def phi(features, beta):\n",
        "#   features = np.array(features)\n",
        "#   beta = np.array(beta)\n",
        "#   phi_linear = np.dot(beta,features)\n",
        "#   return phi_linear\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv6HgrkCrgpS"
      },
      "outputs": [],
      "source": [
        "# def phi2(features, beta):\n",
        "#     features = np.array(features)\n",
        "#     beta = np.array(beta)\n",
        "\n",
        "#     # Calculate the matrix multiplication and then element-wise multiplication\n",
        "#     phi_quadratic = np.dot(features, np.dot(beta, features.T))\n",
        "\n",
        "#     return phi_quadratic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subset Policies"
      ],
      "metadata": {
        "id": "ZpIJL3SyZjZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BpxUSqy7K86"
      },
      "outputs": [],
      "source": [
        "\n",
        "def subset_policies(policies, percent_to_estimate_phi):\n",
        "    seed_value = 0\n",
        "    np.random.seed(seed_value)\n",
        "    num_policies = len(policies)\n",
        "    num_policies_to_estimate_phi = int(num_policies * percent_to_estimate_phi)\n",
        "\n",
        "    policies_for_scope = policies[num_policies_to_estimate_phi:]\n",
        "    policies_for_phi = policies[:num_policies_to_estimate_phi]\n",
        "\n",
        "    return policies_for_scope, policies_for_phi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1njZecNjG27E"
      },
      "source": [
        "## Variance Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NdwpKSkhxZR"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# # gamma = 0.9\n",
        "# # beta = [random.random() for _ in range(3)]\n",
        "# def variance_terms(policy_set,gamma, beta):\n",
        "#   all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "#   y_w_r_all = 0\n",
        "#   r_all = 0\n",
        "#   f_a = 0\n",
        "#   for n in range(len(policy_set)):\n",
        "#     y_w_r = 0\n",
        "#     r = 0\n",
        "#     for t in range(len(policy_set[n])-1):\n",
        "#       features = policy_set[n][t][0]\n",
        "#       y_w_r += gamma**(t)*all_weights[n][t]*policy_set[n][t][2]\n",
        "#       if t>0:\n",
        "#         r += phi(features, beta)*(all_weights[n][t-1]-all_weights[n][t])\n",
        "#     features_last = policy_set[n][-1][0]\n",
        "#     features_first = policy_set[n][0][0]\n",
        "#     y_w_r_all += y_w_r\n",
        "#     f_a +=  gamma**(len(policy_set[n]))*all_weights[n][-1]*phi(features_last,beta) - phi(features_first, beta)\n",
        "#     r_all += r\n",
        "\n",
        "#   IS = y_w_r_all/len(policy_set)\n",
        "#   R = r_all/len(policy_set)\n",
        "#   F = f_a/len(policy_set)\n",
        "#   return IS, R, F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_terms(policy_set, gamma, feature_network):\n",
        "    all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "    y_w_r_all = 0\n",
        "    r_all = 0\n",
        "    f_a = 0\n",
        "    for n in range(len(policy_set)):\n",
        "        y_w_r = 0\n",
        "        r = 0\n",
        "        for t in range(len(policy_set[n]) - 1):\n",
        "            state = policy_set[n][t][0]\n",
        "            distances = feature_network(torch.tensor(state, dtype=torch.float32))\n",
        "            y_w_r += gamma**(t) * all_weights[n][t] * policy_set[n][t][2]\n",
        "            if t > 0:\n",
        "                r += distances * (all_weights[n][t-1] - all_weights[n][t])\n",
        "        state_last = policy_set[n][-1][0]\n",
        "        state_first = policy_set[n][0][0]\n",
        "        y_w_r_all += y_w_r\n",
        "        f_a += gamma**(len(policy_set[n])) * all_weights[n][-1] * feature_network(torch.tensor(state_last, dtype=torch.float32)) - feature_network(torch.tensor(state_first, dtype=torch.float32))\n",
        "        r_all += r\n",
        "\n",
        "    IS = y_w_r_all / len(policy_set)\n",
        "    R = r_all / len(policy_set)\n",
        "    F = f_a / len(policy_set)\n",
        "    return IS, R, F\n"
      ],
      "metadata": {
        "id": "-xgMMDYe3ltg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTxzFiNTwV30"
      },
      "outputs": [],
      "source": [
        "def calc_variance(phi_policies, gamma, beta, num_bootstrap_samples):\n",
        "  # Set the seed value (you can use any integer value)\n",
        "  seed_value = 0\n",
        "  np.random.seed(seed_value)\n",
        "  num_trajectories_to_sample = max(1, len(phi_policies))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(phi_policies, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstrap_samples)]\n",
        "  IS_all = []\n",
        "  R_all = []\n",
        "  F_all = []\n",
        "\n",
        "  for pol in bootstrap_samples:\n",
        "    IS, R, F = variance_terms(pol,0.9,beta)\n",
        "    IS_all.append(IS)\n",
        "    R_all.append(R)\n",
        "    F_all.append(F)\n",
        "  IS_sq = np.mean([num**2 for num in IS_all])\n",
        "  IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "  R_sq = np.mean([num**2 for num in R_all])\n",
        "  IS_sq_all = (np.mean(IS_all))**2\n",
        "  IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "  R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "  variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "  variance_is = IS_sq - IS_sq_all\n",
        "  return variance_scope, variance_is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W75oAdqlrSBL"
      },
      "outputs": [],
      "source": [
        "# def calc_variance1(phi_policies, gamma, beta, num_bootstrap_samples):\n",
        "#   # Set the seed value (you can use any integer value)\n",
        "#   # seed_value = 42\n",
        "#   # np.random.seed(seed_value)\n",
        "#   num_trajectories_to_sample = max(1, len(phi_policies))\n",
        "\n",
        "#   bootstrap_samples = [np.random.choice(phi_policies, size=num_trajectories_to_sample, replace=True)\n",
        "#                          for _ in range(num_bootstrap_samples)]\n",
        "#   IS_all = []\n",
        "#   R_all = []\n",
        "#   F_all = []\n",
        "\n",
        "#   for pol in bootstrap_samples:\n",
        "#     IS, R, F = variance_terms(pol,0.9,beta)\n",
        "#     IS_all.append(IS)\n",
        "#     R_all.append(R)\n",
        "#     F_all.append(F)\n",
        "#   IS_sq = np.mean([num**2 for num in IS_all])\n",
        "#   IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "#   R_sq = np.mean([num**2 for num in R_all])\n",
        "#   IS_sq_all = (np.mean(IS_all))**2\n",
        "#   IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "#   R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "#   variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "#   variance_is = IS_sq - IS_sq_all\n",
        "#   return variance_scope, variance_is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vQxkjcnrEM"
      },
      "source": [
        "An example of an initial guess of phi can be seen below, as you can see the SCOPE variance is not ideal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74yBmg6_edJv"
      },
      "outputs": [],
      "source": [
        "# scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "# variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "# print(\"Var SCOPE: \",variance_scope)\n",
        "# print(\"Var IS: \",variance_is)\n",
        "# print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefNOC1GC8Wm"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWx9IbJon2IH"
      },
      "source": [
        "Here we aim to optimize beta to minimize SCOPE variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws00tRw1KZnN"
      },
      "outputs": [],
      "source": [
        "# # Define the objective function to minimize variance_scope\n",
        "# def objective_function(beta, phi_set):\n",
        "#     # scope_set, phi_set = subset_policies(phi_set, phi_trajectories)\n",
        "#     variance_scope, variance_is = calc_variance(phi_set, 0.9, beta, 100)\n",
        "#     return variance_scope\n",
        "\n",
        "# # Set the initial values of beta\n",
        "# # initial_beta = np.array([ 0.2610704,   0.30396575, -0.43850237])\n",
        "\n",
        "\n",
        "# def optimize_variance_scope(initial_beta, phi_set, phi_trajectories):\n",
        "#     # Lists to store beta and variance_scope values at each iteration\n",
        "#     all_betas = []\n",
        "#     all_variance_scopes = []\n",
        "\n",
        "#     # Callback function to record beta and variance_scope values at each iteration\n",
        "#     def callback_function(beta):\n",
        "#         all_betas.append(beta.copy())\n",
        "#         variance_scope = objective_function(beta, phi_set)\n",
        "#         all_variance_scopes.append(variance_scope)\n",
        "#         print(\"Iteration:\", len(all_betas))\n",
        "#         print(\"Beta:\", beta)\n",
        "#         print(\"Variance Scope:\", variance_scope)\n",
        "#         print(\"----------\")\n",
        "\n",
        "#     # Run the optimization with the callback\n",
        "#     result = minimize(\n",
        "#         objective_function,\n",
        "#         initial_beta,\n",
        "#         args=(phi_set),\n",
        "#         method='L-BFGS-B',\n",
        "#         callback=callback_function\n",
        "#     )\n",
        "\n",
        "#     # Extract the optimal beta values\n",
        "#     optimal_beta = result.x\n",
        "\n",
        "#     return optimal_beta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward Neural Network"
      ],
      "metadata": {
        "id": "KJnAAVOK8Hn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "class FeatureNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(FeatureNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        features = torch.relu(self.fc3(x))\n",
        "        return features\n",
        "\n"
      ],
      "metadata": {
        "id": "fUkdee3V8MuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop\n",
        "input_size = 2  # x, y coordinates\n",
        "hidden_size = 3  # Define based on your needs\n",
        "\n",
        "feature_network = FeatureNetwork(input_size, hidden_size)\n",
        "optimizer = optim.Adam(feature_network.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "_,policies_for_phi = subset_policies(pi_b, 0.3)\n",
        "for epoch in range(num_epochs):\n",
        "    for trajectory in policies_for_phi:\n",
        "        for step in trajectory:\n",
        "            state = step[0]\n",
        "            target_distances = step[3]  # Use the calculated distances as targets\n",
        "\n",
        "            distances = feature_network(torch.tensor(state, dtype=torch.float32))\n",
        "\n",
        "            # Calculate the loss based on your custom variance expression\n",
        "            loss, _ = calc_variance(policies_for_phi, 0.9, feature_network, 500)\n",
        "\n",
        "            # Zero the gradients, perform backpropagation, and update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "r7AAw0TXbLA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply optimized weights (beta) to test set of policies and calculate variance reduction\n",
        "policies_for_test,_ = subset_policies(pi_b, 0.3)[0]  # Use policies for test\n",
        "variance_scope_after_optimization, _ = calc_variance(policies_for_test, 0.9, feature_network.fc3.weight.detach().numpy(), num_bootstrap_samples)\n",
        "\n",
        "print(f\"Variance reduction after optimization: {variance_scope_before - variance_scope_after_optimization}\")\n",
        "\n",
        "# ... (rest of the code)"
      ],
      "metadata": {
        "id": "qtA2e326zZVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcxeSyT-zZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGxf7jTmp4Gv"
      },
      "source": [
        "# Playground to run individual trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vXP1i-LNIuh"
      },
      "outputs": [],
      "source": [
        "# env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 1, -2, 3, 0.1)\n",
        "# behavior_policies = create_policy_set(env, run_policy,behav_policy, 400)\n",
        "# initial_beta = [random.uniform(-0.5, 0.5) for _ in range(len(env.good_regions + env.bad_regions))]\n",
        "# scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "# optimal_beta = optimize_variance_scope(initial_beta, phi_set, 0.3)\n",
        "# variance_scope, variance_is = calc_variance(phi_set,0.9,optimal_beta, 500)\n",
        "# print(\"Var SCOPE_phi: \",variance_scope)\n",
        "# print(\"Var IS_phi: \",variance_is)\n",
        "# print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNCld_TeS1XZ"
      },
      "outputs": [],
      "source": [
        "# scope_results = SCOPE(scope_set,optimal_beta,500)\n",
        "# IS_results = per_step_IS(scope_set,500)\n",
        "# print(\"SCOPE results: \", scope_results)\n",
        "# print(\"IS results: \", IS_results)\n",
        "# evaluation_policies = create_policy_set(env, run_policy,eval_policy, 1000)\n",
        "# true_evaluation = calc_V_pi_e(evaluation_policies)\n",
        "# print(\"true eval: \", true_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym8uyhzWhgl3"
      },
      "source": [
        "# Modify Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5WpWxt_mFam"
      },
      "outputs": [],
      "source": [
        "def modify_data(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "  file = filename(env, behav_policy, eval_policy, num_episodes, phi_traj, sparsity)\n",
        "  # Check if the file already exists\n",
        "  if os.path.exists(file):\n",
        "    loaded_data = load_data_from_file(file)\n",
        "    behavior_policies = loaded_data['policy_set']\n",
        "    scope_set, phi_set = subset_policies(behavior_policies, phi_traj)\n",
        "    beta = loaded_data['optimal_beta']\n",
        "    # scope_results = loaded_data['scope_results']\n",
        "    scope_results_new = SCOPE1(scope_set,beta,500)\n",
        "    IS_results_new = per_step_IS1(scope_set,500)\n",
        "    loaded_data['scope_results'] = scope_results_new\n",
        "    loaded_data['IS_results'] = IS_results_new\n",
        "    true_evals = []\n",
        "    for i in range(5):\n",
        "      evaluation_policies = create_policy_set(env, run_policy,eval_policy, 1000)\n",
        "      true_evaluation = calc_V_pi_e(evaluation_policies)\n",
        "      true_evals.append(true_evaluation)\n",
        "    loaded_data['True Evaluations'] = true_evals\n",
        "    save_data_to_file(loaded_data, file)\n",
        "    print(\"modifying... \")\n",
        "    print(\"scope new: \",scope_results_new)\n",
        "    print(\"IS new: \", IS_results_new)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zSCb9vXoalr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lp7-77cVv4t"
      },
      "outputs": [],
      "source": [
        "# def run_experiment(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "#   file = filename(env, behav_policy, eval_policy, num_episodes, phi_traj, sparsity)\n",
        "#   # Check if the file already exists\n",
        "#   if os.path.exists(file):\n",
        "#     loaded_data = load_data_from_file(file)\n",
        "#     # modify_data(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity)\n",
        "#     return loaded_data\n",
        "\n",
        "\n",
        "#   behavior_policies = create_policy_set(env, run_policy,behav_policy, num_episodes)\n",
        "#   initial_beta = [random.uniform(-0.5, 0.5) for _ in range(len(env.good_regions + env.bad_regions))]\n",
        "#   scope_set, phi_set = subset_policies(behavior_policies, phi_traj)\n",
        "#   optimal_beta = optimize_variance_scope(initial_beta, phi_set, phi_traj)\n",
        "#   variance_scope, variance_is = calc_variance(phi_set,0.9,optimal_beta, 500)\n",
        "#   print(\"Var SCOPE_phi: \",variance_scope)\n",
        "#   print(\"Var IS_phi: \",variance_is)\n",
        "#   print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)\n",
        "#   scope_results = SCOPE(scope_set,optimal_beta,500)\n",
        "#   IS_results = per_step_IS(scope_set,500)\n",
        "#   print(\"SCOPE results: \", scope_results)\n",
        "#   print(\"IS results: \", IS_results)\n",
        "#   true_evals = []\n",
        "#   for i in range(5):\n",
        "#     evaluation_policies = create_policy_set(env, run_policy,eval_policy, 1000)\n",
        "#     true_evaluation = calc_V_pi_e(evaluation_policies)\n",
        "#     true_evals.append(true_evaluation)\n",
        "#   print(\"true eval: \", np.mean(np.array(true_evals)))\n",
        "#   data_to_save = {\n",
        "#     'policy_set': behavior_policies,\n",
        "#     'optimal_beta': optimal_beta,\n",
        "#     'variance_scope_train': variance_scope,\n",
        "#     'variance_IS_train': variance_is,\n",
        "#     'scope_results': scope_results,\n",
        "#     'IS_results': IS_results,\n",
        "#     'True Evaluations': true_evals\n",
        "#   }\n",
        "#   save_data_to_file(data_to_save, file)\n",
        "\n",
        "#   return data_to_save\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6UDEPbgEG7i"
      },
      "source": [
        "# Experiment Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ICHMVSzs7j"
      },
      "outputs": [],
      "source": [
        "def run_experiment1(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "  file = filename(env, behav_policy, eval_policy, num_episodes, phi_traj, sparsity)\n",
        "  # Check if the file already exists\n",
        "  if os.path.exists(file):\n",
        "    loaded_data = load_data_from_file(file)\n",
        "    # loaded_data = modify_data(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity)\n",
        "    return loaded_data\n",
        "\n",
        "  behavior_policies = create_policy_set(env, run_policy,behav_policy, num_episodes)\n",
        "  initial_beta = [random.uniform(-0.5, 0.5) for _ in range(len(env.good_regions + env.bad_regions))]\n",
        "  scope_set, phi_set = subset_policies(behavior_policies, phi_traj)\n",
        "  optimal_beta = optimize_variance_scope(initial_beta, phi_set, phi_traj)\n",
        "  variance_scope, variance_is = calc_variance(phi_set,0.9,optimal_beta, 500)\n",
        "  print(\"Var SCOPE_phi: \",variance_scope)\n",
        "  print(\"Var IS_phi: \",variance_is)\n",
        "  print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)\n",
        "  scope_results = SCOPE1(scope_set,optimal_beta,500)\n",
        "  IS_results = per_step_IS1(scope_set,500)\n",
        "  print(\"SCOPE results: \", scope_results)\n",
        "  print(\"IS results: \", IS_results)\n",
        "  true_evals = []\n",
        "  for i in range(5):\n",
        "    evaluation_policies = create_policy_set(env, run_policy,eval_policy, 1000)\n",
        "    true_evaluation = calc_V_pi_e(evaluation_policies)\n",
        "    true_evals.append(true_evaluation)\n",
        "  print(\"true eval: \", np.mean(np.array(true_evals)))\n",
        "  data_to_save = {\n",
        "    'policy_set': behavior_policies,\n",
        "    'optimal_beta': optimal_beta,\n",
        "    'variance_scope_train': variance_scope,\n",
        "    'variance_IS_train': variance_is,\n",
        "    'scope_results': scope_results,\n",
        "    'IS_results': IS_results,\n",
        "    'True Evaluations': true_evals\n",
        "  }\n",
        "  save_data_to_file(data_to_save, file)\n",
        "\n",
        "  return data_to_save\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMKubDFqtuPQ"
      },
      "outputs": [],
      "source": [
        "def run_loaded(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "  file = filename(env, behav_policy, eval_policy, num_episodes, phi_traj, sparsity)\n",
        "  # Check if the file already exists\n",
        "  if os.path.exists(file):\n",
        "    loaded_data = load_data_from_file(file)\n",
        "    # loaded_data = modify_data(env, num_episodes, behav_policy, eval_policy, phi_traj, sparsity)\n",
        "    return loaded_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve862FgPV2W9"
      },
      "outputs": [],
      "source": [
        "# def plot_rewards_over_trajectories(env, num_trajectories, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "#     combined_scope = []\n",
        "#     combined_is = []\n",
        "\n",
        "#     for i in range(len(num_trajectories)):\n",
        "#         results = run_experiment(env, num_trajectories[i], behav_policy, eval_policy, phi_traj, sparsity)\n",
        "#         if num_trajectories[i] == 200:\n",
        "#             true_val = results['True Evaluations']\n",
        "\n",
        "#         scope_results = results['scope_results']\n",
        "#         is_results = results['IS_results']\n",
        "\n",
        "#         quartiles_scope = scope_results['quartiles']\n",
        "#         quartiles_is = is_results['quartiles']\n",
        "\n",
        "#         combined_scope.append(quartiles_scope)\n",
        "#         combined_is.append(quartiles_is)\n",
        "\n",
        "\n",
        "#     # Transpose the quartiles data for compatibility with boxplot\n",
        "#     combined_scope = np.array(combined_scope).T\n",
        "#     combined_is = np.array(combined_is).T\n",
        "\n",
        "#     # Create box and whisker plots for both SCOPE and IS data on the same plot\n",
        "#     plt.boxplot(combined_scope, positions=np.array(range(len(num_trajectories))) * 2 - 0.4, labels=num_trajectories, widths=0.4, patch_artist=True, boxprops=dict(facecolor='blue'), vert=True)\n",
        "#     plt.boxplot(combined_is, positions=np.array(range(len(num_trajectories))) * 2 + 0.4, labels=num_trajectories, widths=0.4, patch_artist=True, boxprops=dict(facecolor='orange'), vert=True)\n",
        "\n",
        "#     plt.xlabel('Number of Trajectories')\n",
        "#     plt.ylabel('Value Estimate')\n",
        "#     plt.title('SCOPE and stepIS Box and Whisker Plots vs. Number of Trajectories')\n",
        "\n",
        "#     # Add horizontal line for true_val\n",
        "#     plt.axhline(y=true_val, color='green', linestyle='--', label='True Value')\n",
        "\n",
        "#     # Create custom legend handles and labels\n",
        "#     custom_legend_handles = [\n",
        "#         Line2D([0], [0], color='blue', marker='s', markersize=10, label='SCOPE'),\n",
        "#         Line2D([0], [0], color='orange', marker='s', markersize=10, label='stepIS'),\n",
        "#         Line2D([0], [0], color='green', linestyle='--', label='True Value')\n",
        "#     ]\n",
        "\n",
        "#     plt.legend(handles=custom_legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "#     plt.grid(True)\n",
        "#     plt.tight_layout()  # Ensures proper spacing and avoids clipping\n",
        "#     plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNb6dblWEJ9f"
      },
      "source": [
        "# Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6UbFx2hfuSe"
      },
      "outputs": [],
      "source": [
        "def plot_rewards_over_trajectories1(env, num_trajectories, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "    # combined_scope = []\n",
        "    # combined_is = []\n",
        "\n",
        "    combined_scope_means = []\n",
        "    combined_scope_std_devs = []\n",
        "    combined_is_means = []\n",
        "    combined_is_std_devs = []\n",
        "\n",
        "    combined_scope_vars = []\n",
        "    combined_scope_var_std_devs = []\n",
        "    combined_is_vars = []\n",
        "    combined_is_var_std_devs = []\n",
        "\n",
        "    combined_scope_bias = []\n",
        "    combined_scope_bias_std_devs = []\n",
        "    combined_is_bias = []\n",
        "    combined_is_bias_std_devs = []\n",
        "\n",
        "    combined_scope_mse = []\n",
        "    combined_is_mse = []\n",
        "\n",
        "\n",
        "    for i in range(len(num_trajectories)):\n",
        "        results = run_loaded(env, num_trajectories[i], behav_policy, eval_policy, phi_traj, sparsity)\n",
        "        print(\"Trajectories: \", num_trajectories[i])\n",
        "        # if num_trajectories[i] == 200:\n",
        "        true_val = results['True Evaluations']\n",
        "        print(\"True Val: \",true_val)\n",
        "\n",
        "        optimal_beta = results['optimal_beta']\n",
        "        print(\"Optimal Beta: \", optimal_beta)\n",
        "\n",
        "        true_value = np.mean(np.array(true_val))\n",
        "        scope_results = results['scope_results']\n",
        "        is_results = results['IS_results']\n",
        "\n",
        "        print('SCOPE Results: ', scope_results)\n",
        "        print('IS Results: ', is_results)\n",
        "\n",
        "        sd_scope = scope_results['std_devs_list']\n",
        "        sd_is = is_results['std_devs_list']\n",
        "\n",
        "        means_scope = scope_results['mean_list']\n",
        "        means_is = is_results['mean_list']\n",
        "\n",
        "        combined_scope_means.append(np.mean(np.array(means_scope)))\n",
        "        combined_scope_std_devs.append(np.std(np.array(means_scope)))\n",
        "        combined_is_means.append(np.mean(np.array(means_is)))\n",
        "        combined_is_std_devs.append(np.std(np.array(means_is)))\n",
        "\n",
        "        combined_scope_vars.append(np.mean(np.array(sd_scope)**2))\n",
        "        combined_scope_var_std_devs.append(np.std(np.array(sd_scope)**2))\n",
        "        combined_is_vars.append(np.mean(np.array(sd_is)**2))\n",
        "        combined_is_var_std_devs.append(np.std(np.array(sd_is)**2))\n",
        "\n",
        "\n",
        "        combined_scope_bias.append(np.mean(np.array(means_scope)-np.array(true_val)))\n",
        "        combined_scope_bias_std_devs.append(np.std(np.array(means_scope)-np.array(true_val)))\n",
        "        combined_is_bias.append(np.mean(np.array(means_is)-np.array(true_val)))\n",
        "        combined_is_bias_std_devs.append(np.std(np.array(means_is)-np.array(true_val)))\n",
        "\n",
        "        mse_scope = np.mean(np.array(sd_scope)**2) + (np.mean(np.array(means_scope)-np.array(true_val)))**2\n",
        "        # print(\"mse_scope: \", mse_scope)\n",
        "        combined_scope_mse.append(mse_scope)\n",
        "        combined_is_mse.append(np.mean(np.array(sd_is)**2) + (np.mean(np.array(means_is)-np.array(true_val)))**2)\n",
        "\n",
        "    plt.figure()\n",
        "    # Plotting\n",
        "    plt.errorbar(num_trajectories, combined_scope_vars, yerr=combined_scope_var_std_devs, fmt='bs', label='SCOPE')\n",
        "    plt.errorbar(num_trajectories, combined_is_vars, yerr=combined_is_var_std_devs, fmt='ko', label='stepIS')\n",
        "\n",
        "    plt.xlabel('Number of Trajectories')\n",
        "    plt.ylabel('Variance')\n",
        "    plt.title('SCOPE and stepIS Variance Plots vs. Number of Trajectories')\n",
        "    custom_legend_handles = [\n",
        "        Line2D([0], [0], color='blue', marker='s', markersize=10, label='SCOPE'),\n",
        "        Line2D([0], [0], color='black', marker='o', markersize=10, label='stepIS')\n",
        "    ]\n",
        "\n",
        "    plt.legend(handles=custom_legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()  # Ensures proper spacing and avoids clipping\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.figure()\n",
        "    # Plotting\n",
        "    plt.errorbar(num_trajectories, combined_scope_bias, yerr=combined_scope_bias_std_devs, fmt='bs', label='SCOPE')\n",
        "    plt.errorbar(num_trajectories, combined_is_bias, yerr=combined_is_bias_std_devs, fmt='ko', label='stepIS')\n",
        "\n",
        "    plt.xlabel('Number of Trajectories')\n",
        "    plt.ylabel('Bias')\n",
        "    plt.title('SCOPE and stepIS Bias Plots vs. Number of Trajectories')\n",
        "    custom_legend_handles = [\n",
        "        Line2D([0], [0], color='blue', marker='s', markersize=10, label='SCOPE'),\n",
        "        Line2D([0], [0], color='black', marker='o', markersize=10, label='stepIS')\n",
        "    ]\n",
        "\n",
        "    plt.legend(handles=custom_legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()  # Ensures proper spacing and avoids clipping\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    # Plotting\n",
        "    plt.errorbar(num_trajectories, combined_scope_means, yerr=combined_scope_std_devs, fmt='bs', label='SCOPE')\n",
        "    plt.errorbar(num_trajectories, combined_is_means, yerr=combined_is_std_devs, fmt='ko', label='stepIS')\n",
        "\n",
        "    plt.xlabel('Number of Trajectories')\n",
        "    plt.ylabel('Value Estimate')\n",
        "    plt.title('SCOPE and stepIS Value Estimate Plots vs. Number of Trajectories')\n",
        "\n",
        "    # Add horizontal line for true_val\n",
        "    plt.axhline(y=true_value, color='green', linestyle='--', label='True Value')\n",
        "\n",
        "    # Create custom legend handles and labels\n",
        "    custom_legend_handles = [\n",
        "        Line2D([0], [0], color='blue', marker='s', markersize=10, label='SCOPE'),\n",
        "        Line2D([0], [0], color='black', marker='o', markersize=10, label='stepIS'),\n",
        "        Line2D([0], [0], color='green', linestyle='--', label='True Value')\n",
        "    ]\n",
        "\n",
        "    plt.legend(handles=custom_legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()  # Ensures proper spacing and avoids clipping\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    # Plotting\n",
        "    plt.plot(num_trajectories, combined_scope_mse, color='blue', marker='s', label='SCOPE')\n",
        "    plt.plot(num_trajectories, combined_is_mse, color='black', marker='o', label='stepIS')\n",
        "\n",
        "\n",
        "    plt.xlabel('Number of Trajectories')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('SCOPE and stepIS MSE Plots vs. Number of Trajectories')\n",
        "\n",
        "\n",
        "    # Create custom legend handles and labels\n",
        "    custom_legend_handles = [\n",
        "        Line2D([0], [0], color='blue', marker='s', markersize=10, label='SCOPE'),\n",
        "        Line2D([0], [0], color='black', marker='o', markersize=10, label='stepIS')\n",
        "    ]\n",
        "\n",
        "    plt.legend(handles=custom_legend_handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()  # Ensures proper spacing and avoids clipping\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGJs4uAQWCEJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def plot_mse_over_trajectories(env, num_trajectories, behav_policy, eval_policy, phi_traj, sparsity):\n",
        "    combined_scope = []\n",
        "    combined_is = []\n",
        "    bias_scope = []\n",
        "    bias_is = []\n",
        "    variance_scope = []\n",
        "    variance_is = []\n",
        "\n",
        "    for i in range(len(num_trajectories)):\n",
        "        results = run_experiment(env, num_trajectories[i], behav_policy, eval_policy, phi_traj, sparsity)\n",
        "        if num_trajectories[i] == 200:\n",
        "            true_val = results['True Evaluation']\n",
        "\n",
        "        scope_results = results['scope_results']\n",
        "        is_results = results['IS_results']\n",
        "\n",
        "        scope_std_dev = scope_results['std_deviation']\n",
        "        scope_mean = scope_results['mean']\n",
        "        is_std_dev = is_results['std_deviation']\n",
        "        is_mean = is_results['mean']\n",
        "\n",
        "        scope_bias = true_val - scope_mean\n",
        "        is_bias = true_val - is_mean\n",
        "\n",
        "        scope_mse = scope_std_dev ** 2 + (scope_bias) ** 2\n",
        "        is_mse = is_std_dev ** 2 + (is_bias) ** 2\n",
        "\n",
        "        bias_scope.append(scope_bias)\n",
        "        bias_is.append(is_bias)\n",
        "\n",
        "        variance_scope.append(scope_std_dev ** 2)\n",
        "        variance_is.append(is_std_dev ** 2)\n",
        "\n",
        "        combined_scope.append(scope_mse)\n",
        "        combined_is.append(is_mse)\n",
        "    plt.figure()\n",
        "    plt.plot(num_trajectories, bias_scope, marker='o', label=f'SCOPE')\n",
        "    plt.plot(num_trajectories, bias_is, marker='x', label=f'stepIS')\n",
        "    plt.title('SCOPE and stepIS Bias vs. Number of Trajectories')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(num_trajectories, variance_scope, marker='o', label=f'SCOPE')\n",
        "    plt.plot(num_trajectories, variance_is, marker='x', label=f'stepIS')\n",
        "    plt.title('SCOPE and stepIS Variance vs. Number of Trajectories')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(num_trajectories, combined_scope, marker='o', label=f'SCOPE')\n",
        "    plt.plot(num_trajectories, combined_is, marker='x', label=f'stepIS')\n",
        "\n",
        "    plt.xlabel('Number of Trajectories')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('SCOPE and stepIS MSE vs. Number of Trajectories')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Trajectories"
      ],
      "metadata": {
        "id": "yrr_Arv0Wjbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Tb-jzUWHD5"
      },
      "outputs": [],
      "source": [
        "def compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, phi_traj, sparsity):\n",
        "  for i in num_trajectories:\n",
        "    # print(\"Number of trajectories: \",i)\n",
        "    run_experiment1(env,i ,behav_policy, eval_policy, phi_traj, sparsity)\n",
        "\n",
        "  plot_rewards_over_trajectories1(env,num_trajectories,behav_policy, eval_policy, phi_traj,sparsity)\n",
        "  # plot_mse_over_trajectories(env,num_trajectories,behav_policy, eval_policy, phi_traj,sparsity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwzqPJrkXLqD"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfTLKRZiWMEX"
      },
      "source": [
        "## Dense 2 bad regions, 1 good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuE2r_BwL84C"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwgs4v_LWtlc"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)], 1, -2, 3, 0.9)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.9 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFb3Zl7wgmb6"
      },
      "source": [
        "## Sparse 2 bad, 1 good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJA9nBtigfst"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   print('num_trajectories: ', i)\n",
        "#   modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVi5v0wcf-8B"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "modify_data(env, 1000, behav_policy, eval_policy , 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg-46zbWno9G"
      },
      "source": [
        "## Sparse 2 good, 1 bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlXhjU9aniQB"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1)], [(2, 2),(3,3)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   print('num_trajectories: ', i)\n",
        "#   modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxUR71zC4Q4Y"
      },
      "outputs": [],
      "source": [
        "np.random.seed(31)\n",
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQGHwXX24K6C"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(1, 1)], [(2, 2),(3,3)], 1, -2, 3, 0.101)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBLyXXu2HTkA"
      },
      "source": [
        "## Sparse, 1 bad 1 good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ckyTpWGHUrr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1)], [(3,3)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   print('num_trajectories: ', i)\n",
        "#   modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjG1irXk_7Gn"
      },
      "source": [
        "### Same rewards for bad and good regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw9aAJV5_bBN"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(1, 1)], [(3,3)], 2, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   print('num_trajectories: ', i)\n",
        "#   modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-c-kUt6XwSN"
      },
      "source": [
        "## Dense 2 bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xp0deXvX0hy"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 1, -2, 3, 0.9)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   print('num_trajectories: ', i)\n",
        "#   modify_data(env, i, behav_policy, eval_policy, 0.3,0.9)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.9 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVWT7YhQvcTQ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 2), (1, 4)], [], 1, -2, 3, 0.9)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.9)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.9 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOKRbVgSXNJx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 1, -2, 3, 0.88)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.88)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.88)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6atyMk5ZmUB"
      },
      "source": [
        "## Dense 2 bad, 0.7 for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-QUBYAOZs1w"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 1, -2, 3, 0.9)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   run_experiment(env,i ,behav_policy, eval_policy, 0.3, 0.9)\n",
        "#   print(i,\" trajectories done\")\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.7, 0.9 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqZijgMwWo3Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_tiB6oW0RT"
      },
      "source": [
        "## Sparse 2 bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y9m66tOWxqJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klniDzqO8zOV"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 1, -2, 3, 0.2)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.2)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6BO7Y3IFO4q"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (3, 2)], [], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n-5WUK4aMRE"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 2), (2, 3)], [], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HByOCyGtLLZ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(2, 2), (3, 3)], [], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGH-BbHjoJG6"
      },
      "source": [
        "## Sparse 3 bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhd5e3mMoHLq"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(2, 1), (2, 2), (2,3)], [], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iSAu_tEtbGL"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2), (3,3)], [], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_1Q2ySRbpz"
      },
      "source": [
        "### Mid Sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8P0TE27Jyv8"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2), (3,3)], [], 1, -2, 3, 0.5)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.5)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCaK3MJbofvE"
      },
      "source": [
        "## Sparse 2 good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twmD2EtloetZ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [], [(2, 1), (2, 2)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wu_E-PGJn1j"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [], [(1, 1), (2, 2)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxAojTYJ4fRr"
      },
      "source": [
        "## Sparse 1 good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7VMSt0E4oV1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [], [(1, 2)], 1, -2, 3, 0.1)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "for i in num_trajectories:\n",
        "  print('num_trajectories: ', i)\n",
        "  modify_data(env, i, behav_policy, eval_policy, 0.3,0.1)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHBOdzz971lG"
      },
      "source": [
        "## Sparse, 1 bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxvMHe2K75pr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(2, 2)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   print('num_trajectories: ', i)\n",
        "#   modify_data(env, i, behav_policy, eval_policy, 0.3,0.0)\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh7EX2xl054K"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(2, 3)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAPG9Rep39H7"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(1, 4)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF6ZhCFA7Ti_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(3, 4)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xtrxyffHAxvG"
      },
      "outputs": [],
      "source": [
        "# np.random.seed(42)\n",
        "# env = GridWorld(height, width, start, end, [(3, 4)], [], 1, -2,3, 0.0)\n",
        "# eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "# behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "# num_trajectories = [200,400, 600, 800,1000]\n",
        "# compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX4rDFNeM0Me"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(3, 1)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIKMb5QjmE81"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "env = GridWorld(height, width, start, end, [(4, 2)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4BO-qbfCXFI"
      },
      "source": [
        "## Dense, 1 bad, similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yz766DbCpYy-"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2, 2)], [], 1, -2,3, 0.8)\n",
        "eval_policy = {\"up\": 0.36, \"down\": 0.14, \"left\": 0.14, \"right\": 0.36}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   run_experiment(env,i ,behav_policy, eval_policy, 0.3, 0.8)\n",
        "#   print(i,\" trajectories done\")\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.8 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK_GXecGCMyG"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(4, 2)], [], 1, -2,3, 0.8)\n",
        "eval_policy = {\"up\": 0.36, \"down\": 0.14, \"left\": 0.14, \"right\": 0.36}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   run_experiment(env,i ,behav_policy, eval_policy, 0.3, 0.8)\n",
        "#   print(i,\" trajectories done\")\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.8 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMxw9pjqCq0_"
      },
      "source": [
        "## Sparse, 1 bad, similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWxp4FAYChaj"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2, 3)], [], 1, -2,3, 0.01)\n",
        "eval_policy = {\"up\": 0.36, \"down\": 0.14, \"left\": 0.14, \"right\": 0.36}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# for i in num_trajectories:\n",
        "#   run_experiment(env,i ,behav_policy, eval_policy, 0.3, 0.01)\n",
        "#   print(i,\" trajectories done\")\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.01 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVKv_cvy_0Hl"
      },
      "source": [
        "# Fix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1rwwmhk-Az1"
      },
      "source": [
        "## Dense same policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk9Pb2ApHXHV"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2,3)], [], 1, -2,3, 0.8)\n",
        "eval_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.8 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fW6paiYWYU0"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2,3)], [], 1, -2,3, 0.85)\n",
        "eval_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.85 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym9mr3oI-IqF"
      },
      "source": [
        "## Sparse same policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egBEjYBnnXgT"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2,3)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLLbs59IB7iY"
      },
      "source": [
        "## Sparse 2 bad regions, same policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cr4zbmWZB2x5"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2,3),(3,4)], [], 1, -2,3, 0.0)\n",
        "eval_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo5eAOu7Wgqv"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2,3),(3,4)], [], 1, -2,3, 0.1)\n",
        "eval_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "# num_trajectories = [200,400, 600, 800,1000]\n",
        "num_trajectories = [200,400]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEghCrviU4yT"
      },
      "source": [
        "## Sparse one bad region, very similar policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HbidVcsU399"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(height, width, start, end, [(2, 2)], [], 1, -2,3, 0.1)\n",
        "eval_policy = {\"up\": 0.26, \"down\": 0.24, \"left\": 0.24, \"right\": 0.26}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]\n",
        "# num_trajectories = [200,400]\n",
        "compare_experiments_over_trajectories(env, behav_policy, eval_policy, num_trajectories, 0.3, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHzM9twKpFeG"
      },
      "source": [
        "## Two bad regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUB7QtVtd5Ja"
      },
      "outputs": [],
      "source": [
        "env_bad = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 0.5, -2, 3 )\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_trajectories = [200,400, 600, 800,1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70V9ZZOmd_ll"
      },
      "outputs": [],
      "source": [
        "results = run_experiment(env_bad,200 ,behav_policy, eval_policy, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hggvFZ3mkl7"
      },
      "outputs": [],
      "source": [
        "for i in num_trajectories:\n",
        "  run_experiment(env_bad,i ,behav_policy, eval_policy, 0.3)\n",
        "  print(i,\" trajectories done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7C9p_TI0tm5"
      },
      "outputs": [],
      "source": [
        "plot_rewards_over_trajectories(env_bad, num_trajectories, behav_policy, eval_policy, 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnB1-MR9p4mP"
      },
      "source": [
        "## Single bad region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22V2zzp0lstx"
      },
      "outputs": [],
      "source": [
        "num_trajectories = [200, 400, 600, 800, 1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiUnM1eTHTk6"
      },
      "outputs": [],
      "source": [
        "env_bad_one = GridWorld(height, width, start, end, [(2, 2)], [], 0.5, -2, 3 )\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I78rynTvHAX"
      },
      "outputs": [],
      "source": [
        "for i in num_trajectories:\n",
        "  run_experiment(env_bad_one,i ,behav_policy, eval_policy, 0.3)\n",
        "  print(i,\" trajectories done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClBIDUN4OvlC"
      },
      "outputs": [],
      "source": [
        "plot_rewards_over_trajectories(env_bad_one, num_trajectories ,behav_policy, eval_policy, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEYuirxFLWki"
      },
      "outputs": [],
      "source": [
        "evaluation_policies = create_policy_set(env_bad_one, run_policy,eval_policy, 1000)\n",
        "true_evaluation = print(\"True Eval: \",calc_V_pi_e(evaluation_policies))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rpywb5srR0U"
      },
      "source": [
        "## Two bad regions One good region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkhlDWkEoSbs"
      },
      "outputs": [],
      "source": [
        "env_2bad_1good = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)], 0.5, -2, 3 )\n",
        "eval_policy = {\"up\": 0.36, \"down\": 0.14, \"left\": 0.14, \"right\": 0.36}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-jUlZHHosjR"
      },
      "outputs": [],
      "source": [
        "run_experiment(env_2bad_1good,200, behav_policy, eval_policy, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKdqmhfPqGvx"
      },
      "outputs": [],
      "source": [
        "num_trajectories = [200, 400, 600, 800, 1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rGV9Tz6qE-J"
      },
      "outputs": [],
      "source": [
        "for i in num_trajectories:\n",
        "  run_experiment(env_2bad_1good,i ,behav_policy, eval_policy, 0.3)\n",
        "  print(i,\" trajectories done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U34jzCs0z-WR"
      },
      "outputs": [],
      "source": [
        "plot_rewards_over_trajectories(env_2bad_1good, num_trajectories, behav_policy, eval_policy, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID5KOA4GLBxY"
      },
      "outputs": [],
      "source": [
        "evaluation_policies = create_policy_set(env_2bad_1good, run_policy,eval_policy, 1000)\n",
        "true_evaluation = print(\"True Eval: \",calc_V_pi_e(evaluation_policies))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16UtIx9D_Qn4"
      },
      "source": [
        "## Two bad regions One good region similar policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or090tX85xBm"
      },
      "outputs": [],
      "source": [
        "env_2bad_1good_similar = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)], 0.5, -2, 3 )\n",
        "behav_policy = {\"up\": 0.36, \"down\": 0.14, \"left\": 0.14, \"right\": 0.36}\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9osZRpc6TuE"
      },
      "outputs": [],
      "source": [
        "for i in num_trajectories:\n",
        "  run_experiment(env_2bad_1good_similar,i ,behav_policy, eval_policy, 0.3)\n",
        "  print(i,\" trajectories done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pvXHlc88vQn"
      },
      "outputs": [],
      "source": [
        "plot_rewards_over_trajectories(env_2bad_1good_similar, num_trajectories, behav_policy, eval_policy, 0.3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}